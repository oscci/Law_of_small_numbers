
@article{aberson2000,
  title = {Evaluation of an {{Interactive Tutorial}} for {{Teaching}} the {{Central Limit Theorem}}},
  author = {Aberson, Christopher L. and Berger, Dale E. and Healy, Michael R. and Kyle, Diana J. and Romero, Victoria L.},
  year = {2000},
  month = oct,
  journal = {Teaching of Psychology},
  volume = {27},
  number = {4},
  pages = {289--291},
  publisher = {{SAGE Publications Inc}},
  issn = {0098-6283},
  doi = {10.1207/S15328023TOP2704_08},
  abstract = {In this article, we present an evaluation of a Web-based, interactive tutorial used to present the sampling distribution of the mean. The tutorial allows students to draw samples and explore the shapes of sampling distributions for several sample sizes. To evaluate the effectiveness of the tutorial, 111 students enrolled in statistics or research methods courses used either the interactive tutorial or attended a lecture and a demonstration on the sampling distribution of the mean. Students in both groups improved from pretest to posttest and no statistically significant differences between improvement scores were found between groups. Additionally, students rated the tutorial as easy to use and understand. In this study, we provide evidence that an Internet tutorial can be comparable in effectiveness to standard lecture or demonstration techniques.},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/A63H5BIY/Aberson et al. - 2000 - Evaluation of an Interactive Tutorial for Teaching.pdf}
}

@article{anwyl-irvine2020,
  title = {Gorilla in Our Midst: An Online Behavioral Experiment Builder},
  shorttitle = {Gorilla in Our Midst},
  author = {{Anwyl-Irvine}, Alexander L. and Massonni{\'e}, Jessica and Flitton, Adam and Kirkham, Natasha and Evershed, Jo K.},
  year = {2020},
  month = feb,
  journal = {Behavior Research Methods},
  volume = {52},
  number = {1},
  pages = {388--407},
  issn = {1554-3528},
  doi = {10.3758/s13428-019-01237-x},
  abstract = {Behavioral researchers are increasingly conducting their studies online, to gain access to large and diverse samples that would be difficult to get in a laboratory environment. However, there are technical access barriers to building experiments online, and web browsers can present problems for consistent timing\textemdash an important issue with reaction-time-sensitive measures. For example, to ensure accuracy and test\textendash retest reliability in presentation and response recording, experimenters need a working knowledge of programming languages such as JavaScript. We review some of the previous and current tools for online behavioral research, as well as how well they address the issues of usability and timing. We then present the Gorilla Experiment Builder (gorilla.sc), a fully tooled experiment authoring and deployment platform, designed to resolve many timing issues and make reliable online experimentation open and accessible to a wider range of technical abilities. To demonstrate the platform's aptitude for accessible, reliable, and scalable research, we administered a task with a range of participant groups (primary school children and adults), settings (without supervision, at home, and under supervision, in both schools and public engagement events), equipment (participant's own computer, computer supplied by the researcher), and connection types (personal internet connection, mobile phone 3G/4G). We used a simplified flanker task taken from the attentional network task (Rueda, Posner, \& Rothbart, 2004). We replicated the ``conflict network'' effect in all these populations, demonstrating the platform's capability to run reaction-time-sensitive experiments. Unresolved limitations of running experiments online are then discussed, along with potential solutions and some future features of the platform.},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/UU4FXCBH/Anwyl-Irvine et al. - 2020 - Gorilla in our midst An online behavioral experim.pdf}
}

@article{bar-hillel1979,
  title = {The Role of Sample Size in Sample Evaluation},
  author = {{Bar-Hillel}, Maya},
  year = {1979},
  journal = {Organizational Behavior and Human Performance},
  volume = {24},
  number = {2},
  pages = {245--257},
  issn = {0030-5073},
  doi = {10.1016/0030-5073(79)90028-X},
  abstract = {D. Kahneman and A. Tversky (Cognitive Psychology, 1972, 3, 430\textendash 454) claimed that ``the notion that sampling variance decreases in proportion to sample size is apparently not part of man's repertoire of intuitions.'' This study presents a series of experiments showing that it is possible to elicit judgments indicating that perceived sample accuracy increases with sample size. However, these judgments seem to reflect sensitivity to sample-to-population ratio rather than absolute sample size. In fact, people may trade sample size for sample-to-population ratio, even when this actually decreases expected sample accuracy. The widely held belief that the accuracy of a sample is connected with its relative size to the universe is mistaken. A sample smaller than 1\%, taken from one universe, can be much more reliable than one comprising 10\% of another. To determine with equal accuracy the average age of the population of New York City and of Peoria, Illinois, will require samples of equal size (variances of population being equal). (Zeisel, 1960).},
  langid = {english},
  keywords = {Law of Small Numbers},
  file = {/Users/dorothybishop/Zotero/storage/DS9DLY23/Bar-Hillel - 1979 - The role of sample size in sample evaluation.pdf;/Users/dorothybishop/Zotero/storage/EMU3N9Q7/003050737990028X.html}
}

@article{bishop2020,
  title = {The Psychology of Experimental Psychologists: Overcoming Cognitive Constraints to Improve Research: The 47th {{Sir Frederic Bartlett Lecture}}},
  shorttitle = {The Psychology of Experimental Psychologists},
  author = {Bishop, D. V. M.},
  year = {2020},
  journal = {Quarterly Journal of Experimental Psychology (2006)},
  volume = {73},
  number = {1},
  pages = {1--19},
  issn = {1747-0218},
  doi = {10.1177/1747021819886519},
  abstract = {Like many other areas of science, experimental psychology is affected by a ``replication crisis'' that is causing concern in many fields of research. Approaches to tackling this crisis include better training in statistical methods, greater transparency and openness, and changes to the incentives created by funding agencies, journals, and institutions. Here, I argue that if proposed solutions are to be effective, we also need to take into account human cognitive constraints that can distort all stages of the research process, including design and execution of experiments, analysis of data, and writing up findings for publication. I focus specifically on cognitive schemata in perception and memory, confirmation bias, systematic misunderstanding of statistics, and asymmetry in moral judgements of errors of commission and omission. Finally, I consider methods that may help mitigate the effect of cognitive constraints: better training, including use of simulations to overcome statistical misunderstanding; specific programmes directed at inoculating against cognitive biases; adoption of Registered Reports to encourage more critical reflection in planning studies; and using methods such as triangulation and ``pre mortem'' evaluation of study design to foster a culture of dialogue and criticism.},
  pmcid = {PMC6909195},
  pmid = {31724919},
  keywords = {Cognitive bias,Reasoning},
  file = {/Users/dorothybishop/Zotero/storage/AIXFNULE/Bishop - 2020 - The psychology of experimental psychologists Over.pdf}
}

@article{bradstreet1996,
  title = {Teaching {{Introductory Statistics Courses}} so {{That Nonstatisticians Experience Statistical Reasoning}}},
  author = {Bradstreet, Thomas E.},
  year = {1996},
  journal = {The American Statistician},
  volume = {50},
  number = {1},
  pages = {69--78},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0003-1305},
  doi = {10.2307/2685047},
  abstract = {The most critical point in developing dynamic introductory statistics courses for nonstatisticians is deciding whether to teach statistical reasoning (concepts and thinking), statistical methods (computations), or both. Statistical reasoning should precede statistical methods. Workshop-based courses effectively provide situated learning and an intimate teaching environment. Use real (or realistic) data, graphics, and teach exploratory data analysis before classical methods. Evaluate the students' levels of statistical anxiety prior to and during the course.}
}

@article{braun2014,
  title = {R {{Tricks}} for {{Kids}}},
  author = {Braun, W. John and White, Bethany JG and Craig, Gavin},
  year = {2014},
  journal = {Teaching Statistics},
  volume = {36},
  number = {1},
  pages = {7--12},
  issn = {1467-9639},
  doi = {10.1111/test.12016},
  abstract = {Real-world phenomena simulation models, which can be used to engage middle-school students with probability, are described. Links to R instructional material and easy-to-use code are provided to facilitate implementation in the classroom.},
  langid = {english},
  keywords = {fire,middle-school education,probability,R,random numbers,randomness,simulation},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/test.12016},
  file = {/Users/dorothybishop/Zotero/storage/GIG7LKU6/Braun et al. - 2014 - R Tricks for Kids.pdf;/Users/dorothybishop/Zotero/storage/56XNRM2V/test.html}
}

@article{chalmers2020,
  title = {Writing {{Effective}} and {{Reliable Monte Carlo Simulations}} with the {{SimDesign Package}}},
  author = {Chalmers, R. Philip and Adkins, Mark C.},
  year = {2020},
  month = may,
  journal = {The Quantitative Methods for Psychology},
  volume = {16},
  number = {4},
  pages = {248--280},
  issn = {2292-1354},
  doi = {10.20982/tqmp.16.4.p248},
  abstract = {The purpose of this tutorial is to discuss and demonstrate how to write safe, effective, and intuitive computer code for Monte Carlo simulation experiments containing one or more simulation factors. Throughout this tutorial the SimDesign package (Chalmers, 2020), available within the R programming environment, will be adopted due to its ability to accommodate a number of desirable execution features. The article begins by discussing a selection of attractive coding strategies that should be present in Monte Carlo simulation experiments, showcases how the SimDesign package can satisfy many of these desirable strategies, and provides a worked mediation analysis simulation example to demonstrate the implementation of these features. To demonstrate how the package can be used for real-world experiments, the simulation explored by Flora and Curran (2004) pertaining to a confirmatory factor analysis robustness study with ordinal response data is also presented and discussed.},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/NS3VRZJG/Chalmers and Adkins - 2020 - Writing Effective and Reliable Monte Carlo Simulat.pdf}
}

@misc{clarke2017,
  title = {Ggbeeswarm: Categorical {{Scatter}} ({{Violin Point}}) {{Plots}}. {{R}}   Package Version 0.6.0.},
  author = {Clarke, Eric and {Sherrill-Mix}, Scott},
  year = {2017}
}

@article{cohen1962,
  title = {The Statistical Power of Abnormal-Social Psychological Research: A Review},
  shorttitle = {The Statistical Power of Abnormal-Social Psychological Research},
  author = {Cohen, Jacob},
  year = {1962},
  journal = {The Journal of Abnormal and Social Psychology},
  volume = {65},
  number = {3},
  pages = {145--153},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {0096-851X(Print)},
  doi = {10.1037/h0045186},
  keywords = {Power},
  file = {/Users/dorothybishop/Zotero/storage/8YY9WUQS/Cohen - 1962 - The statistical power of abnormal-social psycholog.pdf;/Users/dorothybishop/Zotero/storage/JYSTJ8V8/1964-09448-001.html}
}

@article{diedenhofen2015,
  title = {Cocor: A Comprehensive Solution for the Statisticalc Comparison of Correlations},
  author = {Diedenhofen,, B and Musch, J},
  year = {2015},
  journal = {PLOS One},
  volume = {10},
  number = {4},
  pages = {e0121945},
  doi = {10.1371/journal.pone.0121945}
}

@article{dietrichson2021,
  title = {Targeted School-Based Interventions for Improving Reading and Mathematics for Students with or at Risk of Academic Difficulties in {{Grades K}}-6: A Systematic Review},
  shorttitle = {Targeted School-Based Interventions for Improving Reading and Mathematics for Students with or at Risk of Academic Difficulties in {{Grades K}}-6},
  author = {Dietrichson, Jens and Filges, Trine and Seerup, Julie K. and Klokker, Rasmus H. and Viinholt, Bj{\o}rn C. A. and B{\o}g, Martin and Eiberg, Misja},
  year = {2021},
  journal = {Campbell Systematic Reviews},
  volume = {17},
  number = {2},
  pages = {e1152},
  issn = {1891-1803},
  doi = {10.1002/cl2.1152},
  abstract = {Background Low levels of numeracy and literacy skills are associated with a range of negative outcomes later in life, such as reduced earnings and health. Obtaining information about effective interventions for children with or at risk of academic difficulties is therefore important. Objectives The main objective was to assess the effectiveness of interventions targeting students with or at risk of academic difficulties in kindergarten to Grade 6. Search Methods We searched electronic databases from 1980 to July 2018. We searched multiple international electronic databases (in total 15), seven national repositories, and performed a search of the grey literature using governmental sites, academic clearinghouses and repositories for reports and working papers, and trial registries (10 sources). We hand searched recent volumes of six journals and contacted international experts. Lastly, we used included studies and 23 previously published reviews for citation tracking. Selection Criteria Studies had to meet the following criteria to be included: Population: The population eligible for the review included students attending regular schools in kindergarten to Grade 6, who were having academic difficulties, or were at risk of such difficulties. Intervention: We included interventions that sought to improve academic skills, were conducted in schools during the regular school year, and were targeted (selected or indicated). Comparison: Included studies used an intervention-control group design or a comparison group design. We included randomised controlled trials (RCT); quasi-randomised controlled trials (QRCT); and quasi-experimental studies (QES). Outcomes: Included studies used standardised tests in reading or mathematics. Setting: Studies carried out in regular schools in an OECD country were included. Data Collection and Analysis Descriptive and numerical characteristics of included studies were coded by members of the review team. A review author independently checked coding. We used an extended version of the Cochrane Risk of Bias tool to assess risk of bias. We used random-effects meta-analysis and robust-variance estimation procedures to synthesise effect sizes. We conducted separate meta-analyses for tests performed within three months of the end of interventions (short-term effects) and longer follow-up periods. For short-term effects, we performed subgroup and moderator analyses focused on instructional methods and content domains. We assessed sensitivity of the results to effect size measurement, outliers, clustered assignment of treatment, risk of bias, missing moderator information, control group progression, and publication bias. Results We found in total 24,414 potentially relevant records, screened 4247 of them in full text, and included 607 studies that met the inclusion criteria. We included 205 studies of a wide range of intervention types in at least one meta-analysis (202 intervention-control studies and 3 comparison designs). The reasons for excluding studies from the analysis were that they had too high risk of bias (257), compared two alternative interventions (104 studies), lacked necessary information (24 studies), or used overlapping samples (17 studies). The total number of student observations in the analysed studies was 226,745. There were 93\% RCTs among the 327 interventions we included in the meta-analysis of intervention-control contrasts and 86\% were from the United States. The target group consisted of, on average, 45\% girls, 65\% minority students, and 69\% low-income students. The mean Grade was 2.4. Most studies included in the meta-analysis had a moderate to high risk of bias. The overall average effect sizes (ES) for short-term and follow-up outcomes were positive and statistically significant (ES = 0.30, 95\% confidence interval [CI] = [0.25, 0.34] and ES = 0.27, 95\% CI = [0.17, 0.36]), respectively). The effect sizes correspond to around one third to one half of the achievement gap between fourth Grade students with high and low socioeconomic status in the United States and to a 58\% chance that a randomly selected score of an intervention group student is greater than the score of a randomly selected control group student. All measures indicated substantial heterogeneity across short-term effect sizes. Follow-up outcomes pertain almost exclusively to studies examining small-group instruction by adults and effects on reading measures. The follow-up effect sizes were considerably less heterogeneous than the short-term effect sizes, although there was still statistically significant heterogeneity. Two instructional methods, peer-assisted instruction and small-group instruction by adults, had large and statistically significant average effect sizes that were robust across specifications in the subgroup analysis of short-term effects (ES around 0.35\textendash 0.45). In meta-regressions that adjusted for methods, content domains, and other study characteristics, they had significantly larger effect sizes than computer-assisted instruction, coaching of personnel, incentives, and progress monitoring. Peer-assisted instruction also had significantly larger effect sizes than medium-group instruction. Besides peer-assisted instruction and small-group instruction, no other methods were consistently significant across the analyses that tried to isolate the association between a specific method and effect sizes. However, most analyses showed statistically significant heterogeneity also within categories of instructional methods. We found little evidence that effect sizes were larger in some content domains than others. Fractions had significantly higher associations with effect sizes than all other math domains, but there were only six studies of interventions targeting fractions. We found no evidence of adverse effects in the sense that no method or domain had robustly negative associations with effect sizes. The meta-regressions revealed few other significant moderators. Interventions in higher Grades tend to have somewhat lower effect sizes, whereas there were no significant differences between QES and RCTs, general tests and tests of subdomains, and math tests and reading tests. Authors' Conclusions Our results indicate that interventions targeting students with or at risk of academic difficulties from kindergarten to Grade 6 have on average positive and statistically significant short-term and follow-up effects on standardised tests in reading and mathematics. Peer-assisted instruction and small-group instruction are likely to be effective components of such interventions. We believe the relatively large effect sizes together with the substantial unexplained heterogeneity imply that schools can reduce the achievement gap between students with or at risk of academic difficulties and not-at-risk students by implementing targeted interventions, and that more research into the design of effective interventions is needed.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cl2.1152},
  file = {/Users/dorothybishop/Zotero/storage/T5CBRSQK/Dietrichson et al. - 2021 - Targeted school-based interventions for improving .pdf;/Users/dorothybishop/Zotero/storage/GXPSCVZ6/cl2.html}
}

@article{dockrell2007,
  title = {Measuring and Understanding Patterns of Change in Intervention Studies with Children: Implications for Evidence-Based Practice},
  shorttitle = {Measuring and Understanding Patterns of Change in Intervention Studies with Children},
  author = {Dockrell, Julie E. and Law, James},
  year = {2007},
  month = jun,
  journal = {Evidence-Based Communication Assessment and Intervention},
  volume = {1},
  number = {2},
  pages = {86--97},
  publisher = {{Taylor \& Francis}},
  issn = {1748-9539},
  doi = {10.1080/17489530701437204},
  abstract = {Purpose: Comparisons across studies of the effects of intervention are problematic. Such analyses raise both methodological and statistical challenges. A single data set was examined to investigate whether different established approaches to measuring change in children with specific language impairments alter the conclusions that can be drawn regarding the efficacy of an intervention. Methods: Measures of cognitive and language skills were collected at baseline and at six months following an intervention. Reliable and valid psychometric measures were used. Data from the intervention study were used to explore the patterns of results obtained using four different measures of change: change of diagnostic category, differential improvement across assessment measures, item specific changes and predictors of individual change. Results: Associations between different tests purporting to measure similar constructs were modest. The measures identified different children as impaired both at baseline and follow-up. No effect of intervention was evident when a categorical analysis of impairment was used. Both treatment and comparison children changed significantly across time on the majority of measures, providing evidence of development, but specific effects of the intensive intervention were evident using ANCOVAs. Item analysis indicated that one of the standardized language tests adopted in the evaluation was insensitive to change over a six month period. Change in individual children's performance was predicted by language level on entry to the project. Conclusion: The implications of the results are discussed in terms of the range of analytic approaches available to intervention researchers and the need to consider combinations of methods when analysing outcome data. \textdagger We would like to thank ICAN, the health trusts involved and the two research officers, Kerry Williams and Belinda Seeff, who collected the data.},
  keywords = {intervention,Language,measuring change,SLI},
  annotation = {\_eprint: https://doi.org/10.1080/17489530701437204},
  file = {/Users/dorothybishop/Zotero/storage/5LLGEGEZ/Dockrell and Law - 2007 - Measuring and understanding patterns of change in .pdf;/Users/dorothybishop/Zotero/storage/3Z6UMVDZ/17489530701437204.html}
}

@article{dockrell2015,
  title = {Measurement {{Issues}}: Assessing Language Skills in Young Children},
  shorttitle = {Measurement {{Issues}}},
  author = {Dockrell, Julie E. and Marshall, Chlo{\"e} R.},
  year = {2015},
  journal = {Child and Adolescent Mental Health},
  volume = {20},
  number = {2},
  pages = {116--125},
  issn = {1475-3588},
  doi = {10.1111/camh.12072},
  abstract = {Background Language and communication skills are central to children's ability to engage in social relationships and access learning experiences. This paper identifies issues which practitioners and researchers should consider when assessing language skills. A range of current language assessments is reviewed. Key findings Current screening measures do not meet psychometric prerequisites to identify language problems. There are significant challenges in the interpretation of language assessments, where socioeconomic status, language status and dialect, hearing impairment and test characteristics impact results. Conclusions Psychometrically sound assessments of language are an essential component of developing effective and efficient interventions. The language trajectories of preschool children vary substantially; current screening measures have significant limitations. Composite measures of language performance are better indicators of language problems and disorders than single measures of component skills.},
  langid = {english},
  keywords = {assessment,dynamic,Language,preschool,psychometrics},
  annotation = {\_eprint: https://acamh.onlinelibrary.wiley.com/doi/pdf/10.1111/camh.12072},
  file = {/Users/dorothybishop/Zotero/storage/AJDYINNM/Dockrell and Marshall - 2015 - Measurement Issues Assessing language skills in y.pdf;/Users/dorothybishop/Zotero/storage/2MZNMZ3U/camh.html}
}

@article{evans1977,
  title = {Proportionality and Sample Size as Factors in Intuitive Statistical Judgement},
  author = {Evans, J. St. B. T. and Dusoir, A. E.},
  year = {1977},
  month = apr,
  journal = {Acta Psychologica},
  volume = {41},
  number = {2},
  pages = {129--137},
  issn = {0001-6918},
  doi = {10.1016/0001-6918(77)90030-0},
  abstract = {An experiment is reported in which Ss are asked to compare pairs of binomial samples and judge which, if either, gives better evidence of asymmetry in their underlying population. The results indicate that whilst all Ss are influenced by sample proportion only about one third give appropriate weighting to differences in sample size. However, according to the theory of `representativeness' postulated by Kahneman and Tversky (1972) sample size should have been ignored altogether. A second experiment confirmed the hypothesis that their results reflected the use of over-complex problem presentation. When one of their problems was systematically simplified, a significant majority of Ss showed insight into the role of sample size.},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/N4VVV978/Evans and Dusoir - 1977 - Proportionality and sample size as factors in intu.pdf;/Users/dorothybishop/Zotero/storage/3RNM6872/0001691877900300.html}
}

@article{fong1986,
  title = {The Effects of Statistical Training on Thinking about Everyday Problems},
  author = {Fong, Geoffrey T and Krantz, David H and Nisbett, Richard E},
  year = {1986},
  month = jul,
  journal = {Cognitive Psychology},
  volume = {18},
  number = {3},
  pages = {253--292},
  issn = {0010-0285},
  doi = {10.1016/0010-0285(86)90001-0},
  abstract = {People possess an abstract inferential rule system that is an intuitive version of the law of large numbers. Because the rule system is not tied to any particular content domain, it is possible to improve it by formal teaching techniques. We present four experiments that support this view. In Experiments 1 and 2, we taught subjects about the formal properties of the law of large numbers in brief training sessions in the laboratory and found that this increased both the frequency and the quality of statistical reasoning for a wide variety of problems of an everyday nature. In addition, we taught subjects about the rule by a ``guided induction'' technique, showing them how to use the rule to solve problems in particular domains. Learning from the examples was abstracted to such an extent that subjects showed just as much improvement on domains where the rule was not taught as on domains where it was. In Experiment 3, the ability to analyze an everyday problem with reference to the law of large numbers was shown to be much greater for those who had several years of training in statistics than for those who had less. Experiment 4 demonstrated that the beneficial effects of formal training in statistics may hold even when subjects are tested completely outside of the context of training. In general, these four experiments support a rather ``formalist'' theory of reasoning: people reason using very abstract rules, and their reasoning about a wide variety of content domains can be affected by direct manipulation of these abstract rules.},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/I5E5K4IS/Fong et al. - 1986 - The effects of statistical training on thinking ab.pdf;/Users/dorothybishop/Zotero/storage/JFHSGD4U/0010028586900010.html}
}

@article{fong1991,
  title = {Immediate and Delayed Transfer of Training Effects in Statistical Reasoning},
  author = {Fong, Geoffrey T. and Nisbett, Richard E.},
  year = {1991},
  journal = {Journal of Experimental Psychology: General},
  volume = {120},
  number = {1},
  pages = {34--45},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-2222(Electronic),0096-3445(Print)},
  doi = {10.1037/0096-3445.120.1.34},
  abstract = {Ss were trained on the law of large numbers in a given domain through the use of example problems. They were then tested either on that domain or on another domain either immediately or after a 2-wk delay. Strong domain independence was found when testing was immediate. This transfer of training was not due simply to Ss' ability to draw direct analogies between problems in the trained domain and in the untrained domain. After the 2-wk delay, it was found that (1) there was no decline in performance in the trained domain and (2) although there was a significant decline in performance in the untrained domain, performance was still better than for control Ss. Memory measures suggest that the retention of training effects is due to memory for the rule system rather than to memory for the specific details of the example problems, contrary to what would be expected if Ss were using direct analogies to solve the test problems. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Education,Generalization (Learning),Reasoning,Retention,Transfer (Learning)},
  file = {/Users/dorothybishop/Zotero/storage/S8X4Y8YP/Fong and Nisbett - 1991 - Immediate and delayed transfer of training effects.pdf;/Users/dorothybishop/Zotero/storage/MMU6ZC3U/1991-17515-001.html}
}

@article{forsythe2019,
  title = {Patient {{Engagement In Research}}: Early {{Findings From The Patient}}-{{Centered Outcomes Research Institute}}},
  shorttitle = {Patient {{Engagement In Research}}},
  author = {Forsythe, Laura P. and Carman, Kristin L. and Szydlowski, Victoria and Fayish, Lauren and Davidson, Laurie and Hickam, David H. and Hall, Courtney and Bhat, Geeta and Neu, Denese and Stewart, Lisa and Jalowsky, Maggie and Aronson, Naomi and Anyanwu, Chinenye Ursla},
  year = {2019},
  month = mar,
  journal = {Health Affairs},
  volume = {38},
  number = {3},
  pages = {359--367},
  publisher = {{Health Affairs}},
  issn = {0278-2715},
  doi = {10.1377/hlthaff.2018.05067},
  abstract = {Charged with ensuring that research produces useful evidence to inform health decisions, the Patient-Centered Outcomes Research Institute (PCORI) requires investigators to engage patients and other health care stakeholders, such as clinicians and payers, in the research process. Many PCORI studies result in articles published in peer-reviewed journals that detail research findings and engagement's role in research. To inform practices for engaging patients and others as research partners, we analyzed 126 articles that described engagement approaches and contributions to research. PCORI projects engaged patients and others as consultants and collaborators in determining the study design, selecting study outcomes, tailoring interventions to meet patients' needs and preferences, and enrolling participants. Many articles reported that engagement provided valuable contributions to research feasibility, acceptability, rigor, and relevance, while a few noted trade-offs of engagement. The findings suggest that engagement can support more relevant research through better alignment with patients' and clinicians' real-world needs and concerns.},
  file = {/Users/dorothybishop/Zotero/storage/2LQV6S87/Forsythe et al. - 2019 - Patient Engagement In Research Early Findings Fro.pdf}
}

@article{garfield2007,
  title = {How {{Students Learn Statistics Revisited}}: A {{Current Review}} of {{Research}} on {{Teaching}} and {{Learning Statistics}}},
  shorttitle = {How {{Students Learn Statistics Revisited}}},
  author = {Garfield, Joan and {Ben-Zvi}, Dani},
  year = {2007},
  journal = {International Statistical Review},
  volume = {75},
  number = {3},
  pages = {372--396},
  issn = {1751-5823},
  doi = {10.1111/j.1751-5823.2007.00029.x},
  abstract = {This paper provides an overview of current research on teaching and learning statistics, summarizing studies that have been conducted by researchers from different disciplines and focused on students at all levels. The review is organized by general research questions addressed, and suggests what can be learned from the results of each of these questions. The implications of the research are described in terms of eight principles for learning statistics from Garfield (1995) which are revisited in the light of results from current studies.},
  langid = {english},
  keywords = {statistical reasoning,Statistics education,teaching and learning statistics},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1751-5823.2007.00029.x},
  file = {/Users/dorothybishop/Zotero/storage/DH97C955/Garfield and Ben-Zvi - 2007 - How Students Learn Statistics Revisited A Current.pdf;/Users/dorothybishop/Zotero/storage/ZJJ6Y5K2/j.1751-5823.2007.00029.html}
}

@book{gelman2017,
  title = {Teaching {{Statistics}}: A Bag of Tricks},
  author = {Gelman, Andrew and Nolan, Deborah},
  year = {2017},
  publisher = {{Oxford University Press}},
  address = {{Oxford}}
}

@article{hancock2020,
  title = {Simulation {{Methods}} for {{Teaching Sampling Distributions}}: Should {{Hands}}-on {{Activities Precede}} the {{Computer}}?},
  shorttitle = {Simulation {{Methods}} for {{Teaching Sampling Distributions}}},
  author = {Hancock, Stacey A. and Rummerfield, Wendy},
  year = {2020},
  month = jan,
  journal = {Journal of Statistics Education},
  volume = {28},
  number = {1},
  pages = {9--17},
  publisher = {{Taylor \& Francis}},
  issn = {null},
  doi = {10.1080/10691898.2020.1720551},
  abstract = {Sampling distributions are fundamental to an understanding of statistical inference, yet research shows that students in introductory statistics courses tend to have multiple misconceptions of this important concept. A common instructional method used to address these misconceptions is computer simulation, often preceded by hands-on simulation activities. However, the results on computer simulation activities' effects on student understanding of sampling distributions, and if hands-on simulation activities are necessary, are mixed. In this article, we describe an empirical intervention study in which each of eight discussion sections of an introductory statistics course at a large research university was assigned to one of two in-class activity sequences on sampling distributions: one consisting of computer simulation activities preceded by hands-on simulation using dice, cards, or tickets, and the other comprised of computer simulation alone with the same time-on-task. Using a longitudinal model of changes in standardized exam scores across three exams, we found significant evidence that students who took part in a hands-on activity before computer simulation had better improvement from the first midterm to the final exam, on average, compared to those who only did computer simulations. Supplementary materials for this article are available online.},
  keywords = {Computer simulation methods,Hands-on,Introductory statistics,Longitudinal analysis,Sampling distributions,Tactile},
  annotation = {\_eprint: https://doi.org/10.1080/10691898.2020.1720551},
  file = {/Users/dorothybishop/Zotero/storage/YLJT4W2H/Hancock and Rummerfield - 2020 - Simulation Methods for Teaching Sampling Distribut.pdf;/Users/dorothybishop/Zotero/storage/4ASS7EH8/10691898.2020.html}
}

@article{hodgson2000,
  title = {On {{Simulation}} and the {{Teaching}} of {{Statistics}}},
  author = {Hodgson, Ted and Burke, Maurice},
  year = {2000},
  journal = {Teaching Statistics},
  volume = {22},
  number = {3},
  pages = {91--96},
  issn = {1467-9639},
  doi = {10.1111/1467-9639.00033},
  abstract = {The use of simulation as an instructional tool can promote a deep conceptual understanding of statistics and lead to misunderstandings. Teachers need to be aware of the misconceptions that can arise as a result of simulation and carefully structure classroom activities so as to derive the benefits of this powerful instructional tool.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9639.00033},
  file = {/Users/dorothybishop/Zotero/storage/STXNSBWU/Hodgson and Burke - 2000 - On Simulation and the Teaching of Statistics.pdf;/Users/dorothybishop/Zotero/storage/MC6YK5DL/1467-9639.html}
}

@article{kahneman1972,
  title = {Subjective Probability: A Judgment of Representativeness},
  shorttitle = {Subjective Probability},
  author = {Kahneman, Daniel and Tversky, Amos},
  year = {1972},
  month = jul,
  journal = {Cognitive Psychology},
  volume = {3},
  number = {3},
  pages = {430--454},
  issn = {0010-0285},
  doi = {10.1016/0010-0285(72)90016-3},
  abstract = {This paper explores a heuristic\textemdash representativeness\textemdash according to which the subjective probability of an event, or a sample, is determined by the degree to which it: (i) is similar in essential characteristics to its parent population; and (ii) reflects the salient features of the process by which it is generated. This heuristic is explicated in a series of empirical examples demonstrating predictable and systematic errors in the evaluation of uncertain events. In particular, since sample size does not represent any property of the population, it is expected to have little or no effect on judgment of likelihood. This prediction is confirmed in studies showing that subjective sampling distributions and posterior probability judgments are determined by the most salient characteristic of the sample (e.g., proportion, mean) without regard to the size of the sample. The present heuristic approach is contrasted with the normative (Bayesian) approach to the analysis of the judgment of uncertainty.},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/HGL8PMBW/Kahneman and Tversky - 1972 - Subjective probability A judgment of representati.pdf;/Users/dorothybishop/Zotero/storage/MRBUF4FY/0010028572900163.html}
}

@article{kahneman1982,
  title = {On the Study of Statistical Intuitions},
  author = {Kahneman, Daniel and Tversky, Amos},
  year = {1982},
  journal = {Cognition},
  volume = {11},
  number = {2},
  pages = {123--141},
  issn = {0010-0277},
  doi = {10.1016/0010-0277(82)90022-1},
  abstract = {The study of intuitions and errors in judgment under uncertainty is complicated by several factors: discrepancies between acceptance and application of normative rules; effects of content on the application of rules; Socratic hints that create intuitions while testing them; demand characteristics of within-subject experiments; subjects' interpretations of experimental messages according to standard conversational rules. The positive analysis of a judgmental error in terms of heuristics may be supplemented by a negative analysis, which seeks to explain why the correct rule is not intuitively compelling. A negative analysis of non-regressive prediction is outlined. R\'esum\'e Plusieurs facteurs rendent complexe l'\'etude des intuitions it des erreurs de jugement dans des conditions d'incertitude: l'\'etude des \'ecarts entre l'acceptation et l'application des r\`egles normatives, les effets du contenu sur l'application des r\`egles, les allusions Socratiques cr\'eatrices d'illusions pendant qu'on les teste, les contraintes sp\'ecifiques des exp\'eriences inter-sujets, les interpr\'etations par les sujets des messages exp\'erimentaux selon les r\`egles conversationnelles standards. L'analyse positive d'une erreur de jugement en terne d'heuristiques peut \^etre compl\'et\'ee par une analyse n\'egative pour expliquer pourquoi une r\`egle correcte n'est pas intuitivement contraignante. Une analyse n\'egative de pr\'ediction nonregressive est propos\'ee.},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/AQZ2B9GJ/Kahneman and Tversky - 1982 - On the study of statistical intuitions.pdf;/Users/dorothybishop/Zotero/storage/59KS6HME/0010027782900221.html}
}

@article{kareev1997,
  title = {Through a Narrow Window: Sample Size and the Perception of Correlation},
  shorttitle = {Through a Narrow Window},
  author = {Kareev, Yaakov and Lieberman, Iris and Lev, Miri},
  year = {1997},
  journal = {Journal of Experimental Psychology: General},
  volume = {126},
  number = {3},
  pages = {278--287},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-2222(Electronic),0096-3445(Print)},
  doi = {10.1037/0096-3445.126.3.278},
  abstract = {A theoretical analysis (Y. Kareev, 1995b) of the sampling distribution of correlations led to the surprising conclusion that the use of small samples has a potential advantage for the early detection of a correlation. This is so because the distribution is highly skewed, and the smaller the sample size, the more the distribution is skewed. This article describes 2 experiments that were designed as empirical tests of this conclusion. In Experiment 1 (N\hspace{0.6em}=\hspace{0.6em}112), the authors compared the predictions of participants differing in their working-memory capacity (hence in the size of the samples they were likely to consider). In Experiment 2 (N\hspace{0.6em}=\hspace{0.6em}144), the authors compared the predictions of participants who viewed samples of different sizes, whose size was determined by the authors. The results fully supported Y. Kareev's conclusion: In both experiments, participants with lower capacity (or smaller samples) indeed perceived the correlation as more extreme and were more accurate in their predictions. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Sample Size,Statistical Correlation},
  file = {/Users/dorothybishop/Zotero/storage/YLK59FJ7/1997-05801-004.html}
}

@article{lane2006,
  title = {Interactive Simulations in the Teaching of Statistics: Promise and Pitfalls},
  shorttitle = {Interactive Simulations in the Teaching of Statistics},
  author = {Lane, David and Peres, S.},
  year = {2006},
  month = jan,
  journal = {International Conference on Teaching Statistics},
  volume = {7},
  abstract = {Research on discovery learning and simulation training are reviewed with the focus on principles relevant to the teaching of statistics. Research indicates that even a well-designed simulation is unlikely to be an effective teaching tool unless students' interaction with it is carefully structured. Asking students to anticipate the results of a simulation before interacting with it appears to be an effective instructional technique. Examples of simulations using this technique from the project Online Statistics Education: An Interactive Multimedia Course of Study (http://psych.rice.edu/online\_stat/) are presented. INTRODUCTION Statistical education has made great strides toward making the student a more active learner. Over a decade ago, Cobb (1994) noted that teaching laboratories are increasingly incorporating computer simulations to illustrate important statistical concepts and to allow students to discover important principles themselves. Since the publication of Cobb's article, the widespread availability of the World Wide Web and interactive technologies such as Java and Flash have lead to an explosion in the development of simulations; most of these simulations are easily and freely accessible. A few examples are given below. EXAMPLES OF SIMULATIONS The Rice Virtual Lab in Statistics (RVLS) developed by David Lane (2000) has over 20 Java applets that simulate various statistical concepts (http://www.ruf.rice.edu/\textasciitilde lane/rvls.html). The sampling distribution simulation estimates and plots the sampling distribution of various statistics (http://www.ruf.rice.edu/\textasciitilde lane/stat\_sim/sampling\_dist/index.html). The student specifies the population distribution, sample size, and statistic. Then an animated sample from the population is shown and the statistic is plotted. The student can then repeat the process many times to estimate the sampling distribution. The simulation on the effects of range restriction (Figure 1) shows how the correlation between two variables is affected by the range of the variable plotted on the X-axis (http://www.ruf.rice.edu/\textasciitilde lane/stat\_sim/restricted\_range/index.html). The top portion of Figure 1 shows the entire dataset and allows the student to "restrict the range" by sliding the vertical bars toward the left and/or right. Also shown are the statistics for the entire dataset. The bottom portion of Figure 1 shows the scatter plot without the excluded data and the statistics for these data. By sliding the bars on the top scatter plot, the student can interactively explore the effects of restricting the range.},
  file = {/Users/dorothybishop/Zotero/storage/DB99NCU8/Lane and Peres - 2006 - Interactive simulations in the teaching of statist.pdf}
}

@article{makowski2018,
  title = {The {{Psycho Package}}: An Efficient and Publishing-Oriented Workflow for  Psychological Science},
  author = {Makowski, D},
  year = {2018},
  journal = {Journal of Open Source Software},
  volume = {3},
  number = {22},
  pages = {470}
}

@article{meyer2001,
  title = {Psychological Testing and Psychological Assessment: A Review of Evidence and Issues},
  shorttitle = {Psychological Testing and Psychological Assessment},
  author = {Meyer, Gregory J. and Finn, Stephen E. and Eyde, Lorraine D. and Kay, Gary G. and Moreland, Kevin L. and Dies, Robert R. and Eisman, Elena J. and Kubiszyn, Tom W. and Reed, Geoffrey M.},
  year = {2001},
  journal = {American Psychologist},
  volume = {56},
  number = {2},
  pages = {128--165},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1935-990X(Electronic),0003-066X(Print)},
  doi = {10.1037/0003-066X.56.2.128},
  abstract = {This article summarizes evidence and issues associated with psychological assessment. Data from more than 125 meta-analyses on test validity and 800 samples examining multimethod assessment suggest 4 general conclusions: (a) Psychological test validity is strong and compelling, (b) psychological test validity is comparable to medical test validity, (c) distinct assessment methods provide unique sources of information, and (d) clinicians who rely exclusively on interviews are prone to incomplete understandings. Following principles for optimal nomothetic research, the authors suggest that a multimethod assessment battery provides a structured means for skilled clinicians to maximize the validity of individualized assessments. Future investigations should move beyond an examination of test scales to focus more on the role of psychologists who use tests as helpful tools to furnish patients and referral sources with professional consultation. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Psychological Assessment,Test Validity,Testing},
  file = {/Users/dorothybishop/Zotero/storage/W5PUBWBZ/2001-00159-003.html}
}

@article{mills2004,
  title = {Learning Abstract Statistics Concepts Using Simulation},
  author = {Mills, Jamie D},
  year = {2004},
  journal = {Educational Research Quarterly},
  volume = {28},
  number = {4},
  pages = {18--33},
  file = {/Users/dorothybishop/Zotero/storage/L23PJERA/Mills - 2004 - Learning abstract statistics concepts using simula.pdf}
}

@article{morewedge2015,
  title = {Debiasing {{Decisions}}: Improved {{Decision Making With}} a {{Single Training Intervention}}},
  shorttitle = {Debiasing {{Decisions}}},
  author = {Morewedge, Carey K. and Yoon, Haewon and Scopelliti, Irene and Symborski, Carl W. and Korris, James H. and Kassam, Karim S.},
  year = {2015},
  month = oct,
  journal = {Policy Insights from the Behavioral and Brain Sciences},
  volume = {2},
  number = {1},
  pages = {129--140},
  publisher = {{SAGE Publications}},
  issn = {2372-7322},
  doi = {10.1177/2372732215600886},
  abstract = {From failures of intelligence analysis to misguided beliefs about vaccinations, biased judgment and decision making contributes to problems in policy, business, medicine, law, education, and private life. Early attempts to reduce decision biases with training met with little success, leading scientists and policy makers to focus on debiasing by using incentives and changes in the presentation and elicitation of decisions. We report the results of two longitudinal experiments that found medium to large effects of one-shot debiasing training interventions. Participants received a single training intervention, played a computer game or watched an instructional video, which addressed biases critical to intelligence analysis (in Experiment 1: bias blind spot, confirmation bias, and fundamental attribution error; in Experiment 2: anchoring, representativeness, and social projection). Both kinds of interventions produced medium to large debiasing effects immediately (games {$\geq$} -31.94\% and videos {$\geq$} -18.60\%) that persisted at least 2 months later (games {$\geq$} -23.57\% and videos {$\geq$} -19.20\%). Games that provided personalized feedback and practice produced larger effects than did videos. Debiasing effects were domain general: bias reduction occurred across problems in different contexts, and problem formats that were taught and not taught in the interventions. The results suggest that a single training intervention can improve decision making. We suggest its use alongside improved incentives, information presentation, and nudges to reduce costly errors associated with biased judgments and decisions.},
  langid = {english},
  keywords = {cognitive bias,debiasing,decisions,feedback,incentives,judgment and decision making,nudges,training},
  file = {/Users/dorothybishop/Zotero/storage/FSA26M42/Morewedge et al. - 2015 - Debiasing Decisions Improved Decision Making With.pdf}
}

@misc{morey2019,
  title = {Use of Significance Test Logic by Scientists in a Novel Reasoning Task},
  author = {Morey, Richard D. and Hoekstra, Rink},
  year = {2019},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/sxj3r},
  abstract = {Although statistical significance testing is one of the most widely-used techniques across science, previous research has suggested that scientists have a poor understanding of how it works. If scientists misunderstand one of their primary inferential tools the implications are dramatic: potentially unchecked, unjustified conclusions and wasted resources. Scientists' apparent difficulties with significance testing have led to calls for its abandonment or increased reliance on alternative tools, which would represent a substantial, untested, shift in scientific practice. However, if scientists' understanding of significance testing is truly as poor as thought, one could argue such drastic action is required. We show using a novel experimental method that scientists do, in fact, understand the logic of significance testing and can use it effectively. This suggests that scientists may not be as statistically-challenged as often believed, and that reforms should take this into account.},
  keywords = {cognitive psychology,Cognitive Psychology,decision making,judgement,Judgment and Decision Making,significance testing,Social and Behavioral Sciences,statistics},
  file = {/Users/dorothybishop/Zotero/storage/GVTRXUBT/Morey and Hoekstra - 2019 - Use of significance test logic by scientists in a .pdf}
}

@article{nisbett1987,
  title = {Teaching Reasoning},
  author = {Nisbett, R. E. and Fong, G. T. and Lehman, D. R. and Cheng, P. W.},
  year = {1987},
  month = oct,
  journal = {Science},
  volume = {238},
  number = {4827},
  pages = {625--631},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.3672116},
  abstract = {Twentieth-century psychologists have been pessimistic about teaching reasoning, prevailing opinion suggesting that people may possess only domain-specific rules, rather than abstract rules; this would mean that training a rule in one domain would not produce generalization to other domains. Alternatively, it was thought that people might possess abstract rules (such as logical ones) but that these are induced developmentally through self-discovery methods and cannot be trained. Research suggests a much more optimistic view: even brief formal training in inferential rules may enhance their use for reasoning about everyday life events. Previous theorists may have been mistaken about trainability, in part because they misidentified the kind of rules that people use naturally.},
  chapter = {Articles},
  copyright = {\textcopyright{} 1987},
  langid = {english},
  pmid = {3672116},
  file = {/Users/dorothybishop/Zotero/storage/3LQP7Y9T/Nisbett et al. - 1987 - Teaching reasoning.pdf;/Users/dorothybishop/Zotero/storage/7YXNI6VI/625.html}
}

@article{oakes2017,
  title = {Sample Size, Statistical Power, and False Conclusions in Infant Looking-Time Research},
  author = {Oakes, Lisa M.},
  year = {2017},
  journal = {Infancy},
  volume = {22},
  number = {4},
  pages = {436--469},
  issn = {1532-7078},
  doi = {10.1111/infa.12186},
  abstract = {Infant research is hard. It is difficult, expensive, and time-consuming to identify, recruit, and test infants. As a result, ours is a field of small sample sizes. Many studies using infant looking time as a measure have samples of 8\textendash 12 infants per cell, and studies with more than 24 infants per cell are uncommon. This paper examines the effect of such sample sizes on statistical power and the conclusions drawn from infant looking-time research. An examination of the state of the current literature suggests that most published looking-time studies have low power, which leads in the long run to an increase in both false positive and false negative results. Three data sets with relatively large samples ({$>$}30 infants) were used to simulate experiments with smaller sample sizes; 1,000 random subsamples of 8, 12, 16, 20, and 24 infants from the overall samples were selected, making it possible to examine the systematic effect of sample size on the results. This approach revealed that despite clear results with the original large samples, the results with smaller subsamples were highly variable, yielding both false positive and false negative outcomes. Finally, a number of emerging possible solutions are discussed.},
  copyright = {Copyright \textcopyright{} International Congress of Infant Studies (ICIS)},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/infa.12186},
  file = {/Users/dorothybishop/Zotero/storage/9ZNB6SDR/Oakes - 2017 - Sample Size, Statistical Power, and False Conclusi.pdf;/Users/dorothybishop/Zotero/storage/WART2LBD/infa.html}
}

@article{olson1976,
  title = {Some Apparent Violations of the Representativeness Heuristic in Human Judgment},
  author = {Olson, Chester L.},
  year = {1976},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {2},
  number = {4},
  pages = {599--608},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1277},
  doi = {10.1037/0096-1523.2.4.599},
  abstract = {According to the representativeness heuristic, the probability that an element is an exemplar of a given class is judged to be high to the extent that the element is representative of the class with respect to its salient features. In 3 experiments involving situations previously called upon in support of representativeness theory, questionnaire responses from 265 university students demonstrated systematic biases that deviated sharply from the obvious predictions of the theory. One such bias, the students' misinterpretation of proportion information as absolute-number information, is comparable to Piaget's concrete operations. The implications for representativeness theory are discussed in terms of the theory's relationship to concrete thinking, the importance of task characteristics, and the difficulty of a priori specifications of the salient features with respect to which representativeness is assessed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Cognitive Processes,Heuristics,Judgment,Piagetian Tasks,Thinking},
  file = {/Users/dorothybishop/Zotero/storage/CW39LPDK/1977-04935-001.html}
}

@misc{pederson2020,
  title = {Gganimate: A {{Grammar}} of {{Animated Graphics}}. {{R}} Package   Version 1.0.7.},
  author = {Pederson, T. L. and Robinson, D},
  year = {2020}
}

@article{poldrack2017,
  title = {Scanning the Horizon: Towards Transparent and Reproducible Neuroimaging Research},
  shorttitle = {Scanning the Horizon},
  author = {Poldrack, Russell A. and Baker, Chris I. and Durnez, Joke and Gorgolewski, Krzysztof J. and Matthews, Paul M. and Munaf{\`o}, Marcus R. and Nichols, Thomas E. and Poline, Jean-Baptiste and Vul, Edward and Yarkoni, Tal},
  year = {2017},
  journal = {Nature Reviews Neuroscience},
  volume = {18},
  number = {2},
  pages = {115--126},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn.2016.167},
  abstract = {There is growing concern about the reproducibility of scientific research, and neuroimaging research suffers from many features that are thought to lead to high levels of false results.Statistical power of neuroimaging studies has increased over time but remains relatively low, especially for group comparison studies. An analysis of effect sizes in the Human Connectome Project demonstrates that most functional MRI studies are not sufficiently powered to find reasonable effect sizes.Neuroimaging analysis has a high degree of flexibility in analysis methods, which can lead to inflated false-positive rates unless controlled for. Pre-registration of analysis plans and clear delineation of hypothesis-driven and exploratory research are potential solutions to this problem.The use of appropriate corrections for multiple tests has increased, but some common methods can have highly inflated false-positive rates. The use of non-parametric methods is encouraged to provide accurate correction for multiple tests.Software errors have the potential to lead to incorrect or irreproducible results. The adoption of improved software engineering methods and software testing strategies can help to reduce such problems.Reproducibility will be improved through greater transparency in methods reporting and through increased sharing of data and code.},
  copyright = {2017 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/VZ39FJ3T/Poldrack et al. - 2017 - Scanning the horizon towards transparent and repr.pdf;/Users/dorothybishop/Zotero/storage/TW8RJ3VY/nrn.2016.html}
}

@misc{rcoreteam2020,
  title = {R: A Language and Environment for Statistical Computing.},
  author = {R Core Team,},
  year = {2020},
  publisher = {{R Foundation for Statistical Computing, Vienna, Austria}}
}

@article{sedlmeier1989,
  title = {Do Studies of Statistical Power Have an Effect on the Power of Studies?},
  author = {Sedlmeier, Peter and Gigerenzer, Gerd},
  year = {1989},
  journal = {Psychological Bulletin},
  volume = {105},
  number = {2},
  pages = {309--316},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1455(Electronic),0033-2909(Print)},
  doi = {10.1037/0033-2909.105.2.309},
  abstract = {The long-term impact of studies of statistical power is investigated using J. Cohen's (1962) pioneering work as an example. We argue that the impact is nil; the power of studies in the same journal that Cohen reviewed (now the Journal of Abnormal Psychology) has not increased over the past 24 years. In 1960 the median power (i.e., the probability that a significant result will be obtained if there is a true effect) was .46 for a medium size effect, whereas in 1984 it was only .37. The decline of power is a result of alpha-adjusted procedures. Low power seems to go unnoticed: only 2 out of 64 experiments mentioned power, and it was never estimated. Nonsignificance was generally interpreted as confirmation of the null hypothesis (if this was the research hypothesis), although the median power was as low as .25 in these cases. We discuss reasons for the ongoing neglect of power. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Effect Size (Statistical),Statistical Significance},
  file = {/Users/dorothybishop/Zotero/storage/6VVLX3NX/Sedlmeier and Gigerenzer - 1989 - Do studies of statistical power have an effect on .pdf;/Users/dorothybishop/Zotero/storage/ZJEIWDPE/1989-21174-001.html}
}

@article{sedlmeier1997,
  title = {Intuitions about Sample Size: The Empirical Law of Large Numbers},
  shorttitle = {Intuitions about Sample Size},
  author = {Sedlmeier, Peter and Gigerenzer, Gerd},
  year = {1997},
  journal = {Journal of Behavioral Decision Making},
  volume = {10},
  number = {1},
  pages = {33--51},
  issn = {1099-0771},
  doi = {10.1002/(SICI)1099-0771(199703)10:1<33::AID-BDM244>3.0.CO;2-6},
  abstract = {According to Jacob Bernoulli, even the `stupidest man' knows that the larger one's sample of observations, the more confidence one can have in being close to the truth about the phenomenon observed. Two-and-a-half centuries later, psychologists empirically tested people's intuitions about sample size. One group of such studies found participants attentive to sample size; another found participants ignoring it. We suggest an explanation for a substantial part of these inconsistent findings. We propose the hypothesis that human intuition conforms to the `empirical law of large numbers' and distinguish between two kinds of tasks\textendash one that can be solved by this intuition (frequency distributions) and one for which it is not sufficient (sampling distributions). A review of the literature reveals that this distinction can explain a substantial part of the apparently inconsistent results. \textcopyright{} 1997 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 1997 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {frequency distribution,law of large numbers,sample size,sampling distribution},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291099-0771\%28199703\%2910\%3A1\%3C33\%3A\%3AAID-BDM244\%3E3.0.CO\%3B2-6},
  file = {/Users/dorothybishop/Zotero/storage/PLCJRHUK/Sedlmeier and Gigerenzer - 1997 - Intuitions about sample size the empirical law of.pdf;/Users/dorothybishop/Zotero/storage/JAHIWXUY/(SICI)1099-0771(199703)10133AID-BDM2443.0.html}
}

@article{sigal2016,
  title = {Play {{It Again}}: Teaching {{Statistics With Monte Carlo Simulation}}},
  shorttitle = {Play {{It Again}}},
  author = {Sigal, Matthew J. and Chalmers, R. Philip},
  year = {2016},
  month = sep,
  journal = {Journal of Statistics Education},
  volume = {24},
  number = {3},
  pages = {136--156},
  publisher = {{Taylor \& Francis}},
  issn = {null},
  doi = {10.1080/10691898.2016.1246953},
  abstract = {Monte Carlo simulations (MCSs) provide important information about statistical phenomena that would be impossible to assess otherwise. This article introduces MCS methods and their applications to research and statistical pedagogy using a novel software package for the R Project for Statistical Computing constructed to lessen the often steep learning curve when organizing simulation code. A primary goal of this article is to demonstrate how well-suited MCS designs are to classroom demonstrations, and how they provide a hands-on method for students to become acquainted with complex statistical concepts. In this article, essential programming aspects for writing MCS code in R are overviewed, multiple applied examples with relevant code are provided, and the benefits of using a generate\textendash analyze\textendash summarize coding structure over the typical ``for-loop'' strategy are discussed.},
  keywords = {Active learning,R,Simulation,Statistical computing},
  file = {/Users/dorothybishop/Zotero/storage/3L79ZHHJ/Sigal and Chalmers - 2016 - Play It Again Teaching Statistics With Monte Carl.pdf}
}

@article{simpson2021,
  title = {Analysis of Expression {{Quantitative Trait Loci}} for {{NLGN4X}} in Relation to Language and Neurodevelopmental Function: An Exploratory Analysis Using {{FUSION}} and {{GTEx}} Workflows},
  shorttitle = {Analysis of Expression {{Quantitative Trait Loci}} for {{NLGN4X}} in Relation to Language and Neurodevelopmental Function},
  author = {Simpson, Nuala H. and Bishop, Dorothy V. M. and Newbury, Dianne F.},
  year = {2021},
  month = jun,
  journal = {bioRxiv},
  pages = {2021.06.29.450314},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.06.29.450314},
  abstract = {{$<$}p{$>$}Language disorders in children are highly heritable, but progress in identifying genetic variants that contribute to language phenotypes has been slow. Here we applied a novel approach by identifying SNPs that are associated with gene expression in the brain, taking as our focus a gene on the X chromosome, \emph{NLGN4X}, which has been postulated to play a role in neurodevelopmental disorders affecting language and communication. We found no significant associations between expression quantitative trait loci (eQTLs) and phenotypes of nonword repetition, general language ability or neurodevelopmental disorder in two samples of twin children, who had been selected for a relatively high rate of language problems. We report here our experiences with two methods, FUSION and GTEx, for eQTL analysis. It is likely that our null result represents a true negative, but for the interest of others interested in using these methods, we note specific challenges encountered in applying this approach to our data: a) complications associated with studying a gene on the X chromosome; b) lack of agreement between expression estimates from FUSION and GTEx; c) software compatibility issues with different versions of the R programming language.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/68XTKZ7E/Simpson et al. - 2021 - Analysis of expression Quantitative Trait Loci for.pdf;/Users/dorothybishop/Zotero/storage/ZCZ5N3WV/2021.06.29.html}
}

@article{smaldino2016,
  title = {The Natural Selection of Bad Science},
  author = {Smaldino, Paul E. and McElreath, Richard},
  year = {2016},
  journal = {Royal Society Open Science},
  volume = {3},
  number = {9},
  pages = {160384},
  publisher = {{Royal Society}},
  doi = {10.1098/rsos.160384},
  abstract = {Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing\textemdash no deliberate cheating nor loafing\textemdash by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more `progeny,' such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.},
  file = {/Users/dorothybishop/Zotero/storage/WZLTX9PG/Smaldino and McElreath - The natural selection of bad science.pdf;/Users/dorothybishop/Zotero/storage/ENZ3YZTB/rsos.html}
}

@article{stanley2018,
  title = {What Meta-Analyses Reveal about the Replicability of Psychological Research},
  author = {Stanley, T. D. and Carter, Evan C. and Doucouliagos, Hristos},
  year = {2018},
  journal = {Psychological Bulletin},
  volume = {144},
  number = {12},
  pages = {1325--1346},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1455(Electronic),0033-2909(Print)},
  doi = {10.1037/bul0000169},
  abstract = {[Correction Notice: An Erratum for this article was reported in Vol 145(7) of Psychological Bulletin (see record 2019-32510-001). In the article, the Open Science Framework (OSF) URL for the data has now been included in the author note. The online version of this article has been corrected.] Can recent failures to replicate psychological research be explained by typical magnitudes of statistical power, bias or heterogeneity? A large survey of 12,065 estimated effect sizes from 200 meta-analyses and nearly 8,000 papers is used to assess these key dimensions of replicability. First, our survey finds that psychological research is, on average, afflicted with low statistical power. The median of median power across these 200 areas of research is about 36\%, and only about 8\% of studies have adequate power (using Cohen's 80\% convention). Second, the median proportion of the observed variation among reported effect sizes attributed to heterogeneity is 74\% (I2). Heterogeneity of this magnitude makes it unlikely that the typical psychological study can be closely replicated when replication is defined as study-level null hypothesis significance testing. Third, the good news is that we find only a small amount of average residual reporting bias, allaying some of the often-expressed concerns about the reach of publication bias and questionable research practices. Nonetheless, the low power and high heterogeneity that our survey finds fully explain recent difficulties to replicate highly regarded psychological studies and reveal challenges for scientific progress in psychology. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Effect Size (Statistical),Experimental Replication,Experimentation,Experimenter Bias,Magnitude Estimation,Median,Meta Analysis,Null Hypothesis Testing,Psychology,Statistical Power},
  file = {/Users/dorothybishop/Zotero/storage/2278WSHM/2018-51211-001.html}
}

@article{steel2019,
  title = {Beyond Calculations: A Course in Statistical Thinking},
  shorttitle = {Beyond {{Calculations}}},
  author = {Steel, E. Ashley and Liermann, Martin and Guttorp, Peter},
  year = {2019},
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {392--401},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1505657},
  abstract = {Statisticians are in general agreement that there are flaws in how science is currently practiced; there is less agreement in how to make repairs. Our prescription for a Post-p {$<$} 0.05 Era is to develop and teach courses that expand our view of what constitutes the domain of statistics and thereby bridge undergraduate statistics coursework and the graduate student experience of applying statistics in research. Such courses can speed up the process of gaining statistical wisdom by giving students insight into the human propensity to make statistical errors, the meaning of a single test within a research project, ways in which p-values work and don't work as expected, the role of statistics in the lifecycle of science, and best practices for statistical communication. The course we have developed follows the story of how we use data to understand the world, leveraging simulation-based approaches to perform customized analyses and evaluate the behavior of statistical procedures. We provide ideas for expanding beyond the traditional classroom, two example activities, and a course syllabus as well as the set of statistical best practices for creating and consuming scientific information that we develop during the course.},
  keywords = {p-Values,Probabilistic thinking,Simulation-based testing,Statistical intuition Statistics education},
  annotation = {\_eprint: https://doi.org/10.1080/00031305.2018.1505657},
  file = {/Users/dorothybishop/Zotero/storage/VE792TZ7/Steel et al. - 2019 - Beyond Calculations A Course in Statistical Think.pdf;/Users/dorothybishop/Zotero/storage/BYESHDJV/00031305.2018.html}
}

@article{tintle2015,
  title = {Combating {{Anti}}-{{Statistical Thinking Using Simulation}}-{{Based Methods Throughout}} the {{Undergraduate Curriculum}}},
  author = {Tintle, Nathan and Chance, Beth and Cobb, George and Roy, Soma and Swanson, Todd and VanderStoep, Jill},
  year = {2015},
  month = oct,
  journal = {The American Statistician},
  volume = {69},
  number = {4},
  pages = {362--370},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.2015.1081619},
  abstract = {The use of simulation-based methods for introducing inferen-ce is growing in popularity for the Stat 101 course, due in part to increasing evidence of the methods ability to improve studen-ts' statistical thinking. This impact comes from simulation-based methods (a) clearly presenting the overarching logic of inference, (b) strengthening ties between statistics and probability/mathematical concepts, (c) encouraging a focus on the entire research process, (d) facilitating student thinking about advanced statistical concepts, (e) allowing more time to explore, do, and talk about real research and messy data, and (f) acting as a firm-er foundation on which to build statistical intuition. Thus, we argue that simulation-based inference should be an entry point to an undergraduate statistics program for all students, and that simulation-based inference should be used throughout all under-graduate statistics courses. To achieve this goal and fully recognize the benefits of simulation-based inference on the undergraduate statistics program, we will need to break free of historical forces tying undergraduate statistics curricula to mathematics, consider radical and innovative new pedagogical approaches in our courses, fully implement assessment-driven content innovations, and embrace computation throughout the curriculum. [Received December 2014. Revised July 2015]},
  keywords = {Bootstrap,Education,Permutation,Randomization.},
  file = {/Users/dorothybishop/Zotero/storage/R2QL83XW/Tintle et al. - 2015 - Combating Anti-Statistical Thinking Using Simulati.pdf}
}

@article{tversky1971,
  title = {Belief in the Law of Small Numbers},
  author = {Tversky, Amos and Kahneman, Daniel},
  year = {1971},
  journal = {Psychological Bulletin},
  volume = {76},
  number = {2},
  pages = {105--110},
  file = {/Users/dorothybishop/Zotero/storage/VLCAQD7N/HTML.html}
}

@article{well1990,
  title = {Understanding the Effects of Sample Size on the Variability of the Mean},
  author = {Well, Arnold D and Pollatsek, Alexander and Boyce, Susan J},
  year = {1990},
  month = dec,
  journal = {Organizational Behavior and Human Decision Processes},
  volume = {47},
  number = {2},
  pages = {289--312},
  issn = {0749-5978},
  doi = {10.1016/0749-5978(90)90040-G},
  abstract = {In the first three experiments, we attempted to learn more about subjects' understanding of the importance of sample size by systematically changing aspects of the problems we gave to subjects. In a fourth study, understanding of the effects of sample size was tested as subjects went through a computerassisted training procedure that dealt with random sampling and the sampling distribution of the mean. Subjects used sample size information more appropriately for problems that were stated in terms of the accuracy of the sample average or the center of the sampling distribution than for problems stated in terms of the tails of the sampling distribution. Apparently, people understand that the means of larger samples are more likely to resemble the population mean but not the implications of this fact for the variability of the mean. The fourth experiment showed that although instruction about the sampling distribution of the mean led to better understanding of the effects of sample size, subjects were still unable to make correct inferences about the variability of the mean. The appreciation that people have for some aspects of the law of large numbers does not seem to result from an in-depth understanding of the relation between sample size and variability.},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/J9LYGXMZ/Well et al. - 1990 - Understanding the effects of sample size on the va.pdf;/Users/dorothybishop/Zotero/storage/G88C33BZ/074959789090040G.html}
}

@book{wickham2016,
  title = {Ggplot2: Elegant {{Graphics}} for {{Data Analysis}}.},
  author = {Wickham, H},
  year = {2016},
  publisher = {{Springer Verlag}},
  address = {{New York}}
}

@article{yoon2021,
  title = {Decision Making Can Be Improved through Observational Learning},
  author = {Yoon, Haewon and Scopelliti, Irene and Morewedge, Carey K.},
  year = {2021},
  month = jan,
  journal = {Organizational Behavior and Human Decision Processes},
  volume = {162},
  pages = {155--188},
  issn = {0749-5978},
  doi = {10.1016/j.obhdp.2020.10.011},
  abstract = {Observational learning can debias judgment and decision making. One-shot observational learning-based training interventions (akin to ``hot seating'') can produce reductions in cognitive biases in the laboratory (i.e., anchoring, representativeness, and social projection), and successfully teach a decision rule that increases advice taking in a weight on advice paradigm (i.e., the averaging principle). These interventions improve judgment, rule learning, and advice taking more than practice. We find observational learning-based interventions can be as effective as information-based interventions. Their effects are additive for advice taking, and for accuracy when advice is algorithmically optimized. As found in the organizational learning literature, explicit knowledge transferred through information appears to reduce the stickiness of tacit knowledge transferred through observational learning. Moreover, observational learning appears to be a unique debiasing training strategy, an addition to the four proposed by Fischhoff (1982). We also report new scales measuring individual differences in anchoring, representativeness heuristics, and social projection.},
  langid = {english},
  keywords = {Cognitive bias,Debiasing,Knowledge transfer,Social learning,Tacit knowledge,Weight on advice},
  file = {/Users/dorothybishop/Zotero/storage/Q7W8A8G9/Yoon et al. - 2021 - Decision making can be improved through observatio.pdf;/Users/dorothybishop/Zotero/storage/2KL9I2EU/S0749597820303976.html}
}


