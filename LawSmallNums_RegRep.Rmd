---
title: 'Stage 1 Registered Report: Can we shift belief in the ''Law of Small Numbers''?'
author: D V M Bishop^1, Jackie Thompson^2 & Adam Parker^1
date: 
output:
  word_document: default
  html_document:
    df_print: paged

bibliography: LawSmallNums.bib 
---
output:
  word_document:
    reference_docx: StylesTemplate.docx
    keep_md: true
csl:apa
```{r setup, include=FALSE,echo=FALSE}

# NB This script reads output from two other scripts
# 'gorilla_spreadsheets/spreadsheet1.csv' (created by make_gifs_datasim_script.R)
# 'powsummary_1000_A12_pgood0.5_0.66_props0_0.25_0.33_0.5_N50_75_100.csv' (created by simulate_power_proportions.rmd)
#These .csv files are provided on OSF, or can be created by re-running the scripts listed above.
#The spreadsheet1.csv sheet should be saved in a subdirectory called 'gorilla_spreadsheets'


#6 Jun 2021 - updated with new reward schedule and new pilot data.


#https://fishandwhistle.net/post/2020/getting-started-zotero-better-bibtex-rmarkdown/

date()
knitr::opts_chunk$set(echo = TRUE)
require(beeswarm)
require(tidyverse)
library(RColorBrewer) #used later for 'zone of uncertainty' plots

library(dplyr)
library(yarrr)
library(ggplot2)
library(lme4)
library(MASS)
library(effects)
library(simr)
library("ggbeeswarm")
library(pwr)
library(flextable)
tabnumber <- 0 #for counting tables.
fignumber <- 0 #for counting figures
subn <- c(50,75,100)
allarrayvals <- c( 1,1,2,2,2,2,3,3,3,3,3,3,4,4,4,5,5,5,6,6) #distribution of array values used in simulation for block 1

```
Affiliations:  
1: Department of Experimental Psychology, University of Oxford, Anna Watts Building, Woodstock Road, Oxford, OX2 6GG. 

2: School of Experimental Psychology, University of Bristol, The Priory Road Complex, Priory Road, Clifton, BS8 1TU.  
  
Acknowledgment: This work was supported by Wellcome Trust Programme Grant no. 082498/Z/07/Z.

## Abstract 
<!---100 word limit for RSOS--->
Even statistically-trained people show ‘sample size neglect,’ i.e. they underestimate how much the variability of mean estimates increases as sample size decreases. We consider whether judging whether simulated datasets come from different populations can help people overcome this bias. Before and after training with simulated datasets, participants are given parallel forms of a 12-item quiz testing statistical knowledge, including six items testing sample size neglect. We hypothesise that the training will counteract sample size neglect in some participants, and those who show learning will also show improvement on quiz items relating to sample size neglect.  

#### Keywords:  statistical reasoning, power, online training, sample size neglect


 




```{r makefig1, include=FALSE, echo=FALSE}
#This chunk just generates figure 1 - it could probably be made more efficient
fignumber <- fignumber + 1
samplesizes <- c(10,60)
set.seed(2)
mymean <- 178 #mean  - can vary this - here this is in raw units
mysd <- 10 
nsim <- 6
plotname <- paste0('Fig1.tiff')
tiff(plotname, width = 8, height = 5, units = 'in', res = 300)
par(mfrow=c(1,2))
for (i in samplesizes){
    thisdat<-rnorm(i*nsim,mymean,mysd)
    thisset <- rep(1:nsim,i)
    alldat<-data.frame(cbind(thisdat,thisset))
    colnames(alldat)<-c('Height','Set')
    beeswarm(thisdat~thisset,cex=.75,pch=16,col='darkgrey',ylim=c(160,200),xlab='Sample',ylab='Height (cm)',main=paste0('Sample size ',i))
    mymeans<-aggregate(alldat$Height,by=list(alldat$Set),FUN=mean)
    for (n in 1:nsim){
    segments((n-.3),mymeans$x[n],(n+.3),mymeans$x[n],lwd=2,col='red')
    }
    abline(h=mymean,lty=1,col="blue")
}


dev.off()
```
## Introduction

Compared to laypeople, scientists receive extensive training to help them understand and appropriately address the uncertainty of evidence. Yet many scientists fall short in their understanding of statistical concepts. One cognitive bias demonstrated by @tversky1971 is the 'belief in the law of small numbers'. This refers to the tendency to overestimate the stability of estimates that come from small samples - which, following @yoon2021, we shall term 'sample size neglect'. For instance, as shown in Figure 1, if repeatedly sampling `r samplesizes[1]` men from a population, the mean height of the sample will be far more variable than when repeatedly sampling `r samplesizes[2]` men. People understand that sample size does not affect the expected mean value, but they tend not to appreciate that it has a large effect on the standard error of the mean (i.e. variability of the red bars). This has implications for understanding of statistical power, i.e. the numerical relationship between sample size and ability to detect a true effect. Sample size neglect can help explain why so many studies in psychology, and indeed many other scientific disciplines, are underpowered. 

![Figure `r fignumber`. Six independent samples of simulated male height, each with sample size of 10 or 60. Each point represents the height of one male, drawn from a population with mean 178 cm and SD 10 cm, and the red bar is the mean for that sample. The horizontal blue line represents the population mean.](Fig1.tiff)

In 1962, @cohen1962 embarked on a project of improving psychologists' understanding of statistical power, providing tools to help people compute power and documenting the extent of underpowered studies in social psychology, with the aim of reducing waste in research efforts. He analysed 70 studies published in the Journal of Abnormal and Social Psychology and found that mean power to detect small effects was 0.18, to detect medium effects was 0.48 and to detect large effects was 0.83. Given that most effects in this field are small or medium, this indicated serious limitations of study design.  However, 27 years later, @sedlmeier1989 reported that things had not changed at all. And in 2016, similar conclusions were drawn from a large review of studies in social and behavioral sciences up to 2011 (@smaldino2016). In 2018 a review concluded that low power remains a major factor explaining poor replicability of highly cited studies in psychology (@stanley2018). A wide range of areas are affected, from neuroscience (@poldrack2017), to infancy research (@oakes2017). Why, despite years of attempts to improve the dismal record of low power in psychology, do researchers persist in performing underpowered studies? We suspect the explanation may go beyond lack of training, and reflect the influence of sample size neglect, which leads us to have intuitions about sample size that are at odds with reality. Consider this example (@tversky1971, p.105):

_‘Suppose you have run an experiment on 20 participants, and have obtained a significant result which confirms your theory (z= 2.23, p < .05, two-tailed). You now have cause to run an additional group of 10 participants. What do you think the probability is that the results will be significant, by a one-tailed test, separately for this group?_

Tversky and Kahneman reported that the majority of researchers who responded to this question were wrong in stating that the probability is somewhere around .85, while only 9 out of 84 researchers gave a more accurate answer (i.e. between .40 and .60). This is thought to reflect inaccurate beliefs in sampling that have unfortunate consequences in the course of scientific enquiry. Researchers view randomly drawn samples as highly representative of the population to a greater extent than theory predicts, at least for small samples.  

If we are to tackle the problem of wasteful and misleading underpowered studies, we need to find ways to overcome sample size neglect. There is some evidence that debiasing training can help: @yoon2021 showed improvements on a scale measuring this construct after either direct instruction, direct playing of a game designed to train awareness, or observing another person play the game. However, the scale items assessed rather broad aspects of generalising from a few instances, e.g. respond on a 7-point scale: "Micah's 10-year-old daughter Felicia scores two goals in her very first soccer game. Based on this, Micah proudly predicts that Felicia will be the top scorer for her team for the year (25 games). How confident are you in Micah's prediction?" Our goal was to go beyond a general appreciation of the dangers of relying on small samples to help scientists obtain a more intuitive understanding of statistical power, 

Informally, we have found that exposing students to simulations that allow them to visualize the variation between samples of different sizes can help counteract over-reliance on small samples to evaluate hypotheses. By generating datasets with known effect sizes and drawing random samples and subjecting them to statistical tests, students can learn to appreciate the ease with which we miss a true effect if the sample size is small. Simulations are a core aspect of a course outlined by @steel2019 which trains statistical thinking in undergraduates, using simulations to help interpret patterns in data and to evaluate statistical power. Although their course appears successful in training statistical thinking in the long term, they did not specifically evaluate the utility of simulations in counteracting sample size neglect. That is the goal of the current study.  

In addition to training, simulations have been used to examine the decisions made by researchers. @morey2019 used simulations to examine scientists’ understanding of the logic of significance testing. In their task, scientists were asked to perform a series of experiments to judge which two groups of elves could make more toys based on a randomly assigned group difference between 0 and 1 standard deviations. Sample size and the test statistic were not shown to participants: instead they were shown displays that represented numerical information in terms of colour and location, with an opportunity also to view "random shuffle" displays where they were told there was no true effect. Of the 136 participants for whom the null hypothesis was true, 86% correctly indicated no effect. When there was a true effect, correct decisions increased as a function of effect size.  When the effect size was .3, accuracy was approximately 80%. For larger effect sizes, accuracy approached ceiling. When asked about the heuristics that they applied, 72% indicated using strong significance testing strategies. Thus, Morey and Hoeskstra highlighted the potential utility of simulation-based training to answer questions about how researchers use information. However, their study only indirectly addressed sample size neglect, because participants were not told what the sample size was, and were encouraged to explore the sampling distribution for means when the effect size was zero, which made it relatively easy to see whether a given point fell within that distribution. Our focus is rather on the more lifelike situation when the participant knows the sample size, and has to consider whether an observed distribution of scores is more likely to have come from a population with a true or null effect. As well as evaluating the efficacy of training on statistical judgements, we will consider whether participants show evidence of adopting specific heuristics, such as waiting for a given sample size before making a judgement, or simply basing responses on observed effect size.


## Online training task
To explore whether exposing people to simulated datasets can help develop a more intuitive sense of the relationship between variability of estimates and sample size, we created an online task that mimics the real-world process of gathering and interpreting data in the life and social sciences. In this task, participants (i.e. scientists)  visually compare the distribution of simulated datasets and assess whether the samples come from a population where there is either a true effect (samples drawn from populations with differing means) or null effect (samples drawn from the same population). Potentially, the size of the effect in the population can be experimentally manipulated, but for the current study we focus on cases where the true effect size is Cohen’s _d_ = .3 (equivalent to _r_ = .15). This effect size was selected because, it is on the one hand fairly typical of the kind of effect size obtained in many areas of psychology, biomedicine (@meyer2001) and education (@dietrichson2021), and on the other hand there is a dramatic increase in the confidence with which results can be interpreted as favouring the true or null hypothesis as sample size increases (Figure `r (fignumber+1)`).  

```{r makerNdata,echo=FALSE,include=FALSE}
fignumber <- fignumber+1
keepfignumber <- fignumber #want to refer back to this one
nsub <- 100 #set to size of largest group 
numrange<-c(10,20,40,80,160,320)
nset <- length(numrange)
sampledat<-data.frame(matrix(NA,nrow=nsub,ncol=nset)) #dummy frame to hold simulated data on nsub cases and nset columns
#prepare plot

ES<-.3
plotname <- paste0('bees_ES',ES,'_Nall.jpg')
jpeg(plotname, width = 350, height = 300)


Nsample<-50

for (n in numrange){
  sampledat <- data.frame(matrix(NA,Nsample*2,3))
  colnames(sampledat)<-c('N','mean','ES')
  sampledat$N<-n
  thisrow<-0
  for(es in c(0,ES)){
    for (ns in 1:Nsample){
      thisrow<-thisrow+1
      sampledat$mean[thisrow] <- mean(rnorm(n,es,1))
      sampledat$ES[thisrow]<-es
    }
  }
  
  if(n==numrange[1]){ alldat<-sampledat}
  if(n>numrange[1]){alldat<-rbind(alldat,sampledat)} #bolt on next sample size in long format
}


#specify pink colour for control group, and blue for experimental group
alldat$colour <- 'deeppink'
w <- which(alldat$ES==ES)
alldat$colour[w]<-'blue'

beeswarm(alldat$mean~alldat$N,pwcol=alldat$colour,cex=.7,pch=16,xlab='Sample size',ylab='Observed effect size',ylim=c(-.7,.9))
abline(h=ES,lty=3,lwd=2,col='blue')
abline(h=0,lty=3,lwd=2,col='deeppink')
text(.6,-.62,'Power:',cex=.8)
for (samplesize in 1:6){
  mypower <-  power.t.test(n = numrange[samplesize], delta = ES, sd = 1, sig.level = 0.05,
                           type = c("two.sample"),
                           alternative = c("one.sided"))
  text((samplesize),-.7,round(mypower$power,3),cex=.8)
}
dev.off()
```

![Figure `r fignumber`. Simulated mean scores from samples of varying size, drawn from populations with either a null effect (pink) or a true effect size, Cohen's _d_, of `r ES` (blue). Power is the probability of obtaining p < .05 on a one-tailed t-test comparing group means for each sample size. ](bees_ES0.3_Nall copy.jpg)

<!--We use the copy jpg here, as it doesn't have points that overlap the labels-->

Participants will initially be presented with underpowered samples: they can either respond immediately (true effect or no effect) or can wait to see the sample size increase. Participants' subjective uncertainty is indexed by how long they wait to see more data. Ultimately, this paradigm not only measures perceptions of uncertainty; it is designed to help participants develop an intuitive sense of the uncertainty underlying even convincing-looking mean differences from small sample sizes. 

We will also examine the potential of this task to enhance participants' statistical reasoning by administering a pre- and post-training quiz, testing statistical reasoning using questions that assess sample size neglect.  The quiz also includes questions testing understanding of basic probability,  which act as positive controls: if participants score below 66% on the positive control questions, then this would indicate they either have little training in statistics, or are not taking the task seriously.

We plan to test five predictions (see below) from a three-step hypothesis:    
a) Even people who have a reasnable grasp of probability theory suffer from sample size neglect.  
b) This neglect can be ameliorated by providing training that involves exposure to different sample sizes drawn from a population.  
c) Training will change understanding of how sample size affects accuracy of estimates, and this will be evident beyond the training task.  

## Ethics statement
The protocol has been approved by the University of Oxford’s Medical Sciences Interdivisional Research Ethics Committee, approval number (R60658/RE001).


# Method

## Participants
Criteria for participants are: (1) age of 18 years or over, (2) have studied life or social sciences for at least one term at undergraduate (bachelor’s degree) level. We will recruit up to 100 participants via the online research platform Prolific (www.prolific.co) and social media platforms. The maximum sample size is determined from a power calculation (see section on Simulated data for sample size determination, below) indicating that this is sufficient to detect the case when 33% or more of participants improve their performance on the training task. 

## Procedure  
After providing informed consent, participants will complete the estimation and judgement quiz online at a time and place of their choosing, followed by an alternative form of the post-training  quiz. Participants are paid £7.50 for their time, but can earn a further bonus linked to the number of correct items on the quiz (3p per correctly answered item on each version), and their score on the training game to bring the payment up to a theoretical maximum of £12.  
Training is divided into four blocks, and after each block, participants are told that they can take a break if they wish. The study is implemented using Gorilla @anwyl-irvine2020, a cloud-based research tool for behavioral studies. The study takes around one hour to complete.  
  
## Design
Within-task learning will be assessed in a within-subjects design with one factor with two levels (block of trials, comparing the first and last blocks, each with 20 trials). To avoid any confound between specific training items and sequence in training, a predetermined set of items is presented in new random order for each participant.

In addition, transfer of training will be assessed using the estimation quiz, with parallel forms administered in a within-subject pre-post-test design.  

### Judgement and reasoning quiz
The estimation quiz (Appendix 1) was designed for this study, and is presented to participants as a 'Judgement and reasoning quiz' to minimise negative reactions from those who might feel they are poor at statistics. It has two parallel forms, each of which contains 12 multiple-choice items with 4 choices each. Six items test knowledge of general probability (P-items) and six test knowledge of how sample size determines accuracy (S-items). The parallel forms are counterbalanced, so half the participants receive form 1 at pretest and form 2 at post-test, and half receive the opposite order. The main dependent variable is the number of items correct for S-items, but the pattern of error responses will also be reported. A previous version of the quiz was piloted with 21 participants (Pilot 1), confirming that the difficulty level is appropriate, avoiding ceiling effects. On the basis of pilot testing and suggestions of reviewers, some items were reworded for clarity and to make S-items more relevant to the training.

### Independent and dependent variables
On the training task, the independent variable is block: there are 4 blocks each of 20 items, but our focus is on comparing blocks 1 and 4. The dependent variables are (a) earnings in the game, which reflects both proportion correct for each block, and ability to select the optimal array index at which to respond (see below); (b) mean array index per block, 1 to 6, corresponding to the array size (10, 20, 40, 80, 160, 320 per group) at which the response is made.  The earnings score is highly correlated with percentage correct (r = .989). 

For analysis of self-reported response strategies, we will use the comments in the free text-box asking about specific strategies adopted in the training task, which will be coded by two independent raters as a binary variable: does or does not mention how accuracy is higher if they either wait for more information or focus on larger array sizes. This classification will be used as an independent variable to predict learning gain. On the quiz, the independent variable is session (pre-training or post-training) and the dependent variable is proportion correct for S-items and P-items.  


### Online training
**Instructions**:  After the estimation quiz pre-test, participants are introduced to the game, as follows: 

_Imagine you are a researcher running a large range of experiments. All of the experiments are comparing a control group with a group receiving an experimental treatment (intervention) of some sort. You are testing whether the experimental group has a higher score than the control group after the intervention/treatment._   

_For our purposes, it doesn’t matter too much what the research topic is – the same principle applies to many different types of experiments. E.g., you could be examining:_  


* children’s test scores after an educational intervention versus no intervention  
  
* mouse weight after taking a drug versus a placebo  

* changes in cholesterol levels in people following an experimental diet versus no diet  
  
* effects of a chemical substrate on bacterial cell growth, compared to a control substrate (each data point measures growth of one cell culture)  

_You can choose which of these topics is most relevant to you, and keep that example in mind._  

_Here we assume data in these trials to be normally distributed – i.e., the distribution of scores will fit a bell curve, with most data points close to the middle, and fewer data points at the extremes of very high or very low values._    

_The onion shapes to the right are both (sideways) normal distributions: the width of the shape represents the proportion of data points in this population with a given value on the y-axis. The boxplot in the centre of the plot shows information about the mean and variance of distributions for each condition._  

_The top image on the right compares two distributions separately; the bottom makes a side-by-side comparison. Feedback in this game will be shown like the bottom image._  

_If there is no effect of the experimental treatment, the overall distribution of the data will look like the figure on the bottom right, with the distribution of scores for the experimental group looking very similar to that of the control group._  

_In this game, you will see data from experimental trials. On 50% of trials the data come from a population where there is a true effect of the intervention, which increases scores by 0.3 SD units. (i.e. the effect size is 0.3).  On the other 50% of trials, the intervention has no effect._  

_Your aim is to earn as many points as possible by identifying which trials show data from a population in which there is a true effect, and which trials come from a population with no effect._  
  
_The goal is for you to get a sense of how big a sample you need to make this judgement._  

_The game will include 80 trials in total, with breaks in between each block of 20 trials._  

_Every trial will start by showing a few data points from each group._  

_At that stage you can EITHER:_  
 _judge whether there is a true effect or no effect, by pressing one of the buttons below the data,_  
  
OR   
  
_you can wait a few seconds for the screen to automatically advance and show you more evidence (additional data points)._

_Collecting more evidence can make you more sure of your decision._   

_**Please note: this study does NOT involve any deception. The feedback you get will be accurate, even if it is sometimes surprising.**_  

_Each trial will start with an image like this, showing a set of data points: one from the control group, and one from the experimental group._  

_Along the y-axis you will see scores for each data point. Along the x-axis you will see how many data points there are in each group. Data points for the control group are pink and data points for the experimental group are blue. Each screen will start with 10 data points per condition._  

 _If you wait for more evidence, then more coloured data points will appear._  

_Each new array of data points doubles in size._  

_You can wait for more data until the target sample size of 320 is reached._  

_While data accumulates, you will have to judge whether you think the samples show a true effect or no effect._  

_In other words -- do these samples come from populations with different means, or the same mean? _  

_If you think these samples came from populations that truly differ, respond with **Blue > Pink**._  

_If you think these samples came from populations that are the same, respond with **Blue = Pink**._  

_<u>Once you answer you will hear a *bloop* sound indicating that your response has been recorded.</u>_  
_<u>After your response is made, the display increases until there are 320 datapoints  in each condition.</u>_  
_You will then see a feedback slide telling you whether you answered correctly and displaying how many points you earned._  

_On each trial, you earn 4 points for a correct answer, but you lose 4 points if you make a mistake._  

_You can also earn 2 bonus points if you make a correct response at the optimal array size.  This is the smallest array size at which the likelihood of one scenario (true effect or null)  is 40 times greater than the other.  You should get a sense of what this looks like as you go through the game. The optimal array size will vary from trial to trial, just because of chance factors._  

_<u>You should try to maximise your score as we will convert your points into pennies and add these to your payment.</u>  Don't worry, though: if you make lots of errors and end up with a negative score, we won't subtract anything from the basic payment for this session._  

The participant is then shown some examples that demonstrate what the stimuli look like, and the reward schedule is explained.  

At the end of training they are asked if they thought they had got better at the task over time (options: Yes, Unsure or No) and whether they adopted any specific strategy to guide their responses (free text response box).  

**Training items**  
Gifs for training items were generated by a purpose-designed script which is available on Open Science Framework (https://osf.io/rnkhq/?view_only=b0fcb097cc3044aaa942f15136441c49). For each item, samples of observations were selected in a cumulative fashion, such that, for instance, the sample with 20 observations per group included the sample with 10 observations per group, plus an additional 10 observations.  The array sizes doubled with each step, corresponding to 10, 20, 40, 80, 160 and 320 observations per group.  

All items included a pink sample from a population with mean of zero and SD of one, plus a blue sample. For the blue sample, half the items were drawn from the same population as the pink sample (i.e. null effect) and half were drawn from a population with mean of 0.3 and SD of one (i.e., true effect). `r fignumber <- fignumber+1` Figure `r fignumber` illustrates a set of samples shown side by side for a trial with a true effect. Figures were created using the R programming language (R Core Team, 2020) using the _geom_beeswarm()_ function from the _ggbeeswarm_ package (version 0.6.0; Clarke & Sherrill-Mix, 2017) in conjunction with _ggplot2()_ (version 3.2.1; Wickham, 2016). Each plot was combined into a gif using the _transition_states()_ function from the _gganimate_ package (version 1.0.7; Pederson & Robinson, 2020). In Pilot 1 (see below) we showed only the data points corresponding to the two datasets, but, as discussed below, for Pilot 2 we added the sample mean shown as a horizontal bar with the aim of improving accuracy.  

![Figure `r fignumber`. Sample item from the training set. The pair of distributions corresponding to each sample size is presented in a gif one at a time sequentially for 2 seconds, so the display builds up from left to right over time.](demoitem.jpg)

Items from each figure were animated so that each pair of pink-blue observations at a given array size was shown for 2 s, before the next array size appeared.  With this method, response latencies can be directly converted to the array size at the point of response, i.e. responses under 2 s correspond to array size of 10 per group, those under 4 s to an array size of 20 per group, and so on. For analysis, array is coded as an array index from 1 (10 per sample) to 6 (320 per sample).

Participants receive feedback after each response, in the form of a display showing points earned or lost, plus total points so far, and a violin plot showing the full distribution from which the points were drawn `r fignumber <- fignumber + 1` (see Figure `r fignumber`).  

![Figure `r fignumber`. Feedback display for an item with a true effect.](E3_AB copy.png) 

Data from the training trials are saved as response latency, which is converted to array size, as described above. For data analysis, array size (10, 20, 40, 80, 160, 320) is converted to array index, ranging from 1 to 6.  In addition, stored with each array is the amount earned, true  effect size in the population on that trial (Null or 0.3), coding of each response as correct (1) or incorrect (0), the observed effect size on that trial, and the log likelihood of a true vs null effect obtained from the observed data. The latter two variables could potentially be used in exploratory analyses when evaluating which cues participants base their responses on.

This basic task evolved over three pilot studies, which led to improvements in the design. A document with details of pilot tasks and results is available on Open Science Framework (https://osf.io/s39qd/). In two of the pilot tasks (1 and 3), no learning occurred, but in Pilot 2, there was clear evidence of learning. Accordingly, we will use this version of the task.  As well as exploring the influence of changes to the displays viewed by participants, we recognised the importance of manipulating incentives to make participants take the task seriously.  In the current version we adopt the method used in Pilot 2, which is to encourage thoughtful responding by explaining the idea of an optimal array size - which is the smallest array size at which the odds in favour of either the null or true hypothesis reaches 40:1 (absolute log likelihood of 3.68) or above.  We also convert the total positive points earned (4p for correct, -4p for error, plus bonus) into a financial bonus added to the basic payment for participation.  



## Analysis plan
### Exclusionary criteria
If participants do not complete the post-task quiz they will still be included in the main analysis of learning effects, but replacement participants will be recruited to ensure a sufficient sample size for the pre-post training analysis of the quiz results.  

We will also exclude from the main analysis any participants who score 5 or 6 correct on S-items in the pre-training quiz, as they would be people with a good prior understanding of sample size effects, so would be unlikely to show learning gains. We will check whether that assumption holds, by comparing performance of this subgroup with that of the rest of the sample on Block 1 of the training task.  We will recruit further participants to substitute for these in the main analysis, regardless of how they perform.  

For the regression analysis, we use an index of learning which corresponds to the difference in earnings between first and last blocks. 


## Data analysis  
Analyses are designed to test 5 predictions that follow from the three-step hypothesis outlined above:  

_Prediction 1_:  Responses will reflect overconfidence in small samples. A simple preliminary check will be made using a one-tailed _t_-test to check whether the mean number of S-items correct on the pre-training quiz is less than 5/6. If this prediction is not confirmed, then this would undermine hypothesis (a) and hence the rationale of the study. Thus this test may be regarded as a form of positive control.  In addition, a chi-square analysis of error responses will be conducted on pre-training S-items to test the expectation that errors on S-items will not be random, but will cluster on responses that indicate either that sample size is not important, or, if asked to select an optimal sample size, involve selecting one that is smaller than required. (These responses are marked x in the Appendix). 

_Prediction 2_: Accuracy will increase with training.  
We anticipate replicating the results of Pilot 2, showing improved performance, as reflected in earnings, with exposure to the training task. A one-tailed matched pairs _t_-test will be used to compare earnings on block 4 with earnings on block 1, with the prediction that the block 4 score will be higher. 

_Prediction 3_: Increased accuracy will be associated with selection of larger array sizes as learning proceeds  
This is a subsidiary prediction to prediction 2, which may throw light on the heuristics used by successful participants, if learning is observed. We predict that, as found in Pilot 2, there will be a significant association between mean array size and mean earnings, such that those who earn most will be those who select larger array sizes (tested by significance of Pearson correlation between these variables in block 4). 

_Prediction 4_: Self-reported strategy will be predictive of learning  
Again, this is a subsidiary prediction to prediction 2, with potential to clarify which cues are used by successful learners. We will divide the self-reported responses into those that do and do not indicate awareness that larger sample sizes (or longer waiting) gives more reliable estimates. This will be tested using a one-tailed independent groups _t_-test. Consistent with Pilot 2, we predict that those who show such awareness will obtain higher earnings in the final block than those who do not. 

_Prediction 5_: Generalisation of learning  
Individual differences in learning on the training task will predict improvement on the estimation quiz, specifically for items that are designed to assess sample size neglect. 

The prediction will be tested using linear regression:  
_postS ~ preS + earn.diff_,    
where postS and preS are post-training and pre-training scores on S- quiz items, and earn.diff is the difference in earnings (a measure of success on the task) between the last and first block of training. This simple approach to analysis was settled upon after using simulated data to conduct power analysis comparing linear regression versus a linear mixed models approach. The script for simulation is available on OSF (https://osf.io/kcrvw/files/).

Further exploratory analysis will be used to scrutinise the response profiles on the training task for those participants who do show learning, as well as their self-report of strategies, with the aim of determining which cues they rely on when making a response (see Exploratory analyses, below). 

The positive control P-items from the quiz will be scrutinised to check that the participants are competent in their knowledge of basic probability and motivated to respond accurately. If more than half the participants score less than 50% on P-items, then it would undermine the interpretation of poor performance on S-items. 

## Stopping rule  
We will do an initial check after testing 30 participants to see whether Prediction 1 is supported, indicating that participants on average show sample size neglect. If not, then we will halt data collection, and not proceed with the Registered Report as currently designed, as the basic premise of the study would not be supported. As noted above, Prediction 1 acts as a positive control.

Assuming the study continues, we will stop and check results after testing 50 participants, and then after each increment of 25 participants. Because power depends crucially on the proportion showing learning, and the increase in percent correct with learning, both of which are unknown at this point, we will adopt a Bayesian stopping rule at sequential stages in data collection, starting with a sample of `r subn[1]`, and then increasing to `r subn[2]` and `r subn[3]` if necessary to meet our pre-specified criteria. We will compute Bayes Factors for the _t_-test analysis testing hypothesis 2, and the linear regression for testing hypothesis 5.  While Bayes Factors fall between 1/3 or 3 we will continue with data collection. If the Bayes Factors yield evidence in favor of the alternative or null hypothesis for both analyses, we will stop data collection.  


## Exploratory analyses  
To complement the analysis of self-reported strategies (Prediction 4), the pattern of responses on the training task will be explored to see whether individual participants adopt specific strategies. Accuracy scores will be used to compute signal detection indices, d prime and beta, for each block of training, based on the number of hits (respond yes when there is a true effect); the number of misses (respond no when there is a true effect), the number of false alarms (respond yes when there is a null effect) and the number of correct negatives (respond no when there is a null effect). Calculations of these indices will be conducted using the _dprime()_ function from the _psycho_ package (version 0.5.0; Makowski,2018). The beta values will allow us to detect cases of response bias, i.e. a tendency to always respond 'blue=pink' or 'blue>pink'.  

As shown in the power values in Figure `r keepfignumber`, with an effect size (Cohen's _d_) of .3, participants need to wait for an array of at least 80 per group (index 4) to have a reasonable chance of success (above 50%) for detecting a true effect, and would be well-advised to wait for the largest array (index 6), which would virtually guarantee success.  However, the trial by trial variation allows us to do a rather more sensitive analysis, testing how the evidence available on each specific trial is used.  

`r fignumber <- fignumber+1`  Figure `r fignumber` illustrates the logic. The figure shows how evidence accumulates over a series of trials, labelled A to G.  

```{r readspreadsheet,include=FALSE,echo=FALSE}


plotname <- paste0('accumulateLL.jpg')
jpeg(plotname, width = 650, height = 350)
demospread <- read.csv('gorilla_spreadsheets/spreadsheet1.csv')
demospread$trueES <- demospread$ES+1
demospread$trueES[demospread$trueES==1.3]<-2
levels(demospread$trueES)<-c(2,1)
par(mfrow=c(1,2))
mycols<-c('red','blue')
#plot obs ES
EScol1 <- which(colnames(demospread)=='ObsE1')
  plot(1:nset,demospread[3,EScol1:(EScol1+nset-1)],type='b',pch=LETTERS[1],ylim=c(-1,1),col=mycols[demospread$trueES[3]],xlab='Array index',ylab='Observed Effect size')
for (n in 8:12){
    lines(1:nset,demospread[n,EScol1:(EScol1+nset-1)],type='b',pch=LETTERS[(n-6)],col=mycols[demospread$trueES[n]])
  
  }
abline(h=.3,lty=2)
abline(h=0,lty=2)

#plot LL
LLcol1 <- which(colnames(demospread)=='LL1')

  plot(1:nset,demospread[3,LLcol1:(LLcol1+nset-1)],type='b',pch=LETTERS[1],ylim=c(-12,12),col=mycols[demospread$trueES[3]],xlab='Array index',ylab='Log Likelihood')
for (n in 8:12){
    lines(1:nset,demospread[n,LLcol1:(LLcol1+nset-1)],type='b',pch=LETTERS[(n-6)],col=mycols[demospread$trueES[n]])
  }
abline(h=3.68,lty=2)
abline(h=-3.68,lty=2)



dev.off()
```

![Figure `r fignumber`. Changes in observed effect size (left panel) and log likelihood (right panel) as evidence accumulates for 6 trials of the learning task, labelled A to G. Blue denotes trials with true effect size of .3, and red denotes null trials. Array indices 1 to 6 correspond to sample sizes of 10, 20, 40, 80, 160 and 320 per group. ](accumulateLL.jpg)  

  
Figure `r fignumber` shows the contrast between observed effect sizes, which show more variation at small array indices, and log likelihood, where trials with true and null effects diverge as array index increases.  Considering first the observed effect sizes, it is clear that if a participant relied on this information to make decisions at small array indices, they would make many errors. Two of the True trials, B and C, have negative effect sizes at array indices 1 and 2, and one of the Null trials, D, has an effect size greater than .3 in the first array. Log likelihood values are shown in the right-hand panel, with dotted lines denoting cutoff of +/- 3.68, which we used as a criterion for the optimal array size.  A log likelihood of 3.68 indicates that the True Hypothesis is 40 times more likely to have generated the observed data than the Null Hypothesis, and a log likelihood of -3.68 indicates that the Null Hypothesis is 40 times more likely than the True Hypothesis. The trials shown in Figure `r fignumber` indicate that a strategy of responding True when log likelihood exceeds 3.68 and Null when it is less than -3.68 is not infallible (e.g., trial B has log likelihood below -3.68 at array index 3, yet comes from a distribution with true effect size of .3). Nevertheless, we can use this approach to compute accuracy levels that would arise from adopting different strategies over all trials, and it is clear that reliance on responding when absolute log likelihood exceeds 3.68 is a close to optimal strategy for achieving success without needing to wait until the final array. 

Of course, a problem for participants is that they cannot estimate log likelihood directly from the information they are presented with. However, they can estimate the amount of overlap between the blue and pink samples, which is predictive of log likelihood.  Our primary analysis assumes that, on the basis of feedback, participants may simply learn that they are more likely to be accurate if they wait to see more data, in which case, they would base their judgement on observed effect size, once the array index gets to 5 or above. By observing the pattern of responses in individuals, we may be able to determine if a participant adopts a more specific strategy, and if so, whether they place reliance on observed effect size or sample overlap. 



## Simulated data for sample size determination
```{r getarrayprops, echo=F}
allarrayfreqs <- c(11,54,246,570,860,659) #based on pilot 2 data
arrayprops <- round(allarrayfreqs/sum(allarrayfreqs),3)
```
To simulate data, we used our best judgement combined with insights from the pilot testing to explore feasible scenarios. We started by assuming that at the start of training, on each trial, participants will select an array index in proportions similar to that seen in the first block of Pilot 2:  index 1 = `r arrayprops[1]`, index 2 =`r arrayprops[2]`, index 3 = `r arrayprops[3]`,index 4 = `r arrayprops[4]`, index 5 =`r arrayprops[5]`, index 6 = `r arrayprops[6]`.  By the final block, a proportion (p) of participants will be designated as 'learners'. Whether a participant is a learner is simulated using a variable, L, which is latent, in the sense that it affects performance on observed measures, but is not itself detectable. L is either 0 or 1. The response made by the participant is determined by three factors: (a) whether the participant is a learner; (b) whether log likelihood of distributions at that array index favours a true or null effect; (c) random error, which leads to a higher proportion of errors in non-learners than learners.

Where L is equal to one (i.e. the participant is a learner), the participant waits until an array index where the display corresponds to an absolute log likelihood of at least 3.65. For non-learners, the response is made on the basis of whether log likelihood is positive or negative, regardless of the strength of evidence. In addition, a proportion of all responses for both learners and non-learners are coded as errors, with the proportion being higher for non-learners. A simulated sample of participants is created by applying these rules to the stimulus set used in training, with response on each trial being then scored as correct or incorrect.

Prediction 2 focuses on whether there is evidence of learning, with higher earnings on the last block than the first block. The sample size (N subs) was varied from `r subn[1]`, to `r subn[2]`, to `r subn[3]`. The proportion of participants who learn (prop.learned) could take values of 0, .25, .33 or .5. `r tabnumber <- tabnumber+1` Table `r tabnumber` shows the power for detecting an overall change in mean earnings from block 1 to block 4, using a matched-pairs one-tailed _t_-test, with alpha of .0322 (.1/3, for one-tailed test administered at each of three sample sizes). A one-tailed test is used because the prediction is directional. These figures were derived from 500 iterations of the simulation.  

The case where no participants learn corresponds to a test of the null hypothesis, and so here the power value corresponds to the false positive rate. As expected, this is close to .03. At our smallest sample size (`r subn[1]`), even when only 25% of participants learn, power is above .8, even for the case where learning increases array size by only one index point. 


```{r powertable,include=TRUE,echo=FALSE}
bigname<-"powsummary_1000_L_pgood0.5_0.66_props0_0.25_0.33_0.5_N50_75_100.csv"
#bigname was created in the Simulate_power_proportions script; keeps track of parameters in simulation
bigsummary<- read.csv(bigname)
#bigsummary<-bigsummary[,c(1:4,8,6,9:11)] #final columns 9:11 for array index;
nsubvals<-c(50,75,100) #values that vary for N participants - should match those in bigname
plearnvals<-c(0,.25,.33,.5) #values that vary for proportion of participants who learn- should match those in bigname
quizgoodvals<-c(.5,.66) #all participants start with mean proportion correct on quiz S-items of .33. Those who show learning on training task increase to either .5 or .66. (Equivalent to increase by 1 or 2 items)
thisrow<-0
alpha <- .0166*2 #sequential stopping means need divide .05 by 3, but then note we have one-tailed test
powersummary<-data.frame(matrix(NA,ncol=11,nrow=length(nsubvals)*length(plearnvals)*length(quizgoodvals)))
colnames(powersummary)<-colnames(bigsummary)


row<-0

    for (p in plearnvals){
      for (q in quizgoodvals){
      for (n in nsubvals){

      row<-row+1
      powersummary$nsub[row]<-n
      powersummary$plearn[row]<-p
     powersummary$quiz.pbad[row]<-.33
      powersummary$quiz.pgood[row]<-q
      temp<-filter(bigsummary,nsub==n,plearn==p,quiz.pgood==q)
      powersummary$run<-nrow(temp)
      wc<-which(colnames(temp)=='p.t.array')
      for(mycol in c(4:6,9:11)){
        w<-length(which(temp[,mycol]<alpha))
        powersummary[row,mycol]<-w/nrow(temp)
      }
      
    }
  }

}

#column p.t.earn is power for testing change in mean earnings from block1 to block4 (1-sided t-test)
#column ptquiz is power for testing change in percent correct in S-items on quiz from pre to posttest
# column lmpre.p is power of regression coefficient for quiz pretest score
# column lmearndiff.p is power of regression coefficient for earndifference (block 4 minus block 1) as predictor quiz posttest.

#Make table for learning effect in task
#For learning effect on task, we just average power across levels of quiz success
learntab <- aggregate(powersummary$p.t.earn,by=list(powersummary$nsub,powersummary$plearn),FUN=mean)
table1<-data.frame(matrix(NA,nrow=5,ncol=4))
colnames(table1) <- c('Parameters','N=50','N=75','N=100')
table1[2,1] <- '_plearn = 0'
table1[3,1] <- '_plearn = .25'
table1[4,1] <- '_plearn = .33'
table1[5,1] <- '_plearn = .50'


thisrow=0
thatrow=-2

  for (pl in 1:4){
    thisrow<-thisrow+1
    thatrow<-thatrow+3
    if(pl==1){thisrow <- thisrow+1}
    table1[thisrow,2:4]<-round(learntab[thatrow:(thatrow+2),3],2)
  }



write.csv(table1,'table1.csv',row.names=F)
ftab <- flextable(table1)

ftab <- set_caption(ftab, paste0("Table ",tabnumber,": Power for comparison of earnings for block 4 vs block 1, in relation to proportion of learners (plearn)"))
ftab<-autofit(ftab)
ftab



```


To address Prediction 5, quiz data were simulated using the binomial distribution, assuming the mean number of S-items correct on the pretest is 2/6 (.33), improving by either one item (3/6 = .5) or two items (4/6 = .66) for the subgroup of participants who showed learning on training (where the latent variable, L, = 1).  

`r tabnumber <- tabnumber+1`Results are shown in Table `r tabnumber`, again looking at scenarios with 50, 75 or 100 participants, and where the proportion showing learning ranges from 0, .25, .33 and .5. One other parameter is considered for the quiz data, namely the amount of improvement in quiz scores seen in learners - a mean gain of either 1 quiz item, or 2 quiz items.  

```{r powerquiz,include=TRUE,echo=FALSE}
#summary for quiz improvement
quizsum<-powersummary[,c(2,3,8:11)]
colnames(quizsum) <- c('N subs','prop.learned','learned.quiz2','power.quizdiff','power.quizpre','power.arraydiff')
quizsum[,4:6]<-round(quizsum[,4:6],2)
write.csv(quizsum,'quizsum.csv',row.names=F)

tabnumber<-tabnumber+1
#Make table2 for learning effect on quiz
#For learning effect on task, we just average power across levels of quiz success
qlearntab <- aggregate(powersummary$ptquiz,by=list(powersummary$nsub,powersummary$plearn,powersummary$quiz.pgood),FUN=mean)
colnames(qlearntab)<-c('nsub','plearn','quizlearn','power')
table2<-data.frame(matrix(NA,nrow=10,ncol=4))
colnames(table2) <- c('Parameters','N=50','N=75','N=100')
table2[1,]<-c('Quiz gain = 1','.','.',',')
table2[6,]<-c('Quiz gain = 2','.','.',',')
table2[2,1] <- '_plearn = 0'
table2[3,1] <- '_plearn = .25'
table2[4,1] <- '_plearn = .33'
table2[5,1] <- '_plearn = .50'
table2[7,1] <- '_plearn = 0'
table2[8,1] <- '_plearn = .25'
table2[9,1] <- '_plearn = .33'
table2[10,1] <- '_plearn = .50'

thisrow=0
thatrow=-2
for (A in 1:2){
  for (pl in 1:4){
    thisrow<-thisrow+1
    thatrow<-thatrow+3
    if(pl==1){thisrow <- thisrow+1}
    table2[thisrow,2:4]<-round(qlearntab[thatrow:(thatrow+2),4],2)
  }
}


write.csv(table2,'table2.csv',row.names=F)
ftab2 <- flextable(table2)

ftab2 <- set_caption(ftab2, paste0("Table ",tabnumber,": Power for comparison of quiz pre and post test, in relation to number of participants (N), proportion of learners (plearn) and increase in quiz score in learners (Quiz gain)"))
ftab2<-autofit(ftab2)
ftab2

```
Power to detect any improvement in the overall sample is generally poor if one just compares quiz pre-test and post-test items, except where the gain in quiz score is at least 2 points and at least 33% of participants learn. This follows because the number of quiz items is small, only a proportion of participants show any learning, and the relationship between learning and quiz post-test performance is imperfect. 


```{r powerLM,include=T,echo=F}
#Now do table 3 for power for regression
#Make table3 for prediction of quiz posttest from earngain
#For learning effect on task, we just average power across levels of quiz success
LMtab <- aggregate(powersummary$lmearndiff.p,by=list(powersummary$nsub,powersummary$plearn,
                                                     powersummary$quiz.pgood),FUN=mean)
table3<-data.frame(matrix(NA,nrow=10,ncol=4))
colnames(table3) <- c('Parameters','N=50','N=75','N=100')

table3[2,1] <- '_plearn = 0'
table3[3,1] <- '_plearn = .25'
table3[4,1] <- '_plearn = .33'
table3[5,1] <- '_plearn = .50'
table3[7,1] <- '_plearn = 0'
table3[8,1] <- '_plearn = .25'
table3[9,1] <- '_plearn = .33'
table3[10,1] <- '_plearn = .50'

table3[1,]<-c('Quiz gain = 1','.','.',',')

table3[6,]<-c('Quiz gain = 2','.','.',',')


thisrow=0
thatrow=-2

  for (q in 1:2){
  for (pl in 1:4){
    thisrow<-thisrow+1
    thatrow<-thatrow+3
    if(pl==1){thisrow <- thisrow+1}
    table3[thisrow,2:4]<-round(LMtab[thatrow:(thatrow+2),4],2)
  }
  
}

tabnumber<-tabnumber+1
write.csv(table3,'table3.csv',row.names=F)
ftab3 <- flextable(table3)

ftab3 <- set_caption(ftab3, paste0("Table ",tabnumber,": Power for  regression coefficient predicting post-training score on S-items from observed increase in earnings on the learning task. Power varies with number of participants (N), proportion of learners (plearn), and quiz gain at post-training for learners (quiz gain)"))
ftab3<-autofit(ftab3)
ftab3
```





   
An alternative approach uses linear regression, where the focus is on predicting the quiz post-training score (S-items) from the pre-training score and the index of learning, i.e., the change in earnings from block 1 to block 4. This analysis, then, can detect whether the amount of learning in the training session is predictive of improvement on the quiz.  

As shown in Table `r tabnumber`, the combination of parameters determines power, with high power seen when at least 33% of participants show learning, and when their quiz scores increase by 2 points. Power to detect a one-point gain in quiz scores for learners is modest at best.  Once again, consideration of the case where no participants show learning confirms that the type 1 error rate is well-controlled, so we can be confident that where positive findings are obtained, they are true effects.  


## Interpretation of pattern of results
We can refer to the pattern of results on the training task and the quiz as indicating null result, inconclusive, or prediction confirmed. 

The first possible pattern is where neither the training task nor the quiz shows any improvement. If this pattern were observed, we would conclude that the training task was not effective in debiasing participants away from sample size neglect to a meaningful extent. We would consider using information from the experiment to devise a modified version of the task. There are various ways one could do this: by changing the visual display, or by changing the reward schedule, or providing more explicit instruction. It would, of course, also be possible to extend the training, although this might require having more than one session. In effect, this study could act as a baseline against which to evaluate other approaches to training.  

If results on one or both measures are inconclusive, this could reflect insufficient training/quiz items, and or sample heterogeneity. If the sample is heterogeneous, with a subset responding to the training, we would expect to see a relationship between gains on training and gains on the S-items on the quiz. In addition, we would expect to see an association between learning and self-reported awareness of the importance of sample size. We would also aim in future work to identify characteristics of those who improved with training.  

We may find that we find significant improvements with training, but null or inconclusive results on the quiz, indicating a lack of generalisation. If so, we will explore individual S-items in the quiz, to see whether there is any indication of improvement on a subset of items, and whether they have particular characteristics. We may, for instance, see evidence of selective improvement on items assessing understanding of the relationship between sample size and statistical power, which could be tested in a replication study.

The converse pattern might be obtained, with improvement on the S-items of the quiz but null or inconclusive findings on the training task. This might justify a further study which uses two training sessions, making it possible to look for gains due to training that occur after a delay.  

Finally, if we are able to show an improvement with this short training session, with generalisation to S-items on the quiz, then this would lead us to conclude that the game might be a useful adjunct to statistical training courses for scientists. This could motivate further studies with modified forms of presentation, to identify the optimum conditions for training, and with different effect sizes. It would also be of interest to evaluate the training for longer-term impact on subsequent experimental practices of the trainees.  

Tests of predictions 3 and 4 have potential to provide converging evidence on whether sensitivity to array size is a factor determining learning. If neither prediction is confirmed, but the training is effective, this would suggest we should explore alternative heuristics that might be used by participants to support successful performance. 

## Pilot data  
`r tabnumber <- tabnumber+1`
Three pilot studies have been conducted.  Summary results for all three pilots are available on OSF  (https://osf.io/s39qd). Overall performance on Pilot 1 was poor, with mean of only 65% correct, confirming the difficulty that participants had in judging effect sizes from distributions. A score of 85% or above was observed in only 10% of all blocks x participants. Participants in Pilot 1 found the task reasonably straightforward but scrutiny of individual cases for evidence of response strategies suggested that only a few participants had started to wait longer to respond as the session proceeded, and others went in the opposite direction, selecting smaller arrays in later blocks.  In general, high performers were those who waited for large arrays from the outset, rather than those who changed strategy in the course of training. 

Pilot 1 participants also completed the estimation quizzes that preceded and followed the training session. As noted above, modifications were made to wording of some quiz items for the current study on the basis of pilot testing to avoid confusing or over-complicated language. 

On the basis of Pilot 1 data, we considered ways of making the training more effective. In Pilot 1, the beeswarm displays for the two groups  had omitted the horizontal line corresponding to the mean, and each beeswarm faded out as the next one appeared, which meant it was less obvious that the distributions were cumulative. In addition, although the participants were shown a demonstration of the task, they did not have any familiarisation trials, and it was felt that they might perform better if given the chance to try a few trials to become familiar with the pace of the task. For Pilot 2, the displays included a horizontal bar corresponding to the mean, and prior displays remained visible rather than fading out, to emphasise that, within a trial, each set of points included the prior, smaller set. In addition, participants were given 4 practice trials before the training blocks, and the reward schedule was modified to provide stronger motivation to succeed, as described above. 

A second pilot study, Pilot 2, was run with 30 participants on the new version of the training task (without the quiz).  This confirmed that not only was performance improved relative to Pilot 1, but also there was evidence of learning in terms of the percentage correct and earnings measures (which were highly intercorrelated). The mean array index was numerically larger for the last vs the first block, but the difference was small and not statistically significant. However, for the final block, there was a significant correlation between mean array index and percent correct, and several participants reported that they learned to wait for larger arrays in the course of training, suggesting the task was effective in achieving its aim. 

At the suggestion of a reviewer, we conducted Pilot 3 with 20 participants, using an unpaced version of the task, in which on each trial, the participant was presented with a menu and required to select a sample size to view. A plot showing all sample sizes, similar to Figure `r keepfignumber+1`, was shown alongside feedback on accuracy for each trial. A cost of one point was incurred with each increase in the selected array index, to prevent participants from always selecting the largest array size. No learning was observed in Pilot 3, which gave results similar to Pilot 1, and therefore this approach was abandoned.


# Appendix 1
## Judgement and Reasoning Quiz

Participants receive version A or B (counterbalanced) at pre-test and post-test. P-items and S-items are presented intermixed: P-items test understanding of probability, S-items test sample size neglect. Items with the same number in version A and B are intended to be analogous but use different contexts. The brief descriptor accompanying each item below is not presented as part of the quiz. An asterisk denotes the correct response.  For S-items, x denotes the anticipated response(s) for those susceptible to sample size neglect.

<!---modified from Probqs_edit_from_feedback.rmd--->
### Initial queries (Pre-training only)

i. Compared to others at your level of education in your participant, how good do you think your understanding of basic statistics is?
a) Excellent  
b) Better than average  
c) Average  
d) Below average 

ii. How confident are you in interpreting the results of a t-test?  
a) Very confident  
b) Fairly confident  
c) Not confident  
d) Very little idea about what a t-test is  
  
iii. How familiar are you with the idea of statistical power?  
a) Very familiar  
b) Reasonably familiar  
c) Somewhat familiar  
d) Very little idea about what statistical power is  

### Qualitative report (Post-training only)
Did you feel you got better at the task over time?  Yes/Unsure/No

Did you change how you approached the task?  Please let us know if you adopted any specific strategy to guide your response?

   
### P-items
#### P1A (tests elementary knowledge of normal distribution)
You take a sample of 100 men from the general population and measure their height.  The mean is 70 inches and the standard deviation is 4 inches. What percentage of the sample will be expected to be more than 74 inches tall?  
a) *16%  
b) 2%  
c) 50%  
d) 30%  
 
#### P1B  
A reading test is standardized on 7-year-old children in Scotland. The mean reading age is 84 months with standard deviation of 6 months.  What percentage of 7-year-olds is expected to have a reading age of 72 months or less?  
a) 16%  
b) *2%  
c) 50%  
d) 30%  

```{r q1a, include=F,echo=F}
#All quiz items simulated to check accuracy
#NB for accurate estimate we use N = 10000
mydata <- rnorm(10000,70,4)
w<-which(mydata>74)
ans1<-100*length(w)/10000
print(paste0("The answer is ",ans1))

mydata <- rnorm(10000,84,6)
w<-which(mydata<72)
ans2<-100*length(w)/10000
print(paste0("The answer is ",ans2))

```
#### P2A (Basic probability). 
A container is full of spare change containing 100 10p coins, 100 5p coins, 200 2p coins, and 100 1p coins. The coins are randomly mixed. You will get a prize every time you pick a 5p coin. If you make 100 selections, replacing the selected coin and shaking the jar each time, how often will you expect to pick a 5p coin?  

a) 1 in two occasions  
b) 1 in three occasions  
c) 1 in four occasions  
d) *1 in five occasions  
 
#### P2B 
You are choosing marbles from an opaque jar containing 100 red marbles, 100 blue marbles, and 200 white marbles, randomly mixed. You will get a prize every time you pick a marble that is not white. If you make 100 selections, replacing the selected marble and shaking the jar each time, how often will you expect to get a prize?  

a) *1 in two occasions
b) 1 in three occasions
c) 1 in four occasions
d) 1 in five occasions
 

```{r P2B,include=F,echo=F}
myselect<-vector()
mymarbles<-c(rep(1,100),rep(2,100),rep(3,200))
for (i in 1:100){
  myselect<-c(myselect,sample(mymarbles,1))
}
myt<-table(myselect)
names(myt)<-c('Red','Blue','White')
myt
myt/500
 
```

#### P3A (Classic probability including averaging probabilities)
In the city of Ficticium, there is generally a 10% chance it will rain on any given day in the first half of September, a 50% chance it will rain any given day in the second half of September,  a 25% chance it will rain on any given day in November, and a 30% chance it will rain on any given day in April. (September, November and April are all 30 days long). Which month is likely to have more rainy days?  
a) September  
b)  November  
c)  April  
d) *September and April are equally likely  

```{r P3A, include=F, echo=F}
myrain<-data.frame(matrix(NA,nrow=1000,ncol=3))
names(myrain)<-c('Sep','Nov','Apr')
for (i in 1:10000){
sep1<-rbinom(1,15,.1)
sep2<-rbinom(1,15,.5)
nov<-rbinom(1,30,.25)
apr<-rbinom(1,30,.3)
myrain[i,1]<-sep1+sep2
myrain[i,2]<-nov
myrain[i,3]<-apr
}
print ('Mean rainy days')
print(paste('September ',mean(myrain$Sep)))
print(paste('November ',mean(myrain$Nov)))
print(paste('April ',mean(myrain$Apr)))
```

#### P3B. 
After reviewing sales of coffee, the barista noted there is generally a 50% chance of selling an espresso on any day in the first week of the month. There is a 100% chance of selling an espresso on any day in the second week of the month, and a 75% chance of selling an espresso on any day in the third and fourth week of the month. In what two-week period in the month is there most sales of espresso?  
 
a) *First two weeks and second two weeks are equally likely   
b) First two weeks  
c) Second two weeks  
d) First and fourth weeks  

#### P4A. (basic probability)
At a raffle, you can pick from a green bowl with 2 winning raffle tickets and 8 worthless tickets, a yellow bowl with 10 winning tickets and 90 worthless tickets, or a red bowl with 15 winning tickets and 85 worthless tickets.  Which bowl gives you a better chance of winning?  
a) *Green  
b) Yellow  
c) Red  
d) Green and Yellow give the same chance  
 
```{r P4A, include=F,echo=F}
greencount<-yellowcount<-redcount<-0
for (i in 1:1000){
green<-c(rep(1,2),rep(0,8))
yellow<-c(rep(1,10),rep(0,90))
red<-c(rep(1,15),rep(0,85))
greencount<-greencount+sample(green,1)
yellowcount<-yellowcount+sample(yellow,1)
redcount<-redcount+sample(red,1)
}
print(paste('Green p = ',greencount/1000))
print(paste('Yellow p = ',yellowcount/1000))
print(paste('Red p = ',redcount/1000))
```

#### P4B. 
You find three bags of jelly beans. There is a small bag with 5 red jelly beans and 20 other coloured beans. There is a  medium bag with 12 red jelly beans and 38 other coloured beans. Finally, there is a large bag with 18 red jelly beans and 82 other colour beans. As red jelly beans are your favourite flavour, which size bag gives you the highest percentage of red beans?   

a) Small  
b) *Medium  
c) Large  
d) They are all the same  

#### P5A. (conjoint probability)
There are 100 girls in a class, 20% have red hair and 40% are taller than 60 inches.  How many red-headed girls would you expect who are taller than 60 inches?  
a) 60  
b) 16  
c) *8  
d) 5  

```{r P5A,echo=F,include=F}
# This gives a different answer on each run, but converges on 8 (.4*.2)
girls <- c(rep(1,20),rep(0,80))
tallgirls<-sample(girls,40) #random selection from girls
tallred <- sum(tallgirls)
print(tallred)
```

#### P5B
In a box of 100 CDs, 40% of the disks have been recorded by pop artists and 60% of disks feature female vocalists. Assuming that the female vocalists are equally likely to be pop artists or not, how many CDs have been recorded by female pop vocalists?   

a) 12 CDs  
b) 18 CDs  
c) *24 CDs  
d) 30 CDs  
 
#### P6A (classic probability - multiplication)
10 percent of the children in a school have red hair. Their names are put in a hat and you are asked to pull out two of them. What is the probability that you will select two red-headed children?  

a)  20 percent  
b) *1 percent  
c)  19 percent  
d)  close to zero   

```{r q12a, include=F, echo=F}
print(paste(100*.1*.1,' percent'))
```

#### P6B. 
20 percent of the 500 children in a school have a name beginning with J. Their names are put in a hat and you are asked to pull out two of them. What is the probability that you will select  two children whose name begins with J?  

a) *4 percent  
b)  1 percent  
c)  40 percent  
d)  20 percent   

```{r P6B,include=F,echo=F}
print(paste(100*.2*.2,' percent'))
```

### S-items 
#### S1A (frequency of extreme scores in small samples)

History courses in the UK are rated on a Student Survey, which has an overall satisfaction rating of 1 (low) to 5 (high). The mean rating overall is 3.5, with SD of 1. Those who have an average rating above 4.0 are given a gold star in league tables. There are 60 courses altogether, which can be divided according to size into small (less than 10 students), medium (between 10-50 students) and large (50+ students). If there are no real differences in student satisfaction, will the number of students affect the likelihood of getting a gold star?  

a) Yes. Large courses have a better chance of getting a gold star  
b) Yes. Medium-sized courses have a better chance of getting a gold star  
c) *Yes. Small courses have a better chance of getting a gold star  
d) x No. The number of students makes no difference.  

```{r S1A, echo=F, include=F}
size <- c(5,30,60)

for (u in 1:3){
  ustring<-NA
  for (v in 1:50){
mydat<-rnorm(size[u],3.5,1)
ustring<-c(ustring,mean(mydat))
n[u]<-length(which(ustring>4))
}
}



```

#### S1B. 
A task force is looking at characteristics of schools in its area. They take a measure of mathematical ability, and identify schools as 'failing schools' where 20% or more pupils get scores more than 1 SD below the population average. The smallest schools have on average 100 pupils, middle-sized schools have 250 pupils, and the largest schools have 500 pupils. 
If there are no real differences between schools, will the size of the school affect whether it is a failing school?  
a) *Yes. The smallest schools will be more likely to be a failing school  
b)  No. All else being equal, school size should make no difference  
c)  Yes. The largest schools will be more likely to be a failing school  
d)  Yes. The middle-sized schools will be more likely to be a failing school  

```{r S1B,include=F, echo=F}
N  = 1000 #N times sampled
schoolsize<- c(100,250,500)
myschool <- data.frame(matrix(NA, nrow=N, ncol=6))
names(myschool)<-c('Small','Medium','Large','SmallF','MediumF','LargeF')
for (i in 1:N){
  myschool[i,1]<-length(which(rnorm(schoolsize[1],0,1)<(-1)))
  myschool[i,2]<-length(which(rnorm(schoolsize[2],0,1)<(-1)))
  myschool[i,3]<-length(which(rnorm(schoolsize[3],0,1)<(-1)))
  for (j in 1:3){
    myschool[i,(j+3)]<-0
    if (myschool[i,j]/schoolsize[j]>.2){
      myschool[i,(j+3)]<-1
    }
  }
}
print(paste('Out of ',N,' schools, this is number coded as failing, for Small, Medium and Large respectively'))
for (j in 1:3){
  print(sum(myschool[,(j+3)]))
}

```



#### S2A (probability of result in 'wrong' direction is higher with small sample size)
A forensic archaeologist has a set of 10 male skeletons from an ancient burial site. Experts are divided as to whether the site was colonised by Tribe A or Tribe B- the only 2 tribes in the region. Previous studies with large samples found that men from Tribe A had an average height of 160 cm, with SD of 6, and men from Tribe B had an average height of 158 cm, with SD of 6. 

The mean height of the sample is 158 cm. What can the archaeologist conclude?  

a)    We can be very confident that the sample comes from Tribe B 
b)  * Tribe B is more likely than Tribe A, but should collect 90 more samples to be sure  
c)    Equally likely the sample comes from Tribe A or Tribe B 
d)  x  Can't be sure: Tribe B is more likely than Tribe A, but collecting more samples won't help
    

```{r S2A,include=F,echo=F}
#We simulate data to compute the probability that the observation comes from tribe A
 #The observed value is half a SD below average for sample A       
mA= 160  #tribe A
sA=6
mB=158 #tribe B
sB=6
obsmean <- 158
obsdiffA <- (mA-obsmean)/sA
niter<- 1000
for (n in c(10,50,100)){ #try with different sample sizes
  print(paste0('n = ',n))

counter1 <- 0 #initialise counters for various difference scores

alld<-NA
for (i in 1:niter){
xsample<-rnorm(n,mA,sA) #observed sample - actually from tribe A
d<- mean(xsample)-mA
alld<-c(alld,d) 
critd<-obsdiffA*sA
if (d>critd){counter1 <- counter1+1}	
}



print(paste0('prob that xsample is .33 SD less than xA mean: ',counter1/niter))


}


```


#### S2B (probability of result in 'wrong' direction is higher with small sample size)
There are concerns that kangaroos appear unhealthy in a particular area of the Australian bush. There is concern that this may be due to eating poisoned bait that affects the blood. A naturalist has blood samples from 10 kangaroos, and measures a distinctive blood marker that is lowered in poisoned animals. Previous studies have found that the mean blood marker in healthy kangaroos is 130 with SD of 30, whereas the mean is 120 with SD of 30 in poisoned animals. 

The mean blood marker in the sample is 120. Assuming that there is no other explanation than poisoning for an abnormal blood marker, what can the scientist conclude?

a)    The kangaroos have definitely been poisoned
b)  x  The kangaroos probably have been poisoned but can't be sure. Collecting data from more kangaroos won't help. 
c)  * The kangaroos probably have been poisoned, but would need to collect blood from 90 more kangaroos in the affected area to be sure  
d)    The kangaroos have not been poisoned

    
```{r S2B,include=F,echo=F}
#We simulate data to compute the probability that the observation comes from unpoisoned animals 

mA = 120
sA = 30
mB = 130
sB = 30
alld <- vector()
obsmean <- 120

obsdiff <- (mB - obsmean) / sB
niter <- 1000
for (n in c(10, 50, 100)) {
  #try with different sample sizes
  print(paste0('n = ', n))
  
  counter1 <- 0 #initialise counter
  
  for (i in 1:niter) {
    xsample <-
      rnorm(n, mB, sB) #observed sample - actually from unpoisoned (B)
    d <- mB - mean(xsample)
    alld <- c(alld, d)  #can plot this to see distribution of differences)
    critd <- obsdiff * sB
    if (d > critd) {
      counter1 <- counter1 + 1
    }
  }
  
  print(
    paste0(
      'prob that an unpoisoned sample is .33 SD less than mean of unpoisoned population: ',
      counter1 / niter
    )
  )
}

```

#### S3A (dependence of power on sample size)
 
A fertiliser is trialled to see if it improves crop yields. Without the fertiliser the average yield is 100, with standard deviation of 10. It is expected that the fertiliser will boost yield by 3 points on average.
How many plants would be needed in the treatment and control groups to be confident of demonstrating whether or not the fertiliser was effective?  

a)  20 per group  
b) 50 per group  
c)  100 per group  
d) *300 per group   

```{r S3a, include=F, echo=F}
require(pwr)
pwr.t.test(d = .3, sig.level = .05 , power =.95 , type = "two.sample",alternative="greater") 
```
  
#### S3B. 
You have been asked to test a treatment for obesity. People in the trial have a mean body mass index (BMI) of 35, with SD of 5. The developer argues that the treatment will reduce BMI by 2 points on average, but you are dubious as to whether it has any effect. Assuming you have a control group given a placebo and an experimental group given the treatment, what sample size should you select to give a fair test of the treatment? 

a) x20 per group  
b) x50 per group 
c) *100 per group
d) 500 per group

```{r S3b, include=F, echo=F}
require(pwr)
myp<-pwr.t.test(d = .4, sig.level = .05 , power =.9 , type = "two.sample",alternative="greater") 
```


#### S4A.(likelihood of given value depends on sample size: same as S2 from parallel form, but with initial adequate N)
There are concerns that kangaroos appear unhealthy in a particular area of the Australian bush. There is concern that this may be due to eating poisoned bait that affects the blood. A naturalist has blood samples from 160 kangaroos, and measures a distinctive blood marker that is lowered in poisoned animals. Previous studies have found that the mean blood marker in healthy kangaroos is 130 with SD of 30, whereas the mean is 120 with SD of 30 in poisoned animals. 

The mean blood marker in the sample is 120. Assuming that there is no other explanation than poisoning for an abnormal blood marker, what can the scientist conclude?  

a)  *  The kangaroos have definitely been poisoned
b)    The kangaroos probably have been poisoned but can't be sure. Collecting data from more kangaroos won't help. 
c)   The kangaroos probably have been poisoned, but would need to collect blood from 100 more kangaroos in the affected area of bush to be sure  
d)    The kangaroos have not been poisoned

    
```{r S4A,include=F,echo=F}
#We simulate data to compute the probability that the observation comes from unpoisoned animals 

mA = 120
sA = 30
mB = 130
sB = 30
alld <- vector()
obsmean <- 120

obsdiff <- (mB - obsmean) / sB
niter <- 1000
for (n in c(10, 50, 160)) {
  #try with different sample sizes
  print(paste0('n = ', n))
  
  counter1 <- 0 #initialise counter
  
  for (i in 1:niter) {
    xsample <-
      rnorm(n, mB, sB) #observed sample - actually from unpoisoned (B)
    d <- mB - mean(xsample)
    alld <- c(alld, d)  #can plot this to see distribution of differences)
    critd <- obsdiff * sB
    if (d > critd) {
      counter1 <- counter1 + 1
    }
  }
  
  print(
    paste0(
      'prob that an unpoisoned sample is .33 SD less than mean of unpoisoned population: ',
      counter1 / niter
    )
  )
}

```

#### S4B.(Likelihood of given value depends on sample size: same as S2 from parallel form, but with initial adequate N) 
A forensic archaeologist has a set of 160 male skeletons from an ancient burial site. Experts are divided as to whether the site was colonised by Tribe A or Tribe B 0 the only two tribes in the region. Previous studies with large samples found that men from Tribe A had an average height of 160 cm, with SD of 6, and men from Tribe B had an average height of 158 cm, with SD of 6. 

The mean height of the sample is 158 cm. What can the archaeologist conclude?  

a)   * We can be very confident that the sample comes from Tribe B  
b)    Tribe B is more likely than Tribe A, but should collect 100 more samples to be sure  
c)    Equally likely the sample comes from Tribe A or Tribe B 
d)    Can't be sure: Tribe B is more likely than Tribe A, but collecting more samples won't help
    

```{r S4B,include=F,echo=F}
#We simulate data to compute the probability that the observation comes from tribe A
 #The observed value is half a SD below average for sample A       
mA= 160  #tribe A
sA=6
mB=158 #tribe B
sB=6
obsmean <- 158
obsdiffA <- (mA-obsmean)/sA
niter<- 1000
for (n in c(10,50,160)){ #try with different sample sizes
  print(paste0('n = ',n))

counter1 <- 0 #initialise counters for various difference scores

for (i in 1:niter){
xsample<-rnorm(n,mA,sA) #observed sample - actually from tribe A
d<- mean(xsample)-mA
alld<-c(alld,d) 
critd<-obsdiffA*sA
if (d>critd){counter1 <- counter1+1}	
}



print(paste0('prob that xsample is .33 SD less than xA mean: ',counter1/niter))


}


``` 
 
#### S5A. (classic from Tversky/Kahneman law of small numbers)
In a squash tournament, the organisers are debating whether to have games of best of 3, 9 or 21 points.  Holding all other rules of the game constant, if A is a slightly better player than B, which scoring will give A a better chance of winning?  
a) best of 3 points  
b) best of 9 points  
c) *best of 21 points  
d) x Won't make any difference  
 
 
```{r S5A, echo=F,include=F}
#NB. Simulating this makes it clear that if A is much better than B, it really doesn't make much difference with original Ns! Fine if we simulate so that prob A winning is .6 and if we make lowest the best of 3.

#Assuming we can treat p(A) win as p
#In fact, this only works if p not too high and if lowest N games is v small
p = .6
games<-c(3,9,21)


  for (j in 1:3){
    seq1 <-0
   for (i in 1:1000){

  thisg<-rbinom(games[j],games[j],p)
  thiswin <- length(which(thisg>games[j]/2))
  if(thiswin>games[j]/2) {seq1<-seq1+1}

   }
    print(paste0('N games is ',games[j]))
    print(paste0('Wins in 1000 = ',sum(seq1)))
}

```

#### S5B. 
Two chess players are having a tournament. They are considering whether to play the best of 5, 11, or 17 games.  If player A is slightly better than player B, which tournament size should player B argue for, to get the best chance of winning?  

a) 17 games  
b) 11 games  
c) *5 games  
d) It doesn't matter: The chances are the same regardless of number of games  
 

#### S6A. (Risk of type II error with small samples)
Two scientists are both trying to test whether a certain new drug affects hunger in mice, by giving them the drug (group D) or a placebo (group P) and then measuring their food consumption. At the start of the experiment, the average mouse eats 10g of food pellets, with SD of 3g.  
Scientist A runs 10 studies with 10 mice in each group, and Scientist B runs 10 studies with 30 mice in each group. Unfortunately for them, a careless lab technician distributed a placebo in place of the new drug, so there should not be any effects except by chance. When the scientists look at the results, a large effect, a difference in food consumption of 3g, is seen between the two groups (D and P) on one run of the experiment. Which scenario is more likely:  

a)  *The run with a large difference is found in the smaller group   
b)    The run with a large difference is found in the larger group  
c)    The group difference shows group D eats more than group P  
d)   x A large difference is equally likely D>P or P>D, with no effect of sample size  
 


```{r S6A,include=F,echo=F}
n1<-10
n2<-30
alldiffa<-vector()
alldiffb<-vector()
for (i in 1:10){
miceDa<-rnorm(n1,10,3)
micePa<-rnorm(n1,10,3)
miceDb<-rnorm(n2,10,3)
micePb<-rnorm(n2,10,3)
diffa<-mean(miceDa)-mean(micePa)
diffb<-mean(miceDb)-mean(micePb)
#print(paste0('Mean diff n8 = ',
#             diffa))
#print(paste0('Mean diff n16 = ',
#             diffb))
alldiffa<-c(alldiffa,diffa)
alldiffb<-c(alldiffb,diffb)}
print('Differences for each run of small group: ')
      print(alldiffa)
print('Differences for each run of large group: ')
      print(alldiffb)
```

#### S6B.
Two researchers are testing the effect of oxytocin on prosocial behaviour. In their experiments, participants are given either a dose of oxytocin or a placebo. Researcher A runs 5 studies with 10 participants in each condition and Researcher B runs 5 studies with 20 participants in each condition. Unknown to the researchers, the oxytocin had been replaced by a placebo, yet a large difference (1 SD) is observed between two groups on one run of the experiment. Which is most likely:  

a) *The large difference is seen in a study by the researcher using groups of 10  
b)  The large difference is seen in a study by the researcher using groups of 20  
c) x The difference is a type II error, and could equally likely be seen with groups of 10 or 20  
d)  The difference is real - there was confusion between oxytocin and placebo  



## References 


