---
title: 'Stage 1 Registered Report: Can we shift belief in the ''Law of Small Numbers''?'
author: "D V M Bishop, Jackie Thompson & Adam Parker"
date: "4/05/2021"
output:
  word_document: default
  html_document:
    df_print: paged
---


```{r setup, include=FALSE,echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(beeswarm)
require(tidyverse)
library(RColorBrewer) #used later for 'zone of uncertainty' plots

library(dplyr)
library(yarrr)
library(ggplot2)
library(lme4)
library(MASS)
library(effects)
library(simr)
library("ggbeeswarm")
library(pwr)
library(flextable)
tabnumber <- 0 #for counting tables.
fignumber <- 0 #for counting figures

```
Affiliations:  
1: Department of Experimental Psychology, University of Oxford, Anna Watts Building, Woodstock Road, Oxford, OX2 6GG. 

2: School of Experimental Psychology, University of Bristol, The Priory Road Complex, Priory Road, Clifton, BS8 1TU.  
  
Acknowledgment: This work was supported by Wellcome Trust Programme Grant no. 082498/Z/07/Z.

## Abstract 
<!---297 words--->
In 1971, Tversky and Kahneman noted that even statistically-trained people were poor at judging how variability of mean estimates declined with sample size. This cognitive bias, which we term 'sampling variation blindness' can make it difficult to gain an intuitive appreciation of statistical power. In this study, we consider whether training with simulated datasets can help people overcome this bias. Subjects are presented with beeswarm plots and told that the pink group comes from a control population with mean 0 and SD of 1, and the blue group comes from a comparison group drawn either from the same population as the pink group, or from a population with mean of 0.3 and SD of 1. The task is to respond either 'blue = pink' or 'blue > pink' as soon as they are confident. They receive feedback about accuracy on each trial. Up to 100 people will be recruited for an online training session with 80 trials, each of which involves viewing a gif that displays a series of beeswarm plots from two samples that double in size in six steps, with array size going from 10 to 320.  Before and after training, subjects are given parallel forms of a 12-item 'estimation quiz' testing statistical knowledge, including six items testing sampling variation blindness. We hypothesise that explicit exposure to displays that illustrate how sample size affects mean estimates will counteract sampling variation blindness, leading to improved performance in at least one third of subjects. We further predict generalisation, so that those who show gains in accuracy will also show improvement  on quiz items relating to sampling variation blindness. Finally, we will explore the heuristics used by individual subjects to inform their decision making, e.g. whether their responses are predictable from array size, effect size and/or likelihood ratio. 

#### Keywords:  statistical reasoning, power, online training, sampling variation blindness


 




```{r makefig1, include=FALSE, echo=FALSE}
#This chunk just generates figure 1 - it could probably be made more efficient
fignumber <- fignumber + 1
samplesizes <- c(10,60)
set.seed(2)
mymean <- 178 #mean  - can vary this - here this is in raw units
mysd <- 10 
nsim <- 6
plotname <- paste0('Fig1.jpg')
jpeg(plotname, width = 400, height = 200)
par(mfrow=c(1,2))
for (i in samplesizes){
    thisdat<-rnorm(i*nsim,mymean,mysd)
    thisset <- rep(1:nsim,i)
    alldat<-data.frame(cbind(thisdat,thisset))
    colnames(alldat)<-c('Height','Set')
    beeswarm(thisdat~thisset,cex=.75,pch=16,col='darkgrey',ylim=c(160,200),xlab='Sample',ylab='Height (cm)',main=paste0('Sample size ',i))
    mymeans<-aggregate(alldat$Height,by=list(alldat$Set),FUN=mean)
    for (n in 1:nsim){
    segments((n-.3),mymeans$x[n],(n+.3),mymeans$x[n],lwd=2,col='red')
    }
    abline(h=mymean,lty=1,col="blue")
}


dev.off()
```
## Introduction

Compared to laypeople, scientists receive extensive training to help them understand and appropriately address the uncertainty of evidence. Yet, many scientists fall short in their understanding of statistical concepts and experimental design — an underlying failure to appreciate uncertainty. One cognitive bias demonstrated by Tversky and Kahneman (1971) is the 'belief in the law of small numbers'. This refers to the tendency to overestimate the reliability of data that come from small samples - which we shall term 'sampling variation blindness'. For instance, as shown in Figure 1, if repeatedly sampling `r samplesizes[1]` men from a population, the mean height of the sample will be far more variable than when repeatedly sampling `r samplesizes[2]` men. People understand that sample size does not affect the expected mean value, but they tend not to appreciate that it has a large effect on the standard error of the mean. This has implications for understanding of statistical power, and can help explain why so many studies in psychology, and indeed many other scientific disciplines, are underpowered. 

![Figure `r fignumber`. Six samples of simulated male height with sample size of 10 or 60. Each point represents height of one male, drawn from population with mean 178 cm and SD 10 cm, and the red bar is the mean for that sample. The horizontal blue line represents the sample mean.](Fig1.jpg)

In 1962, Cohen embarked on a project of improving psychologists understanding of statistical power, providing tools to help people compute power and documenting the extent of underpowered studies in social psychology, with the aim of reducing waste in research efforts. However, 27 years later, Sedlmeier and Gigerenzer (Sedlmeier & Gigerenzer, 1989) reported that things had not changed at all. And in 2016, similar conclusions were drawn from a large review of studies in social and behavioral sciences up to 2011 (Smaldino & McElreath, 2016). In 2018 a review concluded that low power remains as a major factor explaining poor replicability of highly cited studies in psychology (Stanley et al., 2018). A wide range of areas are affected, from neuroscience (Poldrack et al., 2017), to infancy research (Oakes, 2017). Why, despite years of attempts to improve the dismal record of low power in psychology, do researchers persist in performing underpowered studies? We suspect the explanation may go beyond lack of training, and reflect the influence of sampling variability blindness, which leads us to have intuitions about sample size that are at odds with reality. Consider this example (Tversky & Kahneman, 1971, p.105):

‘‘Suppose you have run an experiment on 20 subjects, and have obtained a significant result which confirms your theory (z= 2.23, p < .05, two-tailed). You now have cause to run an additional group of 10 subjects. What do you think the probability is that the results will be significant, by a one-tailed test, separately for this group?’’

Tversky and Kahneman reported that the majority of researchers who responded to this question were wrong in stating that the probability is somewhere around .85, while only 9 out of 84 researchers gave a more accurate answer (i.e. between .40 and .60). This is thought to reflect inaccurate beliefs in sampling that have unfortunate consequences in the course of scientific enquiry. Researchers view randomly drawn samples as highly representative of the population to a greater extent than sampling theory predicts, at least for small numbers.  

It would seem that if we are to tackle the problem of wasteful and misleading underpowered studies, we need to find ways to overcome sampling variation blindness. Informally, we have found that exposing students to simulations that allow them to visualize the variation between samples of various sizes can help counteract over-reliance on small samples to evaluate hypotheses.By generating datasets with known effect sizes and drawing random samples and subjecting them to statistical tests, students can learn to appreciate the ease with which we miss a true effect if the sample size is small. Simulations are a core aspect of a course outlined by Steel, Liermann, and Guttorp (2019) which trains statistical thinking in undergraduates, using simulations to help interpret patterns in data and to evaluate statistical power. Although their course appears successful in training statistical thinking in the long term, it does not specifically evaluate the utility of simulations in counteracting sampling variability blindness. That is the goal of the current study.  

In addition to training, simulations have been used to examine the decisions made by researchers. Morey and Hoekstra (2019) used simulations to examine scientists’ understanding of significance testing. In their task, scientists were asked to perform a series of experiments to judge which two groups of elves could make more toys based on a randomly assigned group difference between 0 and 1 standard deviations. Of the 136 participants for whom the null hypothesis was true, 86% correctly indicated no effect. When there was a true effect, correct decision increased as a function of effect size.  When the effect size was .3, accuracy was approximately 80%. After this, accuracy approached ceiling. When asked about the heuristics that they applied, 72% indicated using strong significance testing strategies. Thus, Morey and Hoeskstra highlight the potential utility of simulation-based training to answer questions about how researchers use information. As well as evaluating the efficacy of training on statistical judgments, we will consider whether those who are successful on the task show evidence of adopting specific heuristics, such as waiting for a given sample size before making a judgment. 


## Online training task
To explore whether exposing people to simulated datasets can help develop a more intuitive sense of the relationship between variability of estimates and sample size, we created an online task that mimics the real-world process of gathering and interpreting data in the life and social sciences. In this task, subjects (i.e. scientists)  visually compare the distribution of simulated datasets and assess whether the samples come from a population where there is either a true effect (samples drawn from populations with differing means) or null effect (samples drawn from the same population). Potentially, the size of the effect in the population can be experimentally manipulated, but for the current study we focus on cases where the true effect size is Cohen’s d = .3 (equivalent to r = .15). This effect size was selected because, it is on the one hand fairly typical of the kind of effect size obtained in many areas of psychology and biomedicine (Meyer et al, 2001), and on the other hand shows a dramatic increase in the confidence with which results can be interpreted as favouring the true or null hypothesis as sample size increases (Figure `r (fignumber+1)`).  

```{r makerNdata,echo=FALSE,include=FALSE}
fignumber <- fignumber+1
nsub <- 100 #set to size of largest group 
numrange<-c(10,20,40,80,160,320)
nset <- length(numrange)
sampledat<-data.frame(matrix(NA,nrow=nsub,ncol=nset)) #dummy frame to hold simulated data on nsub cases and nset columns
#prepare plot

ES<-.3
plotname <- paste0('bees_ES',ES,'_Nall.jpg')
jpeg(plotname, width = 350, height = 300)


Nsample<-50

for (n in numrange){
  sampledat <- data.frame(matrix(NA,Nsample*2,3))
  colnames(sampledat)<-c('N','mean','ES')
  sampledat$N<-n
  thisrow<-0
  for(es in c(0,ES)){
    for (ns in 1:Nsample){
      thisrow<-thisrow+1
      sampledat$mean[thisrow] <- mean(rnorm(n,es,1))
      sampledat$ES[thisrow]<-es
    }
  }
  
  if(n==numrange[1]){ alldat<-sampledat}
  if(n>numrange[1]){alldat<-rbind(alldat,sampledat)} #bolt on next sample size in long format
}


#specify pink colour for control group, and blue for experimental group
alldat$colour <- 'deeppink'
w <- which(alldat$ES==ES)
alldat$colour[w]<-'blue'

beeswarm(alldat$mean~alldat$N,pwcol=alldat$colour,cex=.7,pch=16,xlab='Sample size',ylab='Observed effect size',ylim=c(-.7,.9))
abline(h=ES,lty=3,lwd=2,col='blue')
abline(h=0,lty=3,lwd=2,col='deeppink')
text(.6,-.62,'Power:',cex=.8)
for (samplesize in 1:6){
  mypower <-  power.t.test(n = numrange[samplesize], delta = ES, sd = 1, sig.level = 0.05,
                           type = c("two.sample"),
                           alternative = c("one.sided"))
  text((samplesize),-.7,round(mypower$power,3),cex=.8)
}
dev.off()
```
![Figure `r fignumber`. Simulated mean scores from samples of varying size, drawn from populations with either a null effect (pink) or a true effect size, Cohen's d, of `r ES` (blue). Power is the probability of obtaining p < .05 on a one-tailed t-test comparing group means for each sample size. ](bees_ES0.3_Nall copy.jpg)
<!--We use the copy jpg here, as it doesn't have points that overlap the labels-->


Subjects will initially be presented with underpowered samples: they can either respond immediately (true effect or no effect) or can wait to see the sample size increase. Subjects' subjective uncertainty is indexed by how long they wait to see more data. Ultimately, this paradigm not only measures perceptions of uncertainty; it is designed to help subjects develop an intuitive sense of the uncertainty underlying even convincing-looking mean differences from small sample sizes. 

We will also examine the potential of this task to enhance subjects' statistical reasoning by administering a pre- and post-task estimation quiz, testing statistical reasoning using questions that assess sampling variation blindness.  The quiz also includes questions testing understanding of basic probability, to act as positive controls: if subjects score below 66% on this set of questions, then this would indicate they either have little training in statistics, or are not taking the task seriously.

Specific hypotheses are:  
Hypothesis 1: In line with early work by Tversky and Kahneman (1971), people will be overconfident about estimates based on small samples.  We predict that we will demonstrate this in the estimation quiz, where performance will be poor on questions that focus on sampling variation blindness, and also with our training task, showing that typically people make decisions based on samples where the number of data points is too small to give clear evidence.  
Hypothesis 2: Repeated exposure to a training task with feedback can reduce overconfidence in small samples. Accordingly, we predict that, in the course of training, a proportion of subjects will start to wait longer to view evidence from larger arrays, and correspondingly achieve higher levels of accuracy.
Hypothesis 3: Learning will generalize beyond the training task. Individual differences in learning on the training task will predict improvement on sampling variation blindness items on the estimation quiz. 


## Ethics statement
The protocol has been approved by the University of Oxford’s Medical Sciences Interdivisional Research Ethics Committee, approval number (R60658/RE001).


# Method

## Subjects
Our subject inclusion criteria include: (1) age of 18 years or over, (2) have studied life or social sciences for at least one term at undergraduate (bachelor’s degree) level. We will recruit up to 100 subjects via the online research platform Prolific and social media platforms. The maximum sample size is determined from a power calculation (see section on Determination of Sample Size below) indicating that this is sufficient to detect the case when 25% or more of subjects shift from reliance on small arrays at start of training to arrays of 4 or more at the end of training. We will, however, apply a Bayesian stopping rule at sequential stages in data collection. We will compute Bayes Factors for each analysis where a model is compared against a null denominator model. While Bayes Factors fall between 1/3 or 3 we will continue with data collection. If the Bayes Factors yield evidence in favor of the alternative or null hypothesis for all analyses, we will stop data collection.  

## Procedure  
After providing informed consent, subjects will complete the estimation quiz and judgment task online at a time and place of their choosing, followed by an alternative form of the post-test estimation quiz. Training is divided into four blocks, and after each block, subjects are told that they can take a break if they wish. The study is implemented using Gorilla, a cloud-based research tool for behavioral studies. Subjects are paid £5 for their time; the study takes approximately 30 minutes to complete.  

## Design
Within-task learning will be assessed in a within-subjects design with one factor with two levels (block of trials, comparing the first and last blocks, each with 20 trials). To avoid any confound between specific training items and sequence in training, predetermined blocks each of 20 items, A, B, C and D will be presented in counterbalanced order, so that each block is presented equally often in first, second, third or fourth position in the sequence.  

In addition, transfer of training will be assessed using the estimation quiz, with parallel forms administered in a within-subjects pre-post-test design.  

### Judgment and reasoning quiz
The quiz (Appendix 1) was designed for this study, and is termed 'Judgment and reasoning quiz' to minimise negative reactions from those who might feel they are poor at statistics. It has two parallel forms, each of which contains 12 multiple-choice items with 4 choices each. Six items test knowledge of general probability (P-items) and six test knowledge of how sample size determines accuracy (S-items). The parallel forms are counterbalanced, so half the subjects receive form 1 at pretest and form 2 at post-test, and half receive the opposite order. The main dependent variable is number of items correct for S-items, but the pattern of error responses will also be reported. A previous version of the quiz was piloted with 21 subjects, confirming that the difficulty level is appropriate, avoiding ceiling effects, and that accuracy on items of type S is lower than that for type P. On the basis of pilot testing, some items were reworded for clarity.

### Online training
**Instructions** After the estimation quiz pre-test, subjects are introduced to the game, as follows: NEED TO ADD INSTRUCTIONS HERE  - NB I think we should say something explicit as follows:
'Please note: this study does NOT involve any deception. The feedback you get will be accurate, even if it is sometimes surprising.'

**Training items**
Gifs for training items were generated by a purpose-designed which is available on Open Science Framework. For each item, samples of observations were selected in a cumulative fashion, such that, for instance, the sample with 20 observations per group included the sample with 10 observations per group, plus an additional 10 observations.  The array sizes doubled with each step, corresponding to 10, 20, 40, 80, 160 and 320 observations per group.  

All items included a pink sample from a population with mean of zero and SD of one, plus a blue sample. For the blue sample, half the items were drawn from the same population as the pink sample (i.e. null effect) and half were drawn from a population with mean of 0.3 and SD of one (i.e., true effect). `r fignumber <- fignumber+1` Figure `r fignumber` illustrates a set of samples shown side by side for a trial with a true effect. Figures were created using the R programming language (R Core Team, 2020) using the __geom_beeswarm__ function from the __ggbeeswarm__ package (version 0.6.0; Clarke & Sherrill-Mix, 2017) in conjunction with __ggplot2__ (version 3.2.1; Wickham, 2016). Each plot was combined into a gif using the __transition_states__ function from the gganimate package (version 1.0.7; Pederson & Robinson, 2020). Items for pilot testing (see below) showed only the datapoints corresponding to the two datasets, but, as discussed below, for the current study, we added the sample mean shown as a horizontal bar.  `r fignumber<-fignumber+1`

![Figure `r fignumber`. Sample item from the training set. The pair of distributions corresponding to each sample size is presented in a gif one at a time sequentially for 2 seconds.](demoitem.jpg)

Items from each figure were animated so that each pair of pink-blue observations at a given array size was shown for 2 s, before the next array size appeared.  With this method, response latencies can be directly converted to the array size at the point of response, i.e. responses under 2 s correspond to array size of 10 per group, those under 4 s to an array size of 20 per group, and so on.  

Subjects receive feedback after each response, in the form of a display showing points earned or lost, plus total points so far, and a violin plot showing the full distribution from which the points were drawn `r fignumber <- fignumber + 1` (see Figure `r fignumber`).  

(Figure 4 to be added here - screen with feedback)  

The main dependent measures from the training trials are condition (i.e., whether there was a null or true effect on that trial), coding of each response as correct (1) or incorrect (0), and response latency, which is converted to array size, as described above. For data analysis, array size (10, 20, 40, 80, 160, 320) is converted to array index, ranging from 1 to 6.  In addition, stored with each array is the observed effect size on that trial, and the log likelihood of a true vs null effect obtained from the observed data.  

In our original pilot work with this task, the reward schedule was simply 10 points for a correct response and -10 for an incorrect response. One drawback of this approach is that the only incentive to respond before the final display is to minimize time in the experiment. We therefore included a penalty for waiting for larger sample sizes, with a potential for larger rewards at small sample sizes. This makes the subject's decision more life-like, since there is a cost to using larger vs smaller samples. 

Accuracy scores are used to compute signal detection index, d prime, for block of training, based on the number of hits (respond yes when there is a true effect); the number of misses (respond no when there is a true effect), the number of false alarms (respond yes when there is a null effect) and the number of correct negatives (respond no when there is a null effect). Calculations of this metric will be conducted using the __dprime__ function from the __psycho__ package (version 0.5.0; Makowski,2018). 

As shown in the power values in Figure 2, with an effect size (Cohen's d) of .3, participants need to wait for an array of at least 80 per group (index 4) to have a reasonable chance of success (above 50%) for detecting a true effect, and would be well-advised to wait for the largest array (index 6), which would virtually guarantee success. 

Accordingly, we could just do a simple count of how commonly subjects responded when the array index was less than 4, i.e. when the chances of detecting a true effect were below .5. However, the trial by trial variation allows us to do a rather more sensitive analysis, testing how the evidence available on each specific trial is used.  

`r fignumber <- fignumber+1`  Figure `r fignumber` illustrates the logic. The figure shows how evidence accumulates over a series of trials. 

```{r readspreadsheet,include=FALSE,echo=FALSE}


plotname <- paste0('accumulateLL.jpg')
jpeg(plotname, width = 650, height = 350)
demospread <- read.csv('gorilla_spreadsheets/spreadsheet1.csv')
demospread$trueES <- demospread$ES+1
demospread$trueES[demospread$trueES==1.3]<-2
levels(demospread$trueES)<-c(2,1)
par(mfrow=c(1,2))
mycols<-c('red','blue')
#plot obs ES
EScol1 <- which(colnames(demospread)=='ObsE1')
  plot(1:nset,demospread[3,EScol1:(EScol1+nset-1)],type='b',pch=LETTERS[1],ylim=c(-1,1),col=mycols[demospread$trueES[3]],xlab='Array index',ylab='Observed Effect size')
for (n in 8:12){
    lines(1:nset,demospread[n,EScol1:(EScol1+nset-1)],type='b',pch=LETTERS[(n-6)],col=mycols[demospread$trueES[n]])
  
  }
abline(h=.3,lty=2)
abline(h=0,lty=2)

#plot LL
LLcol1 <- which(colnames(demospread)=='LL1')

  plot(1:nset,demospread[3,LLcol1:(LLcol1+nset-1)],type='b',pch=LETTERS[1],ylim=c(-12,12),col=mycols[demospread$trueES[3]],xlab='Array index',ylab='Log Likelihood')
for (n in 8:12){
    lines(1:nset,demospread[n,LLcol1:(LLcol1+nset-1)],type='b',pch=LETTERS[(n-6)],col=mycols[demospread$trueES[n]])
  }
abline(h=3,lty=2)
abline(h=-3,lty=2)



dev.off()
```

![Figure `r fignumber`. Changes in observed effect size (left panel) and log likelihood (right panel) as evidence accumulates for 6 trials of the learning task, labelled A to G. Blue denotes trials with true effect size of .3, and red denotes null trials. Array indices 1 to 6 correspond to sample sizes of 10, 20, 40, 80, 160 and 320 per group. ](accumulateLL.jpg)  

  
Figure `r fignumber` shows the contrast between observed effect sizes, which show more variation at small array indices, and log likelihood, where trials with true and null effects diverge as array index increases.  Considering first the observed effect sizes, it is clear that if a subject relied on this information to make decisions at small array indices, they would make many errors. Two of the True trials, B and C, have negative effect sizes at array indices 1 and 2, and one of the null trials, D, has an effect size greater than .3 in the first array. Log likelihood values are shown in the right-hand panel, with dotted lines denoting cutoff of +/- 3.  A log likelihood of 3 indicates that the True Hypothesis is 20 times more likely to have generated the observed data than the Null Hypothesis, and a log likelihood of -3 indicates that the Null Hypothesis is 20 times more likely than the True Hypothesis. The trials shown in Figure `r fignumber` indicate that a strategy of responding True when log likelihood exceeds 3 and Null when it is less than -3 is not infallible (e.g., trial B has log likelihood below -3 at array index 3, yet comes from a distribution with true effect size of .3). Nevertheless, we can use this approach to compute accuracy levels that would arise from adopting different strategies over all trials, and it is clear that reliance on responding when absolute log likelihood exceeds 3 is a close to optimal strategy for achieving success without needing to wait until the final array. 

Of course, a problem for subjects is that they cannot estimate log likelihood directly from the information they are presented with. It seems more likely that, on the basis of feedback, they may simply learn that they are more likely to be accurate if they wait to see more data, in which case, they might decide to base their judgment on observed effect size, once the array index gets to 5 or above. By observing the pattern of responses, we may be able to determine if a subject adopts a specific strategy, and if so, whether they place reliance on observed effect size, array index, or log likelihood. 


## Pilot data  
`r tabnumber <- tabnumber+1`
Pilot data were collected with 21 participants recruited via Prolific, using a version of the training task that showed the beeswarm plots for the two groups, but omitted the horizontal line corresponding to the mean. In this version, each beeswarm faded out as the next one appeared.  Participants found the task reasonably straightforward but scrutiny of individual cases for evidence of response strategies suggested that only a few participants had started to wait longer to respond as the session proceeded/ The evidence was not compelling, and others showed the opposite effect, starting to respond earlier over time. In general, high performers were those who waited for large arrays from the outset, rather than those who changed strategy in the course of training. On the basis of these observations, we modified the task for the current version, to include a horizontal bar corresponding to the mean, and to retain prior displays within a trial rather than fading them out, to emphasise that, within a trial, each set of points included the prior, smaller set. 


The pilot participants also completed the estimation quizzes that preceded and followed the training session. The correlation between improvement in proportion correct on training and the pre-post quiz difference was marginally statistically significant for S-questions (r = .432, CI 0 to .728) and not for P-questions (-.337), which is consistent with the idea that those who improve on training also show improvements on S-items. The small sample size and the marginal nature of the result mean that the finding is far from compelling, but this result justifies further study to see if this association replicates in a larger sample. Some modifications were made to wording of some quiz items for the current study, to avoid confusing wording.

## Analysis plan
### Exclusionary criteria
If subjects do not complete the post-task quiz they will still be included in the main analysis of learning effects, but replacement subjects will be recruited to ensure a sufficient sample size for the pre-post training analysis of the quiz results.  
In our pilot data, no subjects scored above 80% on the training task, but it is anticipated that the updated version of training (showing the mean bar) will be easier, and it is possible that some will obtain scores that are so high that no learning effect will be detectable. If there are subjects who score above 90% in the first block of training, they will be removed from the main analysis and treated separately. It is predicted that these would be people with a good prior understanding of sample size effects, and accordingly should be superior to other subjects on the S-items at both pre-test and post-test.  

## Data analysis
Hypothesis 1: Responses will reflect overconfidence in small samples.
This hypothesis will be tested using data both from the Pretest estimation quiz and the training task. On the pretest estimation quiz, it is expected that the majority of subjects will fail to select the correct response on items that assess awareness of the importance of sample size, and when they do so, will either respond that sample size is not important, or, when asked to select an optimal sample size, will select one that is smaller than required.  
In the training task, we predict that responses in the first half of training will tend to be non-optimal, made when the array size is small enough to make an error likely. The array index at which a response is made for all subjects will be compared with a value of 5, using a directional one-sample t-tests, with the prediction that most participants will make responses at earlier array indices than this.

Hypothesis 2: Overconfidence in small samples will decline with training
This prediction will be tested by assessing whether array index at the point of selection is higher for the last than the first block of the training session, using a one-tailed test. In addition, it is anticipated that within each block there will be a positive correlation between accuracy and array index, indicating that subjects are able to use the additional information in larger sample sizes to guide their responding.

Hypothesis 3: Generalisation of learning
Here the prediction is that individual differences in learning on the training task will predict improvement on the estimation quiz, specifically for items that are designed to assess sampling variability blindness. We will test this using linear regression:
postS ~ preS + arraydiff
where postS and preS are posttset and pretest scores on S-items, and arraydiff is the difference in mean array index between the last and first block of training. This simple approach to analysis was settled upon after using simulated data to conduct power analysis comparing linear regression versus a linear mixed models approach. The script for simulation is available on OSF. The power analysis from the linear regression showed the false positive rate to be consistent with the expected level of .05, and the power to detect an effect of the arraydiff variable exceeded .8 when at least 1/3 of subjects increased their array index by an average of 2 points from the first to last block of training.  

Further exploratory analysis will be used to scrutinise the response profiles on the training task for those subjects who do show learning, with the aim of determining whether they are sensitive to log likelihood, whether they simply learn a strategy of waiting for a larger array size, or appear to adopt some other strategy.  

The positive control P-items from the quiz will be scrutinised to check that the subjects are competent in their knowledge of basic probability. If this is not the case, then it would undermine the interpretation of poor performance on S-items. 


## Interpretation of pattern of results
We can refer to the pattern of results on the training task and the quiz as N, I or Y, indicating null result, inconclusive, or prediction confirmed. 
The first possible pattern is NN: neither the training task nor the quiz shows any improvement in sampling variation blindness. If this pattern were observed, we would consider modifying the task. There are various ways one could do this: by changing the visual display, or by changing the reward schedule. It would, of course, also be possible to extend the training, although this might require having more than one session. In effect, this study could then act as a baseline against which to evaluate other approaches to training.  

We may find that we find significant improvements with training, but null or inconclusive results on the quiz, this would indicate a lack of generalisation. If so, we will explore individual S-items in the quiz, to see whether there is any indication of improvement on a subset of items, and whether they have particular characteristics. We may, for instance, see evidence of selective improvement on items assessing understanding of the relationship between sample size and statistical power.  

The converse pattern might be obtained, with improvement on the S-items of the quiz but null or inconclusive findings on the training task. This might justify a further study which uses two training sessions, making it possible to look for gains due to training that occur after a delay.  

Finally, if we are able to show an improvement with this short training session, with generalisation to S-items on the quiz, then it would be of interest to do further studies with modified forms of presentation, to identify the optimum conditions for training. Potentially, this kind of training could readily be incorporated into courses on research methods, and evaluated for its impact on subsequent experimental practices of the trainees.  

## Sample size determination

For each hypothesis, we conducted formal power analysis:
```{r H1power, include=F, echo=F}
alpha=.05
power=.8
d = .2
pH1<-pwr.t.test(d=d,power=power,sig.level=alpha,type="one.sample",alternative="greater")


```
-	Hypothesis 1:  We will conduct a one-sided one-sample t-test testing comparing the mean array size for the first half of training with the optimal value of 5, predicting that it will be less than this value. For this one-sampled t-test, we conducted power analysis using the pwr.t.test function from the pwr package (version 1.3-0; Champley et al., 2020) in R. Assuming a = `r alpha`, and power = `r power`, a sample of `r as.integer(pH1$n)` is required to detect a small effect of d = `r d`.  

```{r H2power,include=F, echo=F}
alpha=.05
power=.8
d = .2
pH2<-pwr.t.test(d=d,power=power,sig.level=alpha,type="paired",alternative="greater")


```



# Appendix 1
## Judgment and Reasoning Quiz

Subjects receive version A or B (counterbalanced) at pre-test and post-test. P-items and S-items are presented intermixed: P-items test understanding of probability, S-items test sampling variability blindness. Items with the same number in version A and B are intended to be analogous but use different contexts. The brief descriptor accompanying each item below is not presented as part of the quiz. An asterisk denotes the correct response.  For S-items, x denotes the anticipated response(s) for those susceptible to sampling variability blindness.

<!---modified from Probqs_edit_from_feedback.rmd--->
### Initial queries (Pretest only)

i. Compared to others at your level of education in your subject, how good do you think your understanding of basic statistics is?
a) Excellent  
b) Better than average  
c) Average  
d) Below average 

ii. How confident are you in interpreting the results of a t-test?  
a) Very confident  
b) Fairly confident  
c) Not confident  
d) Very little idea about what a t-test is  
  
iii. How familiar are you with the idea of statistical power?  
a) Very familiar  
b) Reasonably familiar  
c) Somewhat familiar  
d) Very little idea about what statistical power is  

### Qualitative report (Posttest only)
Did you felt you got better at the task over time?  Yes/Unsure/No

Did you change how you approached the task?  Please let us know if you adopted any specific strategy to guide your response?

   
### P-items
#### P1A (tests elementary knowledge of normal distribution)
You take a sample of 100 men from the general population and measure their height.  The mean is 70 inches and the standard deviation is 4 inches. What percentage of the sample will be expected to be more than 74 inches tall?  
a) *16%  
b) 2%  
c) 50%  
d) 30%  
 
#### P1B  
A reading test is standardized on 7-year-old children in Scotland. The mean reading age is 84 months with standard deviation of 6 months.  What percentage of 7-year-olds is expected to have a reading age of 72 months or less?  
a) 16%  
b) *2%  
c) 50%  
d) 30%  

```{r q1a, include=F,echo=F}
#All quiz items simulated to check accuracy
#NB for accurate estimate we use N = 10000
mydata <- rnorm(10000,70,4)
w<-which(mydata>74)
ans1<-100*length(w)/10000
print(paste0("The answer is ",ans1))

mydata <- rnorm(10000,84,6)
w<-which(mydata<72)
ans2<-100*length(w)/10000
print(paste0("The answer is ",ans2))

```
#### P2A (Basic probability). 
A container is full of spare change containing 100 10p coins, 100 5p coins, 200 2p coins, and 100 1p coins. The coins are randomly mixed. You will get a prize every time you pick a 5p coin. If you make 100 selections, replacing the selected coin and shaking the jar each time, how often will you expect to pick a 5p coin?  

a) 1 in two occasions  
b) 1 in three occasions  
c) 1 in four occasions  
d) *1 in five occasions  
 
#### P2B 
You are choosing marbles from an opaque jar containing 100 red marbles, 100 blue marbles, and 200 white marbles, randomly mixed. You will get a prize every time you pick a marble that is not white. If you make 100 selections, replacing the selected marble and shaking the jar each time, how often will you expect to get a prize?  

a) *1 in two occasions
b) 1 in three occasions
c) 1 in four occasions
d) 1 in five occasions
 

```{r P2B,include=F,echo=F}
myselect<-vector()
mymarbles<-c(rep(1,100),rep(2,100),rep(3,200))
for (i in 1:100){
  myselect<-c(myselect,sample(mymarbles,1))
}
myt<-table(myselect)
names(myt)<-c('Red','Blue','White')
myt
myt/500
 
```

#### P3A (Classic probability including averaging probabilities)
In the city of Ficticium, there is generally a 10% chance it will rain on any given day in the first half of September, a 50% chance it will rain any given day in the second half of September,  a 25% chance it will rain on any given day in November, and a 30% chance it will rain on any given day in December. (September, November and December are all 30 days long). Which month is likely to have more rainy days?  
a) September  
b)  November  
c)  December  
d) *September and December are equally likely  

```{r P3A, include=F, echo=F}
myrain<-data.frame(matrix(NA,nrow=1000,ncol=3))
names(myrain)<-c('Sep','Nov','Dec')
for (i in 1:10000){
sep1<-rbinom(1,15,.1)
sep2<-rbinom(1,15,.5)
nov<-rbinom(1,30,.25)
dec<-rbinom(1,30,.3)
myrain[i,1]<-sep1+sep2
myrain[i,2]<-nov
myrain[i,3]<-dec
}
print ('Mean rainy days')
print(paste('September ',mean(myrain$Sep)))
print(paste('November ',mean(myrain$Nov)))
print(paste('December ',mean(myrain$Dec)))
```

#### P3B. 
After reviewing sales of coffee, the barista noted there is generally a 50% chance of selling an espresso on any day in the first week of the month. There is a 100% chance of selling an espresso on any day in the second week of the month, and a 75% chance of selling an espresso on any day in the third and fourth week of the month. In what two-week period in the month is there most sales of espresso?  
 
a) *First two weeks and second two weeks are equally likely   
b) First two weeks  
c) Second two weeks  
d) First and fourth weeks  

#### P4A. (basic probability)
At a raffle, you can pick from a green bowl with 2 winning raffle tickets and 8 worthless tickets, a yellow bowl with 10 winning tickets and 90 worthless tickets, or a red bowl with 15 winning tickets and 85 worthless tickets.  Which bowl gives you a better chance of winning?  
a) *Green  
b) Yellow  
c) Red  
d) Green and Yellow give the same chance  
 
```{r P4A, include=F,echo=F}
greencount<-yellowcount<-redcount<-0
for (i in 1:1000){
green<-c(rep(1,2),rep(0,8))
yellow<-c(rep(1,10),rep(0,90))
red<-c(rep(1,15),rep(0,85))
greencount<-greencount+sample(green,1)
yellowcount<-yellowcount+sample(yellow,1)
redcount<-redcount+sample(red,1)
}
print(paste('Green p = ',greencount/1000))
print(paste('Yellow p = ',yellowcount/1000))
print(paste('Red p = ',redcount/1000))
```

#### P4B. 
You find three bags of jelly beans. There is a small bag with 5 red jelly beans and 20 other coloured beans. There is a  medium bag with 12 red jelly beans and 38 other coloured beans. Finally, there is a large bag with 18 red jelly beans and 82 other colour beans. As red jelly beans are your favourite flavour, which size bag gives you the highest percentage of red beans?   

a) Small  
b) *Medium  
c) Large  
d) They are all the same  

#### P5A. (conjoint probability)
There are 100 girls in a class, 20% have red hair and 40% are taller than 60 inches.  How many red-headed girls would you expect who are taller than 60 inches?  
a) 60  
b) 16  
c) *8  
d) 5  

```{r P5A,echo=F,include=F}
# This gives a different answer on each run, but converges on 8 (.4*.2)
girls <- c(rep(1,20),rep(0,80))
tallgirls<-sample(girls,40) #random selection from girls
tallred <- sum(tallgirls)
print(tallred)
```

#### P5B
In box of 100 CDs, 40% of the disks are have been recorded by pop artists and 60% of disks feature female vocalists. Assuming that the female vocalists are equally likely to be pop artists or not, how many CDs have been recorded by female pop vocalists?   

a) 12 CDs  
b) 18 CDs  
c) *24 CDs  
d) 30 CDs  
 
#### P6A (classic probability - multiplication)
10 percent of the children in a school have red hair. Their names are put in a hat and you are asked to pull out two of them. What is the probability that you will select two red-headed children?  

a)  20 per cent  
b) *1 per cent  
c)  19 per cent  
d)  close to zero   

```{r q12a, include=F, echo=F}
print(paste(100*.1*.1,' per cent'))
```

#### P6B. 
20 percent of the 500 children in a school have a name beginning with J. Their names are put in a hat and you are asked to pull out two of them. What is the probability that you will select  two children whose name begins with J?  

a) *4 per cent  
b)  1 per cent  
c)  40 per cent  
d)  20 per cent   

```{r P6B,include=F,echo=F}
print(paste(100*.2*.2,' per cent'))
```

### S-items 
#### S1A (frequency of extreme scores in small samples)

History courses in the UK are rated on a Student Survey, which has an overall satisfaction rating on a score of 1 (low) to 5 (high). The mean rating overall is 3.5, with SD of 1. Those who have an average rating above 4.0 are given a gold star in league tables. There are 60 courses altogether, which can be divided according to size into small (less than 10 students), medium (between 10-50 students) and large (50+ students). If there are no real differences in student satisfaction, will the number of students affect the likelihood of getting a gold star?  

a) Yes. Large courses have a better chance of getting a gold star  
b) Yes. Medium-sized courses have a better chance of getting a gold star  
c) *Yes. Small courses have a better chance of getting a gold star  
d) x No. The number of students makes no difference.  

```{r S1A, echo=F, include=F}
size <- c(5,30,60)

for (u in 1:3){
  ustring<-NA
  for (v in 1:50){
mydat<-rnorm(size[u],3.5,1)
ustring<-c(ustring,mean(mydat))
n[u]<-length(which(ustring>4))
}
}



```

#### S1B. 
A task force is looking at characteristics of schools in its area. They take a measure of mathematical ability, and identify schools as 'failing schools' where 20% or more pupils get scores more than 1 SD below the population average. The smallest schools have on average 100 pupils, middle-sized schools have 250 pupils, and the largest schools have 500 pupils. 
If there are no real differences between schools, will the size of the school affect whether it is a failing school?  
a) *Yes. The smallest schools will be more likely to be a failing school  
b)  No. All else being equal, school size should make no difference  
c)  Yes. The largest schools will be more likely to be a failing school  
d)  Yes. The middle-sized schools will be more likely to be a failing school  

```{r S1B,include=F, echo=F}
N  = 1000 #N times sampled
schoolsize<- c(100,250,500)
myschool <- data.frame(matrix(NA, nrow=N, ncol=6))
names(myschool)<-c('Small','Medium','Large','SmallF','MediumF','LargeF')
for (i in 1:N){
  myschool[i,1]<-length(which(rnorm(schoolsize[1],0,1)<(-1)))
  myschool[i,2]<-length(which(rnorm(schoolsize[2],0,1)<(-1)))
  myschool[i,3]<-length(which(rnorm(schoolsize[3],0,1)<(-1)))
  for (j in 1:3){
    myschool[i,(j+3)]<-0
    if (myschool[i,j]/schoolsize[j]>.2){
      myschool[i,(j+3)]<-1
    }
  }
}
print(paste('Out of ',N,' schools, this is number coded as failing, for Small, Medium and Large respectively'))
for (j in 1:3){
  print(sum(myschool[,(j+3)]))
}

```



#### S2A (dependence of accuracy of estimate on sample size, proportions)
You are a forensic scientist who is trying to establish the sex of a skeleton. You have a sample of bone; you know that when you classify the cells men typically have 33% type X and 67% type Y. 
Women typically have 50% type X and 50% type Y. 

Doing the test for cell type is very expensive so you do not want to test more cells than necessary, but it is important to get the right answer. 
You take a sample of 24 cells from an individual and find 8 are of type X. 

Should you:  
a)    x Conclude the skeleton is male  
b)    x Add more cells to give a sample of 48 cells before deciding  
c)   *Add more cells to give a sample of 100 cells before deciding  
d)     Add more cells to give a sample of 200 cells before deciding  
    


```{r S2A,include=F,echo=F}
p1=.33
p2=.5
#conventional power analysis

#h is effect size, which is 2*arcsin(sqrt(p1))-2*arcsin(sqrt(p2))
h<-2*asin(sqrt(p1))-2*asin(sqrt(p2))

pwr.p.test(h=h,n=24,sig.level=.05)
pwr.p.test(h=h,n=48,sig.level=.05)
pwr.p.test(h=h,n=100,sig.level=.05)
pwr.p.test(h=h,n=200,sig.level=.05)
```

#### S2B 

A naturalist notices a new disease affecting parrots. She has 20 specimens with the disease, 14 of which are male. It is important to know whether males are more susceptible than females to the disease, as this will affect how it is treated.

Should she:  
 a)  x Conclude that males are more susceptible to the disease than females  
 b)   Conclude that there is no difference in susceptibility for males and females  
 c)   x Gather more cases to give a sample size of 40 before deciding  
 d)   *Gather more cases to give a sample size of 100 before deciding  


```{r S2B,include=F,echo=F}
h=2*asin(sqrt(.7))-2*asin(sqrt(.5))
#h is effect size, which is 2*arcsin(sqrt(p1))-2*arcsin(sqrt(p2))


pwr.p.test(h=h,n=20,sig.level=.05)
pwr.p.test(h=h,n=40,sig.level=.05)
pwr.p.test(h=h,n=100,sig.level=.05)

```


#### S3A (dependence of power on sample size)
 
A fertiliser is trialled to see if it improves crop yields. Without the fertiliser the average yield is 100, with standard deviation of 10. It is expected that the fertiliser will boost yield by 3 points on average.
How many plants would be needed in the treatment and control groups to be confident of demonstrating whether or not the fertiliser was effective?  

a)  20 per group  
b) 50 per group  
c)  100 per group  
d) *300 per group   

```{r Q7a, include=F, echo=F}
require(pwr)
pwr.t.test(d = .3, sig.level = .05 , power =.95 , type = "two.sample",alternative="greater") 
```
  
#### S3B. 
You have been asked to test a treatment for obesity. People in the trial have a mean body mass index (BMI) of 35, with SD of 5. The developer argues that the treatment will reduce BMI by 2 points on average, but you are dubious as to whether it has any effect. Assuming you have a control group given a placebo and an experimental group given the treatment, what sample size should you select to give a fair test of the treatment? 

a) x20 per group  
b) x50 per group 
c) *100 per group
d) 500 per group

```{r S3b, include=F, echo=F}
require(pwr)
myp<-pwr.t.test(d = .4, sig.level = .05 , power =.9 , type = "two.sample",alternative="greater") 
```


#### S4A.(power with proportions)
Imagine you are a researcher studying a particular rare gene variant that is found in 0.01% of the population (1 in 10,000). You want to test the hypothesis that the variant doubles the risk of a rare disease that affects 1 in 300 people in the general population. You have access to the biobank genetic data from (almost) all residents of a large population country (30 million inhabitants), a medium population country (10 million inhabitants), and a small population country (1 million inhabitants). All else being equal, which country’s biobank data would give you the most accurate answer to your research question?  

a) Smallest country  
b) Medium country  
c) *Largest country  
d) x Does not matter: No difference between countries  
 
```{r q8a, include=F, echo=F}
options(scipen = 999)
myN<-c(30000000,10000000,1000000)
for (thisn in 1:3){
  print(paste('Population total is ',myN[thisn]))
  truedat<-rnorm(myN[thisn],0,1)
trueX<-length(which(truedat>3.09)) #corresponds to .001  N in population with variant
trueN<-myN[1]-trueX #N in population without rare variant.
#Risk of those with variant is 1 in 50, and for those without variant is 1 in 100
trueXP <- round(trueX/150,0)
trueXN <-trueX-trueXP
trueNP <- round(trueN/300,0)
trueNN <-trueN-trueNP
tbl <- matrix(c(trueXP,trueXN,trueNP,trueNN),nrow=2)
colnames(tbl)<-c('Rare variant','Common variant')
rownames(tbl)<-c('Affected','Unaffected')
print(tbl)
print(chisq.test(tbl))
}

```

#### S4B.  
You are a biologist interested in studying a particular disease in plants. The disease affects 20% of the population (1 in 5) and leads to most plants dying. You have developed a spray that you think reduces mortality of the disease by 50%. You have to make the decision about how many plants to study to test the spray: in different centres you could gain access to 150 plants, 300 plants, or 1200 plants. Which centre would be best to conduct your research?  

a) 150 plants  
b) 300 plants  
c) *1200 plants  
d) x Whichever centre is easiest to access  

 
 
#### S5A. (classic from Tversky/Kahneman law of small numbers)
In a squash tournament, the organisers are debating whether to have games of best of 3, 9 or 21 points.  Holding all other rules of the game constant, if A is a slightly better player than B, which scoring will give A a better chance of winning?  
a) best of 3 points  
b) best of 9 points  
c) *best of 21 points  
d) x Won't make any difference  
 
 
```{r S5A, echo=F,include=F}
#NB. Simulating this makes it clear that if A is much better than B, it really doesn't make much difference with original Ns! Fine if we simulate so that prob A winning is .6 and if we make lowest the best of 3.

#Assuming we can treat p(A) win as p
#In fact, this only works if p not too high and if lowest N games is v small
p = .6
games<-c(3,9,21)


  for (j in 1:3){
    seq1 <-0
   for (i in 1:1000){

  thisg<-rbinom(games[j],games[j],p)
  thiswin <- length(which(thisg>games[j]/2))
  if(thiswin>games[j]/2) {seq1<-seq1+1}

   }
    print(paste0('N games is ',games[j]))
    print(paste0('Wins in 1000 = ',sum(seq1)))
}

```

#### S5B. 
Two chess players are having a tournament. They are considering whether to play the best of 5, 11, or 17 games.  If player A is slightly better than player B, which tournament size should player B argue for, to get the best chance of winning?  

a) 17 games  
b) 11 games  
c) *5 games  
d) It doesn't matter: The chances are the same regardless of number of games  
 

#### S6A. (Risk of type II error with small samples)
Two scientists are both trying to test whether a certain new drug affects hunger in mice, by giving them the drug (group D) or a placebo (group P) and then measuring their food consumption. At the start of the experiment, the average mouse eats 10g of food pellets, with SD of 3g.  
Scientist A runs 10 studies with 10 mice in each group, and Scientist B runs 10 studies with 30 mice in each group. Unfortunately for them, a careless lab technician distributed a placebo in place of the new drug, so there should not be any effects except by chance. When the scientists looks at the results, a large effect, difference in food consumption of 3g, is seen between the two groups (D and P) on one run of the experiment. Which scenario is more likely:  

a)  *The run with a large difference is found in the smaller group   
b)    The run with a large difference is found in the larger group  
c)    The group difference shows group D eats more than group P  
d)   x A large difference is equally likely D>P or P>D, with no effect of sample size  
 


```{r S6A,include=F,echo=F}
n1<-10
n2<-30
alldiffa<-vector()
alldiffb<-vector()
for (i in 1:10){
miceDa<-rnorm(n1,10,3)
micePa<-rnorm(n1,10,3)
miceDb<-rnorm(n2,10,3)
micePb<-rnorm(n2,10,3)
diffa<-mean(miceDa)-mean(micePa)
diffb<-mean(miceDb)-mean(micePb)
#print(paste0('Mean diff n8 = ',
#             diffa))
#print(paste0('Mean diff n16 = ',
#             diffb))
alldiffa<-c(alldiffa,diffa)
alldiffb<-c(alldiffb,diffb)}
print('Differences for each run of small group: ')
      print(alldiffa)
print('Differences for each run of large group: ')
      print(alldiffb)
```

#### S6B.
Two researchers are testing the effect of oxytocin on prosocial behaviour. In their experiments, participants are given either a dose of oxytocin or a placebo. Researcher A runs 5 studies with 10 participants in each condition and Researcher B runs 5 studies with 20 participants in each condition. Unknown to the researchers, the oxytocin had been replaced by a placebo, yet a large difference (1 SD) is observed between two groups on one run of the experiment. Which is most likely:  

a) *The large difference is seen in a study by the researcher using groups of 10  
b)  The large difference is seen in a study by the researcher using groups of 20  
c) x The difference is a type II error, and could equally likely be seen with groups of 10 or 20  
d)  The difference is real - there was confusion between oxytocin and placebo  

 
## References
Clarke, E., & Sherrill-Mix, S. (2017). ggbeeswarm: Categorical Scatter (Violin Point) Plots. R package version 0.6.0. https://CRAN.R-project.org/package=ggbeeswarm  

Makowski, D. (2018). The Psycho Package: An efficient and publishing-oriented workflow for psychological science. Journal of Open Source Software, 3(22), 470.  
  
Meyer, G. J., Finn, S. E., Eyde, L. D., Kay, G. G., Moreland, K. L., Dies, R. R., Eisman, E. J., Kubiszyn, T. W., & Reed, G. M. (2001). Psychological testing and psychological assessment: A review of evidence and issues. American Psychologist, 56(2), 128–165. https://doi.org/10.1037/0003-066X.56.2.128

Pederson, T. L., & Robinson, D. (2020). gganimate: A Grammar of Animated Graphics. R package version 1.0.7. https://CRAN.R-project.org/package=gganimate  

R Core Team,. (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org/  
  
Wickham, H. (2016). ggplot2: Elegant Graphics for Data Analysis. Springer Verlag.  

