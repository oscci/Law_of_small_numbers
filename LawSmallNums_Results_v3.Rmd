---
title: 'Stage 2 Registered Report: Can we shift belief in the ''Law of Small Numbers''?'
author: "D V M Bishop^1^, Jackie Thompson^1,2^ & Adam Parker^1^"
date: "null"
output:
  html_document:
    df_print: paged
  word_document:
    reference_docx: mystyle.docx
    keep_md: yes
csl: vancouver.csl
bibliography: LawSmallNums.bib
---

```{r setup, include=FALSE,echo=FALSE}
# Currently this won't knit because of CiteprocParseError

# Based on script from stage 1 - updated 17 Sep 2021
# NB This script reads output from two other scripts
# 'gorilla_spreadsheets/spreadsheet1.csv' (created by make_gifs_datasim_script.R)
# 'powsummary_1000_A12_pgood0.5_0.66_props0_0.25_0.33_0.5_N50_75_100.csv' (created by simulate_power_earnings.rmd)

#The spreadsheet1.csv sheet should be saved in a subdirectory called 'gorilla_spreadsheets'

#https://fishandwhistle.net/post/2020/getting-started-zotero-better-bibtex-rmarkdown/

date()
knitr::opts_chunk$set(echo = TRUE)
require(beeswarm)
require(tidyverse)
library(RColorBrewer) #used later for 'zone of uncertainty' plots

library(dplyr)
library(yarrr)
library(ggplot2)
library(lme4)
library(MASS)
library(effects)
library(simr)
library("ggbeeswarm")
library(pwr)
library(flextable)
library(table1)
library(stringr)
library(psycho) #for dprime
library(rstatix)
library(ggpubr)
library(gridExtra) #multiple plots together
options(scipen=999)
load("smallnums.RData")  #reloading variables from previous session that are referenced in Abstract
tabnumber <- 0 #for counting tables.
fignumber <- 0 #for counting figures
subn <- c(50,75,100)
allarrayvals <- c( 1,1,2,2,2,2,3,3,3,3,3,3,4,4,4,5,5,5,6,6) #distribution of array values used in simulation for block 1

authorversion <- 0 #this has extra tables and figures for the authors to look at - set to 1 to see these:  set to 0 for submitted stage 2 RR.



```

```{r makepvalueformat, echo=F}
#make format for p-values from t-test or other objects with p.value
p.format=function(mytest){
if(mytest$p.value <.001){p.report <- '_p_ < .001'}
if(mytest$p.value >.001){p.report <- paste0('_p_ = ',round(mytest$p.value,3))}
return(p.report)
}
```


Affiliations:  
^1^ Department of Experimental Psychology, University of Oxford, Anna Watts Building, Woodstock Road, Oxford, OX2 6GG. 

^2^ School of Psychological Science, University of Bristol, The Priory Road Complex, Priory Road, Clifton, BS8 1TU.  
  
Acknowledgment: This work was supported by Wellcome Trust Programme Grant no. 082498/Z/07/Z.

## Abstract 
<!---200 word limit for RSOS  - Interesting! It was 100 words when we submitted RR and I complained --->
‘Sample size neglect’ is a tendency to underestimate how the variability of mean estimates changes with sample size. We studied 100 participants, from science or social science backgrounds, to test whether a training task (the ‘beeswarm’ task) showing different sized samples of datapoints can help overcome this bias. Ability to judge if two samples came from the same population improved with training,  and 38% of participants reported that they had learned to wait for larger samples before making a response. Before and after training, participants completed a 12-item estimation quiz, including items testing sample size neglect (S-items). Bonus payments were given for correct responses. The quiz confirmed sample size neglect: 20% of participants scored zero on S-items, and only 2 participants achieved more than 4/6 items correct.  Performance on the quiz did not improve after training, regardless of how much learning had occurred on the beeswarm task. Error patterns on the quiz were generally consistent with expectation, though there were some intriguing exceptions that could not readily be explained by sample size neglect. We suggest that training with simulated data might need to be accompanied by explicit instruction to be effective in counteracting sample size neglect more generally.
 
#### Keywords:  statistical reasoning, power, online training, sample size neglect

This manuscript received Stage 1 in-principle acceptance (IPA) on 12 August 2021, prior to data collection and analysis. The approved Stage 1 version of this manuscript, unchanged from the point of IPA, is preregistered at https://osf.io/fermv.
 



```{r makefig1, include=FALSE, echo=FALSE}
#This chunk just generates figure 1 - it could probably be made more efficient
fignumber <- fignumber + 1
samplesizes <- c(10,60)
set.seed(2)
mymean <- 178 #mean  - can vary this - here this is in raw units
mysd <- 10 
nsim <- 6
plotname <- paste0('Fig1.tiff')
tiff(plotname, width = 8, height = 4, units = 'in', res = 300)
par(mfrow=c(1,2))
for (i in samplesizes){
    thisdat<-rnorm(i*nsim,mymean,mysd)
    thisset <- rep(1:nsim,i)
    alldat<-data.frame(cbind(thisdat,thisset))
    colnames(alldat)<-c('Height','Set')
    beeswarm(thisdat~thisset,cex=.75,pch=16,col='darkgrey',ylim=c(160,200),xlab='Sample',ylab='Height (cm)',main=paste0('Sample size ',i))
    mymeans<-aggregate(alldat$Height,by=list(alldat$Set),FUN=mean)
    for (n in 1:nsim){
    segments((n-.3),mymeans$x[n],(n+.3),mymeans$x[n],lwd=2,col='red')
    }
    abline(h=mymean,lty=1,col="blue")
}


dev.off()
```
## Introduction

Compared to laypeople, scientists receive extensive training to help them understand and appropriately address the uncertainty of evidence. Yet many scientists fall short in their understanding of statistical concepts. One cognitive bias demonstrated by @tversky1971 is the 'belief in the law of small numbers'. This refers to the tendency to overestimate the stability of estimates that come from small samples - which, following @yoon2021, we shall term 'sample size neglect'. For instance, as shown in Figure 1, if repeatedly sampling `r samplesizes[1]` men from a population, the mean height of the sample will be far more variable than when repeatedly sampling `r samplesizes[2]` men. People understand that sample size does not affect the expected mean value, but they tend not to appreciate that it has a large effect on the standard error of the mean (i.e. variability of the red bars). This has implications for understanding of statistical power, i.e. the numerical relationship between sample size and ability to detect a true effect. Sample size neglect can help explain why so many studies in psychology, and indeed many other scientific disciplines, are underpowered. 

![Six independent samples of simulated male height, each with sample size of 10 or 60. Each point represents the height of one male, drawn from a population with mean 178 cm and SD 10 cm, and the red bar is the mean for that sample. The horizontal blue line represents the population mean.](Fig1.tiff)

More than half a century ago, @cohen1962 embarked on a project of improving psychologists' understanding of statistical power, providing tools to help people compute power and documenting the extent of underpowered studies in social psychology, with the aim of reducing waste in research efforts. He analysed 70 studies published in the Journal of Abnormal and Social Psychology and found that mean power to detect small effects was 0.18, to detect medium effects was 0.48 and to detect large effects was 0.83. Given that most effects in this field are small or medium, this indicated serious limitations of study design.  However, 27 years later, @sedlmeier1989 reported that things had not changed at all. And in 2016, similar conclusions were drawn from a large review of studies in social and behavioral sciences up to 2011 [@smaldino2016]. In 2018 a review concluded that low power remains a major factor explaining poor replicability of highly cited studies in psychology [@stanley2018]. A wide range of areas are affected, from neuroscience [@poldrack2017], to infancy research [@oakes2017]. Why, despite years of attempts to improve the dismal record of low power in psychology, do researchers persist in performing underpowered studies? We suspect the explanation may go beyond lack of training, and reflect the influence of sample size neglect, which leads us to have intuitions about sample size that are at odds with reality. Consider this example [@tversky1971]:

_Suppose you have run an experiment on 20 participants, and have obtained a significant result which confirms your theory (z= 2.23, p < .05, two-tailed). You now have cause to run an additional group of 10 participants. What do you think the probability is that the results will be significant, by a one-tailed test, separately for this group?_
p.105.  

Tversky and Kahneman reported that the majority of researchers who responded to this question were wrong in stating that the probability is somewhere around .85, while only 9 out of 84 researchers gave a more accurate answer (i.e. between .40 and .60). This is thought to reflect inaccurate beliefs in sampling that have unfortunate consequences in the course of scientific enquiry. Researchers view randomly drawn samples as highly representative of the population to a greater extent than theory predicts, at least for small samples.  

If we are to tackle the problem of wasteful and misleading underpowered studies, we need to find ways to overcome sample size neglect. There is some evidence that debiasing training can help: @yoon2021 showed improvements on a scale measuring this construct after either direct instruction, direct playing of a game designed to train awareness, or observing another person play the game. However, the scale items assessed rather broad aspects of generalising from a few instances, e.g. respond on a 7-point scale: _"Micah's 10-year-old daughter Felicia scores two goals in her very first soccer game. Based on this, Micah proudly predicts that Felicia will be the top scorer for her team for the year (25 games). How confident are you in Micah's prediction?"_ Our goal was to go beyond a general appreciation of the dangers of relying on small samples to help scientists obtain a more intuitive understanding of statistical power, 

Informally, we have found that exposing students to simulations that allow them to visualize the variation between samples of different sizes can help counteract over-reliance on small samples to evaluate hypotheses. By generating datasets with known effect sizes and drawing random samples and subjecting them to statistical tests, students can learn to appreciate the ease with which we miss a true effect if the sample size is small. Simulations are a core aspect of a course outlined by @steel2019 which trains statistical thinking in undergraduates, using simulations to help interpret patterns in data and to evaluate statistical power. Although their course appears successful in training statistical thinking in the long term, they did not specifically evaluate the utility of simulations in counteracting sample size neglect. That is the goal of the current study.  

In addition to training, simulations have been used to examine the decisions made by researchers. @morey2019 used simulations to examine scientists’ understanding of the logic of significance testing. In their task, scientists were asked to perform a series of experiments to judge which two groups of elves could make more toys based on a randomly assigned group difference between 0 and 1 standard deviations. Sample size and the test statistic were not shown to participants: instead they were shown displays that represented numerical information in terms of colour and location, with an opportunity also to view "random shuffle" displays where they were told there was no true effect. Of the 136 participants for whom the null hypothesis was true, 86% correctly indicated no effect. When there was a true effect, correct decisions increased as a function of effect size.  When the effect size was .3, accuracy was approximately 80%. For larger effect sizes, accuracy approached ceiling. When asked about the heuristics that they applied, 72% indicated using strong significance testing strategies. Thus, Morey and Hoeskstra highlighted the potential utility of simulation-based training to answer questions about how researchers use information. However, their study only indirectly addressed sample size neglect, because participants were not told what the sample size was, and were encouraged to explore the sampling distribution for means when the effect size was zero, which made it relatively easy to see whether a given point fell within that distribution. Our focus is rather on the more lifelike situation when the participant knows the sample size, and has to consider whether an observed distribution of scores is more likely to have come from a population with a true or null effect. As well as evaluating the efficacy of training on statistical judgments, we will consider whether participants show evidence of adopting specific heuristics, such as waiting for a given sample size before making a judgment, or simply basing responses on observed effect size.


## Online training task (beeswarm task)
To explore whether exposing people to simulated datasets can help develop a more intuitive sense of the relationship between variability of estimates and sample size, we created an online task that mimics the real-world process of gathering and interpreting data in the life and social sciences. In this task, participants (i.e. scientists)  visually compare the distribution of simulated datasets and assess whether the samples come from a population where there is either a true effect (samples drawn from populations with differing means) or null effect (samples drawn from the same population). Potentially, the size of the effect in the population can be experimentally manipulated, but for the current study we focus on cases where the true effect size is Cohen’s _d_ = .3 (equivalent to _r_ = .15). This effect size was selected because, it is on the one hand fairly typical of the kind of effect size obtained in many areas of psychology, biomedicine [@meyer2001] and education [@dietrichson2021], and on the other hand there is a dramatic increase in the confidence with which results can be interpreted as favouring the true or null hypothesis as sample size increases (Figure `r (fignumber+1)`).  

```{r makerNdata,echo=FALSE,include=FALSE}
fignumber <- fignumber+1
keepfignumber <- fignumber #want to refer back to this one
samplesize <- 100 #set to size of largest group 
numrange<-c(10,20,40,80,160,320)
nset <- length(numrange)
sampledat<-data.frame(matrix(NA,nrow=samplesize,ncol=nset)) #dummy frame to hold simulated data on samplesize cases and nset columns
#prepare plot

ES<-.3
plotname <- paste0('Fig',fignumber,'.tiff')
tiff(plotname, units="in", width=6, height=4, res=300)


Nsample<-50

for (n in numrange){
  sampledat <- data.frame(matrix(NA,Nsample*2,3))
  colnames(sampledat)<-c('N','mean','ES')
  sampledat$N<-n
  thisrow<-0
  for(es in c(0,ES)){
    for (ns in 1:Nsample){
      thisrow<-thisrow+1
      sampledat$mean[thisrow] <- mean(rnorm(n,es,1))
      sampledat$ES[thisrow]<-es
    }
  }
  
  if(n==numrange[1]){ alldat<-sampledat}
  if(n>numrange[1]){alldat<-rbind(alldat,sampledat)} #bolt on next sample size in long format
}


#specify pink colour for control group, and blue for experimental group
alldat$colour <- 'deeppink'
w <- which(alldat$ES==ES)
alldat$colour[w]<-'blue'

beeswarm(alldat$mean~alldat$N,pwcol=alldat$colour,cex=.7,pch=16,xlab='Sample size',ylab='Observed effect size',ylim=c(-.7,.9))
abline(h=ES,lty=3,lwd=2,col='blue')
abline(h=0,lty=3,lwd=2,col='deeppink')
text(.6,-.62,'Power:',cex=.8)
for (samplesize in 1:6){
  mypower <-  power.t.test(n = numrange[samplesize], delta = ES, sd = 1, sig.level = 0.05,
                           type = c("two.sample"),
                           alternative = c("one.sided"))
  text((samplesize),-.7,round(mypower$power,3),cex=.8)
}
dev.off()
```

![Simulated mean scores from samples of varying size, drawn from populations with either a null effect (pink) or a true effect size, Cohen's _d_, of `r ES` (blue). Power is the probability of obtaining p < .05 on a one-tailed t-test comparing group means for each sample size. ](Fig2.tiff)

<!--We use the copy jpg here, as it doesn't have points that overlap the labels-->

Participants were initially presented with underpowered samples: they could either respond immediately (true effect or no effect) or wait to see the sample size increase. Participants' subjective uncertainty was indexed by how long they wait to see more data. Ultimately, this paradigm not only measures perceptions of uncertainty; it is designed to help participants develop an intuitive sense of the uncertainty underlying even convincing-looking mean differences from small sample sizes. 

We also examined the potential of this task to enhance participants' statistical reasoning by administering a pre- and post-training quiz, testing statistical reasoning using questions that assess sample size neglect.  The quiz also included questions testing understanding of basic probability,  which act as positive controls: if participants score below 4/6 correct on the positive control questions, then this would indicate they either have little training in statistics, or are not taking the task seriously.

We pre-registered five predictions (see below) from a three-step hypothesis:    
a) Even people who have a reasonable grasp of probability theory suffer from sample size neglect.  
b) This neglect can be ameliorated by providing training that involves exposure to different sample sizes drawn from a population.  
c) Training will change understanding of how sample size affects accuracy of estimates, and this will be evident beyond the beeswarm task.  

## Ethics statement
The protocol has been approved by the University of Oxford’s Medical Sciences Interdivisional Research Ethics Committee, approval number (R60658/RE001).


# Method

## Participants
Criteria for participants were: (1) age of 18 years or over, (2) have studied life or social sciences for at least one term at undergraduate (bachelor’s degree) level. We specified that we would recruit up to 100 participants via the online research platform Prolific (www.prolific.co) and social media platforms. The maximum sample size was determined from a power calculation (see section on Simulated data for sample size determination, below) indicating that this is sufficient to detect the case when 33% or more of participants improve their performance on the beeswarm task. 

## Procedure  
After providing informed consent, participants completed the estimation and judgment quiz online at a time and place of their choosing, followed (after training) by an alternative form of the post-training  quiz. Participants were paid £7.50 for their time, but could earn a further bonus linked to the number of correct items on the quiz (3p per correctly answered item on each version), and their score on the training game to bring the payment up to a theoretical maximum of £12.  

Training was divided into four blocks, and after each block, participants were told that they could take a break if they wish. The study was implemented using Gorilla [@anwyl-irvine2020], a cloud-based research tool for behavioral studies. The study took around one hour to complete.
<!---we could check this by looking at timings-->
  
## Design
Within-task learning was assessed in a within-subjects design with one factor with two levels (block of trials, comparing the first and last blocks, each with 20 trials). To avoid any confound between specific training items and sequence in training, a predetermined set of items was presented in new random order for each participant.

In addition, transfer of training was assessed using the estimation quiz, with parallel forms administered in a within-subject pre-post-test design.  

### Judgment and reasoning quiz
The quiz (Appendix 1) was designed for this study, and was presented to participants as a 'judgment and reasoning quiz' to minimise negative reactions from those who might feel they are poor at statistics. It has two parallel forms, each of which contains 12 multiple-choice items with 4 choices each. Six items test knowledge of general probability (P-items) and six test knowledge of how sample size determines accuracy (S-items). The parallel forms were counterbalanced, so half the participants received form 1 at pretest and form 2 at post-test, and half received the opposite order. The main dependent variable is the number of items correct for S-items, but the pattern of error responses is also reported. A previous version of the quiz was piloted with 21 participants (Pilot 1), confirming that the difficulty level is appropriate, avoiding ceiling effects. On the basis of pilot testing and suggestions of reviewers, some items were reworded for clarity and to make S-items more relevant to the training.

### Independent and dependent variables
On the beeswarm task, the independent variable is block: there are 4 blocks each of 20 items, but our focus is on comparing blocks 1 and 4. The dependent variables are (a) earnings in the game, which reflects both proportion correct for each block, and ability to select the optimal array index at which to respond (see below); (b) mean array index per block, 1 to 6, corresponding to the array size (10, 20, 40, 80, 160, 320 per group) at which the response is made.  In pilot testing we found that the earnings score is highly correlated with percentage correct (r = .989). 

For analysis of self-reported response strategies, we use the comments in the free text-box asking about specific strategies adopted in the beeswarm task, which were coded by two independent raters as a binary variable: does or does not mention how accuracy is higher if they either wait for more information or focus on larger array sizes. This classification can be used as an independent variable to predict learning gain. On the quiz, the independent variable is session (pre-training or post-training) and the dependent variable is proportion correct for S-items and P-items.  


### Online training
**Instructions**:  After the estimation quiz pre-test, participants are introduced to the game, as follows: 

_Imagine you are a researcher running a large range of experiments. All of the experiments are comparing a control group with a group receiving an experimental treatment (intervention) of some sort. You are testing whether the experimental group has a higher score than the control group after the intervention/treatment._   

_For our purposes, it doesn’t matter too much what the research topic is – the same principle applies to many different types of experiments. E.g., you could be examining:_  


* children’s test scores after an educational intervention versus no intervention  
  
* mouse weight after taking a drug versus a placebo  

* changes in cholesterol levels in people following an experimental diet versus no diet  
  
* effects of a chemical substrate on bacterial cell growth, compared to a control substrate (each data point measures growth of one cell culture)  

_You can choose which of these topics is most relevant to you, and keep that example in mind._  

_Here we assume data in these trials to be normally distributed – i.e., the distribution of scores will fit a bell curve, with most data points close to the middle, and fewer data points at the extremes of very high or very low values._    

_The onion shapes to the right are both (sideways) normal distributions: the width of the shape represents the proportion of data points in this population with a given value on the y-axis. The boxplot in the centre of the plot shows information about the mean and variance of distributions for each condition._  

_The top image on the right compares two distributions separately; the bottom makes a side-by-side comparison. Feedback in this game will be shown like the bottom image._  

_If there is no effect of the experimental treatment, the overall distribution of the data will look like the figure on the bottom right, with the distribution of scores for the experimental group looking very similar to that of the control group._  

_In this game, you will see data from experimental trials. On 50% of trials the data come from a population where there is a true effect of the intervention, which increases scores by 0.3 SD units. (i.e. the effect size is 0.3).  On the other 50% of trials, the intervention has no effect._  

_Your aim is to earn as many points as possible by identifying which trials show data from a population in which there is a true effect, and which trials come from a population with no effect._  
  
_The goal is for you to get a sense of how big a sample you need to make this judgement._  

_The game will include 80 trials in total, with breaks in between each block of 20 trials._  

_Every trial will start by showing a few data points from each group._  

_At that stage you can EITHER:_  
 _judge whether there is a true effect or no effect, by pressing one of the buttons below the data,_  
  
OR   
  
_you can wait a few seconds for the screen to automatically advance and show you more evidence (additional data points)._

_Collecting more evidence can make you more sure of your decision._   

_**Please note: this study does NOT involve any deception. The feedback you get will be accurate, even if it is sometimes surprising.**_  

_Each trial will start with an image like this, showing a set of data points: one from the control group, and one from the experimental group._  

_Along the y-axis you will see scores for each data point. Along the x-axis you will see how many data points there are in each group. Data points for the control group are pink and data points for the experimental group are blue. Each screen will start with 10 data points per condition._  

 _If you wait for more evidence, then more coloured data points will appear._  

_Each new array of data points doubles in size._  

_You can wait for more data until the target sample size of 320 is reached._  

_While data accumulates, you will have to judge whether you think the samples show a true effect or no effect._  

_In other words -- do these samples come from populations with different means, or the same mean? _  

_If you think these samples came from populations that truly differ, respond with **Blue > Pink**._  

_If you think these samples came from populations that are the same, respond with **Blue = Pink**._  

_<u>Once you answer you will hear a *bloop* sound indicating that your response has been recorded.</u>_  
_<u>After your response is made, the display increases until there are 320 datapoints  in each condition.</u>_  
_You will then see a feedback slide telling you whether you answered correctly and displaying how many points you earned._  

_On each trial, you earn 4 points for a correct answer, but you lose 4 points if you make a mistake._  

_You can also earn 2 bonus points if you make a correct response at the optimal array size.  This is the smallest array size at which the likelihood of one scenario (true effect or null)  is 40 times greater than the other.  You should get a sense of what this looks like as you go through the game. The optimal array size will vary from trial to trial, just because of chance factors._  

_<u>You should try to maximise your score as we will convert your points into pennies and add these to your payment.</u>  Don't worry, though: if you make lots of errors and end up with a negative score, we won't subtract anything from the basic payment for this session._  

The participant is then shown some examples that demonstrate what the stimuli look like, and the reward schedule is explained.  

At the end of training they are asked if they thought they had got better at the task over time (options: Yes, Unsure or No) and whether they adopted any specific strategy to guide their responses (free text response box).  

**Training items**  
Gifs for training items were generated by a purpose-designed script which is available on Open Science Framework (https://osf.io/x65nk/?view_only=b0fcb097cc3044aaa942f15136441c49). For each item, samples of observations were selected in a cumulative fashion, such that, for instance, the sample with 20 observations per group included the sample with 10 observations per group, plus an additional 10 observations.  The array sizes doubled with each step, corresponding to 10, 20, 40, 80, 160 and 320 observations per group.  

All items included a pink sample from a population with mean of zero and SD of one, plus a blue sample. For the blue sample, half the items were drawn from the same population as the pink sample (i.e. null effect) and half were drawn from a population with mean of 0.3 and SD of one (i.e., true effect). `r fignumber <- fignumber+1` Figure `r fignumber` illustrates a set of samples shown side by side for a trial with a true effect. Figures were created using the R programming language [@rcoreteam2020] using the _geom_beeswarm()_ function from the _ggbeeswarm_ package (version 0.6.0) [@clarke2017] in conjunction with _ggplot2()_ (version 3.2.1) [@wickham2016]. Each plot was combined into a gif using the _transition_states()_ function from the _gganimate_ package (version 1.0.7); [@pederson2020]. In Pilot 1 (see below) we showed only the data points corresponding to the two datasets, but, as discussed below, for Pilot 2 we added the sample mean shown as a horizontal bar with the aim of improving accuracy.  

![Sample item from the training set. The pair of distributions corresponding to each sample size is presented in a gif one at a time sequentially for 2 seconds, so the display builds up from left to right over time.](demoitem.jpg)

Items from each figure were animated so that each pair of pink-blue observations at a given array size was shown for 2 s, before the next array size appeared.  With this method, response latencies can be directly converted to the array size at the point of response, i.e. responses under 2 s correspond to array size of 10 per group, those under 4 s to an array size of 20 per group, and so on. For analysis, array is coded as an array index from 1 (10 per sample) to 6 (320 per sample).

Participants received feedback after each response, in the form of a display showing points earned or lost, plus total points so far, and a violin plot showing the full distribution from which the points were drawn `r fignumber <- fignumber + 1` (see Figure `r fignumber`).  

![Feedback display for an item with a true effect.](E3_AB copy.png) 

Data from the training trials were saved as response latency, which was converted to array size, as described above. For data analysis, array size (10, 20, 40, 80, 160, 320) was converted to array index, ranging from 1 to 6.  In addition, stored with each array is the amount earned, true  effect size in the population on that trial (Null or 0.3), coding of each response as correct (1) or incorrect (0), the observed effect size on that trial, and the log likelihood of a true vs null effect obtained from the observed data. The latter two variables could potentially be used in exploratory analyses when evaluating which cues participants base their responses on.

This basic task evolved over three pilot studies, which led to improvements in the design. A document with details of pilot tasks and results is available on Open Science Framework (https://osf.io/s39qd/). In two of the pilot tasks (1 and 3), no learning occurred, but in Pilot 2, there was clear evidence of learning. Accordingly, we used this version of the task.  As well as exploring the influence of changes to the displays viewed by participants, we recognised the importance of manipulating incentives to make participants take the task seriously.  In the current version we adopted the method used in Pilot 2, which was to encourage thoughtful responding by explaining the idea of an optimal array size - which is the smallest array size at which the odds in favour of either the null or true hypothesis reaches 40:1 (absolute log likelihood of 3.68) or above.  We also converted the total positive points earned (4p for correct, -4p for error, plus bonus) into a financial bonus added to the basic payment for participation.  



## Analysis plan
### Exclusionary criteria
Our pre-registration stated that if participants did not complete the post-task quiz they would still be included in the main analysis of learning effects, but replacement participants would be recruited to ensure a sufficient sample size for the pre-post training analysis of the quiz results. In practice, this was not needed, as all participants completed pre- and post-training quizzes.  

We also specified we would exclude from the main analysis any participants who scored 5 or 6 correct on S-items in the pre-training quiz, as they would be people with a good prior understanding of sample size effects, so would be unlikely to show learning gains. This led to exclusion of `r N2`/100 participants. 

For the regression analysis, we used an index of learning which corresponds to the difference in earnings between first and last blocks. 


## Data analysis  
Preregistered analyses were designed to test 5 predictions that follow from the three-step hypothesis outlined above:  

_Prediction 1_:  Responses will reflect overconfidence in small samples. A simple preliminary check will be made using a one-tailed _t_-test to check whether the mean number of S-items correct on the pre-training quiz is less than 5/6. If this prediction is not confirmed, then this would undermine hypothesis (a) and hence the rationale of the study. Thus this test may be regarded as a form of positive control.  In addition, a chi-square analysis of error responses was conducted on pre-training S-items to test the expectation that errors on S-items would not be random, but would cluster on responses that indicate either that sample size is not important, or, if asked to select an optimal sample size, involve selecting one that is smaller than required. (These responses are marked x in the Appendix). <!--need to check Appendix as some items did not have x marked-->

_Prediction 2_: Accuracy will increase with training.  
We anticipated replicating the results of Pilot 2, showing improved performance, as reflected in earnings, with exposure to the beeswarm task. A one-tailed matched pairs _t_-test was used to compare earnings on block 4 with earnings on block 1, with the prediction that the block 4 score would be higher. 

_Prediction 3_: Increased accuracy will be associated with selection of larger array sizes as learning proceeds  
This is a subsidiary prediction to prediction 2, which may throw light on the heuristics used by successful participants, if learning is observed. We predicted that, as found in Pilot 2, there would be a significant association between mean array size and mean earnings, such that those who earn most would be those who selected larger array sizes (tested by significance of Pearson correlation between these variables in block 4). 

_Prediction 4_: Self-reported strategy will be predictive of learning  
Again, this was a subsidiary prediction to prediction 2, with potential to clarify which cues are used by successful learners. We divided the self-reported responses into those that did (N = 37) and did not (N = 61) indicate awareness that larger sample sizes (or longer waiting) gives more reliable estimates. This was tested using a one-tailed independent groups _t_-test. Consistent with Pilot 2, we predicted that those who showed such awareness will obtain higher earnings in the final block than those who do not. 

_Prediction 5_: Generalisation of learning  
Individual differences in learning on the beeswarm task will predict improvement on the estimation quiz, specifically for items that are designed to assess sample size neglect. 

The prediction was tested using linear regression:  
_postS ~ preS + earn.diff_,    
where postS and preS are post-training and pre-training scores on S- quiz items, and earn.diff is the difference in earnings (a measure of success on the task) between the last and first block of training. This simple approach to analysis was settled upon after using simulated data to conduct power analysis comparing linear regression versus a linear mixed models approach. The script for simulation is available on OSF (https://osf.io/kcrvw/files/).

Further exploratory analysis were used to scrutinise the response profiles on the beeswarm task for those participants who did show learning, as well as their self-report of strategies, with the aim of determining which cues they relied on when making a response (see Exploratory analyses, below). 

The positive control P-items from the quiz were scrutinised to check that the participants were competent in their knowledge of basic probability and motivated to respond accurately. A preregistered criterion was that if more than half the participants scored less than 3/6 correct on P-items, then this would undermine the interpretation of poor performance on S-items. 

## Stopping rule  
We proposed an initial check after testing 30 participants to see whether Prediction 1 was supported, indicating that participants on average show sample size neglect. This was confirmed. If it had not been, then we would have halted data collection, and not proceeded with the Registered Report, as the basic premise of the study would not be supported. As noted above, Prediction 1 acted as a positive control. 

Assuming the study continued, we proposed to stop and check results after testing 50 participants, and then after each increment of 25 participants. Because power depends crucially on the proportion showing learning, and the increase in percent correct with learning, both of which were unknown, we planned to adopt a Bayesian stopping rule at sequential stages in data collection, starting with a sample of `r subn[1]`, and then increasing to `r subn[2]` and `r subn[3]` if necessary to meet our pre-specified criteria. We planned to compute Bayes Factors for the _t_-test analysis testing hypothesis 2, and the linear regression for testing hypothesis 5.  When Bayes Factors yield evidence in favor of the alternative or null hypothesis for both analyses, we would stop data collection.  


## Exploratory analyses: pilot data
To complement the analysis of self-reported strategies (Prediction 4), the pattern of responses on the beeswarm task was explored to see whether individual participants adopted specific strategies. Accuracy scores were used to compute signal detection indices, d prime and beta, for each block of training, based on the number of hits (respond yes when there is a true effect); the number of misses (respond no when there is a true effect), the number of false alarms (respond yes when there is a null effect) and the number of correct negatives (respond no when there is a null effect). Calculations of these indices was conducted using the _dprime()_ function from the _psycho_ package (version 0.5.0) [@makowski2018]. The beta values allow us to detect cases of response bias, i.e. a tendency to always respond 'blue=pink' or 'blue>pink'.  

As shown in the power values in Figure `r keepfignumber`, with an effect size (Cohen's _d_) of .3, participants need to wait for an array of at least 80 per group (index 4) to have a reasonable chance of success (above 50%) for detecting a true effect, and would be well-advised to wait for the largest array (index 6), which would virtually guarantee success.  However, the trial by trial variation allows us to do a rather more sensitive analysis, testing how the evidence available on each specific trial is used.  

Figure `r fignumber+1` illustrates the logic. The figure shows how evidence accumulates over a series of trials, labelled A to G.  

```{r readspreadsheet,include=FALSE,echo=FALSE}

fignumber <- fignumber+1
plotname <- paste0('Fig',fignumber,'.tiff')
tiff(plotname, units="in", width=8, height=5, res=300)
demospread <- read.csv('gorilla_spreadsheets/spreadsheet1.csv')
demospread$trueES <- demospread$ES+1
demospread$trueES[demospread$trueES==1.3]<-2
levels(demospread$trueES)<-c(2,1)
par(mfrow=c(1,2))
mycols<-c('red','blue')
#plot obs ES
EScol1 <- which(colnames(demospread)=='ObsE1')
  plot(1:nset,demospread[3,EScol1:(EScol1+nset-1)],type='b',pch=LETTERS[1],ylim=c(-1,1),col=mycols[demospread$trueES[3]],xlab='Array index',ylab='Observed Effect size')
for (n in 8:12){
    lines(1:nset,demospread[n,EScol1:(EScol1+nset-1)],type='b',pch=LETTERS[(n-6)],col=mycols[demospread$trueES[n]])
  
  }
abline(h=.3,lty=2)
abline(h=0,lty=2)

#plot LL
LLcol1 <- which(colnames(demospread)=='LL1')

  plot(1:nset,demospread[3,LLcol1:(LLcol1+nset-1)],type='b',pch=LETTERS[1],ylim=c(-12,12),col=mycols[demospread$trueES[3]],xlab='Array index',ylab='Log Likelihood')
for (n in 8:12){
    lines(1:nset,demospread[n,LLcol1:(LLcol1+nset-1)],type='b',pch=LETTERS[(n-6)],col=mycols[demospread$trueES[n]])
  }
abline(h=3.68,lty=2)
abline(h=-3.68,lty=2)



dev.off()
```

![Changes in observed effect size (left panel) and log likelihood (right panel) as evidence accumulates for 6 trials of the learning task, labelled A to G. Blue denotes trials with true effect size of .3, and red denotes null trials. Array indices 1 to 6 correspond to sample sizes of 10, 20, 40, 80, 160 and 320 per group. ](accumulateLL.jpg)  

  
Figure `r fignumber` shows the contrast between observed effect sizes, which show more variation at small array indices, and log likelihood, where trials with true and null effects diverge as array index increases.  Considering first the observed effect sizes, it is clear that if a participant relied on this information to make decisions at small array indices, they would make many errors. Two of the True trials, B and C, have negative effect sizes at array indices 1 and 2, and one of the Null trials, D, has an effect size greater than .3 in the first array. Log likelihood values are shown in the right-hand panel, with dotted lines denoting cutoff of +/- 3.68, which we used as a criterion for the optimal array size.  A log likelihood of 3.68 indicates that the True Hypothesis is 40 times more likely to have generated the observed data than the Null Hypothesis, and a log likelihood of -3.68 indicates that the Null Hypothesis is 40 times more likely than the True Hypothesis. The trials shown in Figure `r fignumber` indicate that a strategy of responding True when log likelihood exceeds 3.68 and Null when it is less than -3.68 is not infallible (e.g., trial B has log likelihood below -3.68 at array index 3, yet comes from a distribution with true effect size of .3). Nevertheless, we can use this approach to compute accuracy levels that would arise from adopting different strategies over all trials, and it is clear that reliance on responding when absolute log likelihood exceeds 3.68 is a close to optimal strategy for achieving success without needing to wait until the final array. 

Of course, a problem for participants is that they cannot estimate log likelihood directly from the information they are presented with. However, they can estimate the amount of overlap between the blue and pink samples, which is predictive of log likelihood.  Our primary analysis assumes that, on the basis of feedback, participants may simply learn that they are more likely to be accurate if they wait to see more data, in which case, they would base their judgment on observed effect size, once the array index gets to 5 or above. By observing the pattern of responses in individuals, we aimed to determine if a participant adopts a more specific strategy, and if so, whether they placed reliance on observed effect size or sample overlap. 



## Simulated data for sample size determination
```{r getarrayprops, echo=F}
allarrayfreqs <- c(11,54,246,570,860,659) #based on pilot 2 data
arrayprops <- round(allarrayfreqs/sum(allarrayfreqs),3)
```
To simulate data, we used our best judgment combined with insights from the pilot testing to explore feasible scenarios. We started by assuming that at the start of training, on each trial, participants will select an array index in proportions similar to that seen in the first block of Pilot 2:  index 1 = `r arrayprops[1]`, index 2 =`r arrayprops[2]`, index 3 = `r arrayprops[3]`, index 4 = `r arrayprops[4]`, index 5 =`r arrayprops[5]`, index 6 = `r arrayprops[6]`.  By the final block, a proportion (p) of participants will be designated as 'learners'. Whether a participant is a learner is simulated using a variable, L, which is latent, in the sense that it affects performance on observed measures, but is not itself detectable. L is either 0 or 1. The response made by the participant is determined by three factors: (a) whether the participant is a learner; (b) whether log likelihood of distributions at that array index favours a true or null effect; (c) random error, which leads to a higher proportion of errors in non-learners than learners.

Where L is equal to one (i.e. the participant is a learner), the participant waits until an array index where the display corresponds to an absolute log likelihood of at least 3.65. For non-learners, the response is made on the basis of whether log likelihood is positive or negative, regardless of the strength of evidence. In addition, a proportion of all responses for both learners and non-learners are coded as errors, with the proportion being higher for non-learners. A simulated sample of participants is created by applying these rules to the stimulus set used in training, with response on each trial being then scored as correct or incorrect.

Prediction 2 focuses on whether there is evidence of learning, with higher earnings on the last block than the first block. The sample size (N subs) was varied from `r subn[1]`, to `r subn[2]`, to `r subn[3]`. The proportion of participants who learn (prop.learned) could take values of 0, .25, .33 or .5.  Table `r tabnumber+1` shows the power for detecting an overall change in mean earnings from block 1 to block 4, using a matched-pairs one-tailed _t_-test, with alpha of .0322 (.1/3, for one-tailed test administered at each of three sample sizes). A one-tailed test is used because the prediction is directional. These figures were derived from 500 iterations of the simulation.  

The case where no participants learn corresponds to a test of the null hypothesis, and so here the power value corresponds to the false positive rate. As expected, this is close to .03. At our smallest sample size (`r subn[1]`), even when only 25% of participants learn, power is above .8, even for the case where learning increases array size by only one index point. 


```{r powertable,include=TRUE,echo=FALSE}
tabnumber <- tabnumber+1
bigname<-"powsummary_1000_L_pgood0.5_0.66_props0_0.25_0.33_0.5_N50_75_100.csv"

bigsummary<- read.csv(bigname)
#bigsummary<-bigsummary[,c(1:4,8,6,9:11)] #final columns 9:11 for array index;
nsubvals<-c(50,75,100) #values that vary for N participants - should match those in bigname
plearnvals<-c(0,.25,.33,.5) #values that vary for proportion of participants who learn- should match those in bigname
quizgoodvals<-c(.5,.66) #all participants start with mean proportion correct on quiz S-items of .33. Those who show learning on beeswarm task increase to either .5 or .66. (Equivalent to increase by 1 or 2 items)
thisrow<-0
alpha <- .0166*2 #sequential stopping means need divide .05 by 3, but then note we have one-tailed test
powersummary<-data.frame(matrix(NA,ncol=11,nrow=length(nsubvals)*length(plearnvals)*length(quizgoodvals)))
powersummary<-data.frame(matrix(NA,ncol=14,nrow=length(nsubvals)*length(plearnvals)*length(quizgoodvals)))
colnames(powersummary)<-colnames(bigsummary)


row<-0

    for (p in plearnvals){
      for (q in quizgoodvals){
      for (n in nsubvals){

      row<-row+1
      powersummary$nsub[row]<-n
      powersummary$plearn[row]<-p
     powersummary$quiz.pbad[row]<-.33
      powersummary$quiz.pgood[row]<-q
      temp<-filter(bigsummary,nsub==n,plearn==p,quiz.pgood==q)
      powersummary$run<-nrow(temp)
      wc<-which(colnames(temp)=='p.t.array')
      for(mycol in c(4:6,9:11)){
        w<-length(which(temp[,mycol]<alpha))
        powersummary[row,mycol]<-w/nrow(temp)
      }
      
    }
  }

}

#column p.t.earn is power for testing change in mean earnings from block1 to block4 (1-sided t-test)
#column ptquiz is power for testing change in percent correct in S-items on quiz from pre to posttest
# column lmpre.p is power of regression coefficient for quiz pretest score
# column lmearndiff.p is power of regression coefficient for earndifference (block 4 minus block 1) as predictor quiz posttest.

#Make table for learning effect in task
#For learning effect on task, we just average power across levels of quiz success
learntab <- aggregate(powersummary$p.t.earn,by=list(powersummary$nsub,powersummary$plearn),FUN=mean)
table1<-data.frame(matrix(NA,nrow=5,ncol=4))
colnames(table1) <- c('Parameters','N=50','N=75','N=100')
table1[2,1] <- '_plearn = 0'
table1[3,1] <- '_plearn = .25'
table1[4,1] <- '_plearn = .33'
table1[5,1] <- '_plearn = .50'


thisrow=0
thatrow=-2

  for (pl in 1:4){
    thisrow<-thisrow+1
    thatrow<-thatrow+3
    if(pl==1){thisrow <- thisrow+1}
    table1[thisrow,2:4]<-round(learntab[thatrow:(thatrow+2),3],2)
  }



write.csv(table1,'table1.csv',row.names=F)
ftab <- flextable(table1)
ftab<-autofit(ftab)
ftab <- set_caption(ftab, paste0("Table ",tabnumber,": Power for comparison of earnings for block 4 vs block 1, in relation to proportion of learners (plearn)"))
#ftab<-autofit(ftab)
ftab



```


To address Prediction 5, quiz data were simulated using the binomial distribution, assuming the mean number of S-items correct on the pretest is 2/6 (.33), improving by either one item (3/6 = .5) or two items (4/6 = .66) for the subgroup of participants who showed learning on training (where the latent variable, L, = 1).  

Results are shown in Table `r tabnumber+1`, again looking at scenarios with 50, 75 or 100 participants, and where the proportion showing learning ranges from 0, .25, .33 and .5. One other parameter is considered for the quiz data, namely the amount of improvement in quiz scores seen in learners - a mean gain of either 1 quiz item, or 2 quiz items.  

```{r powerquiz,include=TRUE,echo=FALSE}
#summary for quiz improvement
 tabnumber <- tabnumber+1
quizsum<-powersummary[,c(2,3,8:11)]
colnames(quizsum) <- c('N subs','prop.learned','learned.quiz2','power.quizdiff','power.quizpre','power.arraydiff')
quizsum[,4:6]<-round(quizsum[,4:6],2)
write.csv(quizsum,'quizsum.csv',row.names=F)


#Make table2 for learning effect on quiz
#For learning effect on task, we just average power across levels of quiz success
qlearntab <- aggregate(powersummary$ptquiz,by=list(powersummary$nsub,powersummary$plearn,powersummary$quiz.pgood),FUN=mean)
colnames(qlearntab)<-c('nsub','plearn','quizlearn','power')
table2<-data.frame(matrix(NA,nrow=10,ncol=4))
colnames(table2) <- c('Parameters','N=50','N=75','N=100')
table2[1,]<-c('Quiz gain = 1','.','.',',')
table2[6,]<-c('Quiz gain = 2','.','.',',')
table2[2,1] <- '_plearn = 0'
table2[3,1] <- '_plearn = .25'
table2[4,1] <- '_plearn = .33'
table2[5,1] <- '_plearn = .50'
table2[7,1] <- '_plearn = 0'
table2[8,1] <- '_plearn = .25'
table2[9,1] <- '_plearn = .33'
table2[10,1] <- '_plearn = .50'

thisrow=0
thatrow=-2
for (A in 1:2){
  for (pl in 1:4){
    thisrow<-thisrow+1
    thatrow<-thatrow+3
    if(pl==1){thisrow <- thisrow+1}
    table2[thisrow,2:4]<-round(qlearntab[thatrow:(thatrow+2),4],2)
  }
}


write.csv(table2,'table2.csv',row.names=F)
ftab2 <- flextable(table2)

ftab2 <- set_caption(ftab2, paste0("Table ",tabnumber,": Power for comparison of quiz pre and post test, in relation to number of participants (N), proportion of learners (plearn) and increase in quiz score in learners (Quiz gain)"))
ftab2<-autofit(ftab2)
ftab2

```
Power to detect any improvement in the overall sample is generally poor if one just compares quiz pre-test and post-test items, except where the gain in quiz score is at least 2 points and at least 33% of participants learn. This follows because the number of quiz items is small, only a proportion of participants show any learning, and the relationship between learning and quiz post-test performance is imperfect. 


```{r powerLM,include=T,echo=F}
#Now do table 3 for power for regression
#Make table3 for prediction of quiz posttest from earngain
#For learning effect on task, we just average power across levels of quiz success
tabnumber<-tabnumber+1
LMtab <- aggregate(powersummary$lmearndiff.p,by=list(powersummary$nsub,powersummary$plearn,
                                                     powersummary$quiz.pgood),FUN=mean)
table3<-data.frame(matrix(NA,nrow=10,ncol=4))
colnames(table3) <- c('Parameters','N=50','N=75','N=100')

table3[2,1] <- '_plearn = 0'
table3[3,1] <- '_plearn = .25'
table3[4,1] <- '_plearn = .33'
table3[5,1] <- '_plearn = .50'
table3[7,1] <- '_plearn = 0'
table3[8,1] <- '_plearn = .25'
table3[9,1] <- '_plearn = .33'
table3[10,1] <- '_plearn = .50'

table3[1,]<-c('Quiz gain = 1','.','.',',')

table3[6,]<-c('Quiz gain = 2','.','.',',')


thisrow=0
thatrow=-2

  for (q in 1:2){
  for (pl in 1:4){
    thisrow<-thisrow+1
    thatrow<-thatrow+3
    if(pl==1){thisrow <- thisrow+1}
    table3[thisrow,2:4]<-round(LMtab[thatrow:(thatrow+2),4],2)
  }
  
}


write.csv(table3,'table3.csv',row.names=F)
ftab3 <- flextable(table3)

ftab3 <- set_caption(ftab3, paste0("Table ",tabnumber,": Power for  regression coefficient predicting post-training score on S-items from observed increase in earnings on the learning task. Power varies with number of participants (N), proportion of learners (plearn), and quiz gain at post-training for learners (quiz gain)"))
ftab3<-autofit(ftab3)
ftab3
```





   
An alternative approach uses linear regression, where the focus is on predicting the quiz post-training score (S-items) from the pre-training score and the index of learning, i.e., the change in earnings from block 1 to block 4. This analysis, then, can detect whether the amount of learning in the training session is predictive of improvement on the quiz.  

As shown in Table `r tabnumber`, the combination of parameters determines power, with high power seen when at least 33% of participants show learning, and when their quiz scores increase by 2 points. Power to detect a one-point gain in quiz scores for learners is modest at best.  Once again, consideration of the case where no participants show learning confirms that the type 1 error rate is well-controlled, so we can be confident that where positive findings are obtained, they are true effects.  


## Interpretation of pattern of results
We can refer to the pattern of results on the beeswarm task and the quiz as indicating null result, inconclusive, or prediction confirmed. In our pre-registration, we specified interpretations as follows:

The first possible pattern is where neither the beeswarm task nor the quiz shows any improvement. If this pattern were observed, we would conclude that the beeswarm task was not effective in debiasing participants away from sample size neglect to a meaningful extent. We would consider using information from the experiment to devise a modified version of the task. There are various ways one could do this: by changing the visual display, or by changing the reward schedule, or providing more explicit instruction. It would, of course, also be possible to extend the training, although this might require having more than one session. In effect, this study could act as a baseline against which to evaluate other approaches to training.  

If results on one or both measures are inconclusive, this could reflect insufficient training/quiz items, and or sample heterogeneity. If the sample is heterogeneous, with a subset responding to the training, we would expect to see a relationship between gains on training and gains on the S-items on the quiz. In addition, we would expect to see an association between learning and self-reported awareness of the importance of sample size. We would also aim in future work to identify characteristics of those who improved with training.  

We may find that there are significant improvements with training, but null or inconclusive results on the quiz, indicating a lack of generalisation. If so, we would explore individual S-items in the quiz, to see whether there is any indication of improvement on a subset of items, and whether they have particular characteristics. We may, for instance, see evidence of selective improvement on items assessing understanding of the relationship between sample size and statistical power, which could be tested in a replication study.

The converse pattern might be obtained, with improvement on the S-items of the quiz but null or inconclusive findings on the beeswarm task. This might justify a further study which uses two training sessions, making it possible to look for gains due to training that occur after a delay.  

Finally, if we are able to show an improvement with this short training session, with generalisation to S-items on the quiz, then this would lead us to conclude that the game might be a useful adjunct to statistical training courses for scientists. This could motivate further studies with modified forms of presentation, to identify the optimum conditions for training, and with different effect sizes. It would also be of interest to evaluate the training for longer-term impact on subsequent experimental practices of the trainees.  

Tests of predictions 3 and 4 have potential to provide converging evidence on whether sensitivity to array size is a factor determining learning. If neither prediction is confirmed, but the training is effective, this would suggest we should explore alternative heuristics that might be used by participants to support successful performance. 

## Pilot data  
`r tabnumber <- tabnumber+1`
Three pilot studies were conducted for the Stage 1 Registered Report submission.  Summary results for all three pilots are available on OSF  (https://osf.io/s39qd). Overall performance on Pilot 1 was poor, with mean of only 65% correct, confirming the difficulty that participants had in judging effect sizes from distributions. A score of 85% or above was observed in only 10% of all blocks x participants. Participants in Pilot 1 found the task reasonably straightforward but scrutiny of individual cases for evidence of response strategies suggested that only a few participants had started to wait longer to respond as the session proceeded, and others went in the opposite direction, selecting smaller arrays in later blocks.  In general, high performers were those who waited for large arrays from the outset, rather than those who changed strategy in the course of training. 

Pilot 1 participants also completed the estimation quizzes that preceded and followed the training session. As noted above, modifications were made to wording of some quiz items for the current study on the basis of pilot testing to avoid confusing or over-complicated language. 

On the basis of Pilot 1 data, we considered ways of making the training more effective. In Pilot 1, the beeswarm displays for the two groups  had omitted the horizontal line corresponding to the mean, and each beeswarm faded out as the next one appeared, which meant it was less obvious that the distributions were cumulative. In addition, although the participants were shown a demonstration of the task, they did not have any familiarisation trials, and it was felt that they might perform better if given the chance to try a few trials to become familiar with the pace of the task. For Pilot 2, the displays included a horizontal bar corresponding to the mean, and prior displays remained visible rather than fading out, to emphasise that, within a trial, each set of points included the prior, smaller set. In addition, participants were given 4 practice trials before the training blocks, and the reward schedule was modified to provide stronger motivation to succeed, as described above. 

A second pilot study, Pilot 2, was run with 30 participants on the new version of the beeswarm task (without the quiz).  This confirmed that not only was performance improved relative to Pilot 1, but also there was evidence of learning in terms of the percentage correct and earnings measures (which were highly intercorrelated). The mean array index was numerically larger for the last vs the first block, but the difference was small and not statistically significant. However, for the final block, there was a significant correlation between mean array index and percent correct, and several participants reported that they learned to wait for larger arrays in the course of training, suggesting the task was effective in achieving its aim. 

At the suggestion of a reviewer, we conducted Pilot 3 with 20 participants, using an unpaced version of the task, in which on each trial, the participant was presented with a menu and required to select a sample size to view. A plot showing all sample sizes, similar to Figure `r keepfignumber+1`, was shown alongside feedback on accuracy for each trial. A cost of one point was incurred with each increase in the selected array index, to prevent participants from always selecting the largest array size. No learning was observed in Pilot 3, which gave results similar to Pilot 1, and therefore this approach was abandoned.

# Results
<!--- NEW FOR STAGE 2--->
<!---Data wrangling and analysis based on pilot2_processing.Rmd--->

In a departure from our pre-registered plan, we present here results from two samples of participants. Sample 1 (N = 50) was recruited as planned, but, owing to a coding error, the estimation quiz items were administered in a non-random order, with the six P-items followed by the six S-items. As we could not be sure of the effect of this procedural change on performance, we recruited Sample 2 (N = 50), who were administered the estimation quiz with items randomised as intended. Preliminary analyses (see below) indicated no meaningful differences on quiz scores between the two samples. 

On the basis of the Bayesian stopping rule that we pre-registered, the 50 participants in Sample 2 gave only anecdotal evidence of learning on the training game. As there was no differences between the two samples in administration of the training game, we therefore combined Samples 1 and 2 to give a total sample of 100 participants for our main analysis. 


## Demographics

<!--__Table for individual subjects here just for our interest - has subjects rank-ordered by stats knowledge. For the paper we can report summary stats on these columns.__ -->

Summary data for participants are shown in Table `r (tabnumber+1)`. There was a wide spread in terms of self-rated statistical competence (see Appendix 1 for questions), and educational level.  



```{r readdata,echo=F,warning=F}


all.dat <- read.csv("https://osf.io/k6afy/download",stringsAsFactors = F) #all gorilla files bolted together

all.dat$sample<-2 #sample with correctly ordered quiz

#Now read the original subjects: these were wrongly administered unrandomised quiz
all.datx <- read.csv("https://osf.io/uz3jb/download",stringsAsFactors = F) #all gorilla files bolted together

all.datx$sample <- 1 
#We'll just prune off the last subject as we intended to get 50 and it gives nice round number
datxsubs <- unique(all.datx$Participant.Private.ID)
all.datx<-all.datx[all.datx$Participant.Private.ID %in% datxsubs[1:50],]

#We now combine both samples
all.dat <- rbind(all.datx,all.dat) #older sample (1) first

#First just fix a typo in Question key
w<-which(all.dat$Question.Key=='P1B -quantised') 
all.dat$Question.Key[w]<-'P1B-quantised'
```




```{r makesubjects, echo=F,warning=F}
#create easy to manage subject ID
w<-which(is.na(all.dat$Participant.Private.ID))
sublist <- unique(all.dat$Participant.Private.ID[-w])
all.dat$subject<-NA #create new subject number
for (i in 1:length(sublist)){
  all.dat$subject[all.dat$Participant.Private.ID==sublist[i]]<-i
}
nsub<-length(sublist)
nsub1 <-50 #length of initial set of subjects
```


```{r collectdemog,echo=F,warning=F}
wantcols <- c('subject','Question.Key','Response')
wantvals <- c('Age','Sex','Ed_level-quantised','field','Stats_ability_self_rating','interpeting_t-quantised', 'power_knowledge-quantised','better.over.time')

#For t-test and power qus, 1 = confident, 2 = fairly confident, 3 = not confident, 4 = no idea
#For sex F = 1 and M = 2
#Age bands: 1 18-24, 2 25-30, 3 31-40, 4 41+
w <- which(all.dat$Question.Key %in% wantvals)

demo <- all.dat[w,wantcols]

demotab <-spread(demo,Question.Key, Response)
demotab <- demotab[,c('subject',wantvals)]
colnames(demotab)<-c('ID','Age band','Sex','Ed.level','Field','Stats_self_rating*','Understand_t**','Understand_power***','better_over_time')


demotab$Sex <- as.factor(demotab$Sex)
demotab$Stats_self_rating <- as.numeric(demotab$Stats_self_rating)
demotab$Understand_t <- as.numeric(demotab$Understand_t)
demotab$Understand_power <- as.numeric(demotab$Understand_power)

#########################################################
## one subject coded 6 on Ed.level - change to 4
w <- which(demotab$Ed.level>4)
demotab$Ed.level[w]<-4
#########################################################


demotab$Ed.level <- as.factor(demotab$Ed.level)
levels(demotab$Ed.level)<-c('Pre-degree','1st degree','Masters','Doctorate')

demotab2 <- demotab[order(demotab$Stats_self_rating),] #rank ordered by self-rated stats competence - just so we can get an impression of which subjects are confident or not. 

#NB not all are studying life/biological sciences - some maths or physics or chemistry


if(authorversion==1){
ft <- flextable(demotab2[2:ncol(demotab2)])
#ft <- autofit(ft)
ftab <- set_caption(ft, "Supplementary table (author version): Demographics for individual participants, rank ordered by self-rated statistical skills")

ftab
}

#Summary table for the paper
#Use table1 package - see https://cran.r-project.org/web/packages/table1/vignettes/table1-examples.html
tabnumber <- tabnumber+1
label(demotab$Sex)       <- "Sex"
label(demotab$`Age band`)       <- "Age band"
label(demotab$Ed.level)     <- "Education level completed"
label(demotab$Stats_self_rating) <- "Statistical competence"
label(demotab$Understand_t)<-"Confidence understanding t-test"
label(demotab$Understand_power)<-"Confidence understanding power"
units(demotab$Stats_self_rating) <- "self report: 0-100"
units(demotab$Understand_t) <- "self report: 1=high, 4=low"
units(demotab$Understand_power) <- "self report: 1=high, 4=low"

demotable1 <- table1(~ Sex + `Age band` + Ed.level + Stats_self_rating+ Understand_t+Understand_power, data=demotab[demotab$ID<(nsub1+1),])
demotable2 <- table1(~ Sex + `Age band` + Ed.level + Stats_self_rating+ Understand_t+Understand_power, data=demotab[demotab$ID>nsub1,])
df1 <-as.data.frame(demotable1)
df2 <-as.data.frame(demotable2)
dfboth <- cbind(df1,df2[,2])
colnames(dfboth)[2:3]<-c('Sample 1','Sample 2')
ft1 <- flextable(dfboth)
ft1 <- autofit(ft1)
ft1 <- set_caption(ft1, paste0("Table ",tabnumber,": Demographics and self-rated statistical skills of participants"))
ft1



```
Notes
* “Compared to others at your level of education in your participant, how good do you think your understanding of basic statistics is?”; rated on scale from 0 (very poor) to 100 (excellent).  
** “How confident are you in interpreting the results of a t-test?” rated  from 1 = very confident to 4 = Very little idea of what a t-test is.  
*** “How familiar are you with the idea of statistical power?” rated from 1 = very familiar to 4 = very little idea about what statistical power is”.  


```{r analyselearning,echo=F,warning=F}
#remove all but learning trials (defined by display 'beeswarms')
short.dat<-all.dat[all.dat$display=='beeswarms',]

#retain only Attempt
w<-which(short.dat$Attempt==1)
short.dat<-short.dat[w,]


#check RT distribution
if(authorversion==1){
hist(short.dat$Reaction.Time, breaks=50)
abline(v=c(2000,4000,6000,8000,10000),col='red')
}

short.dat$array= as.numeric(cut(short.dat$Reaction.Time, breaks= c(0,2000,4000,6000,8000,10000,Inf), labels = c("1","2","3","4","5","6")))

#Check scoring worked OK.

if(authorversion==1){
table(short.dat$Correct,short.dat$ANSWER,short.dat$Response)
}

#Compute earnings for each trial
short.dat$earn<-short.dat$EarningWrong #default is error
for (r in 1:nrow(short.dat)){
  if(short.dat$Correct[r]==1){
  short.dat$earn[r]=short.dat$EarningCorrect[r]
  if(short.dat$array[r]==short.dat$bonus[r]){  #get bonus 2 pts if correct selection at optimal index
    short.dat$earn[r]<-short.dat$earn[r]+2
  }
 }
}  

#convert event index into block.

#Event index is nonnumeric
eventlist <- unique(short.dat$Event.Index) #confirm there are 80 values

short.dat$block<-1
w<- which(short.dat$Event.Index %in% eventlist[21:40])
short.dat$block[w]<-2
w<- which(short.dat$Event.Index %in% eventlist[41:60])
short.dat$block[w]<-3
w<- which(short.dat$Event.Index %in% eventlist[61:80])
short.dat$block[w]<-4

if(authorversion==1){
table(short.dat$subject,short.dat$block)
#Most have 20 trials per block as expected; NB some have only 19 trials in a block - maybe timed out on others?
}
short.dat$adiff<-short.dat$array-short.dat$bonus

#add LL and ObsES for selected array - first compute offset for col number
wE <- which(colnames(short.dat)=="ObsE1")-1
wL <- which(colnames(short.dat)=="LL1")-1
short.dat$LL<-NA
short.dat$ObsE <- NA
for (i in 1:nrow(short.dat)){
  short.dat$ObsE[i] <-short.dat[i,(wE+short.dat$array[i])]
  short.dat$LL[i] <-short.dat[i,(wL+short.dat$array[i])]
}
w<-which(short.dat$LL==Inf)
short.dat$LL[w] <-100



```

```{r makeaggregate,echo=FALSE}
#for ObsE and LL, for means the interest is in absolute values at point of response
short.dat$absObsE <-abs(short.dat$ObsE)
short.dat$absLL <-abs(short.dat$LL)

mycols<-c('Correct','bonus','array','earn','adiff','absObsE','absLL','sample')
colnums<-which(colnames(short.dat)%in% mycols)
colnums<-colnums[c(3,1,2,5,4,7,6,8)] #reorder so easier to read
mytab <- aggregate(short.dat[,colnums],by=list(short.dat$subject,short.dat$block),FUN=mean)
colnames(mytab)[1:2]<-c('subject','block')

#means by block
mymeans <- aggregate(mytab[,4:8], by = list(mytab$block,mytab$sample),FUN=mean)
colnames(mymeans)[1:2]<-c('block','sample')

if(authorversion==1){
  mytab2<-mytab[mytab$subject>nsub1,]
cor(mytab2[,3:6],use='complete.obs')
plot(mytab2$array,mytab2$earn,col=mytab2$block,pch=as.character(mytab2$block))
plot(mytab2$Correct,mytab2$earn,col=mytab2$block,pch=as.character(mytab2$block))

plot(mytab2$earn[mytab2$block==1],mytab2$earn[mytab2$block==4])
abline(a=0,b=1)
plot(mytab2$adiff[mytab2$block==1],mytab2$adiff[mytab2$block==4])
abline(a=0,b=1)
}
```

```{r slopes,echo=FALSE,warning=F}
nsub<-length(unique(short.dat$subject))
#Various indices were explored when preparing stage 1 RR, including suggested indices by reviewers
mytab$slope<-NA #create new column - will hold slope in row corresponding to block 1
mytab$pdiff <- NA #alternative for difference in p correct from 4 to 1
mytab$adiff <- NA #change in array index
mytab$ediff <- NA #change in earnings
mytab$eslope<-NA #slope for earnings
mytab$absLLdiff<-NA
for (i in 1:nsub){
  thisdat<-mytab[mytab$subject==i,]
  myreg<-lm(thisdat$Correct~thisdat$block)
    myrege<-lm(thisdat$earn~thisdat$block)
  myrow<-which(mytab$subject==i)[1] #first row with this subject
  lastrow<-which(mytab$subject==i)[4] 
  mytab$slope[myrow]<- myreg$coefficients[2]
  mytab$eslope[myrow]<- myrege$coefficients[2]
  mytab$pdiff[myrow]<-mytab$Correct[lastrow]-mytab$Correct[myrow]
  mytab$adiff[myrow]<-mytab$array[lastrow]-mytab$array[myrow]
  mytab$ediff[myrow]<-mytab$earn[lastrow]-mytab$earn[myrow]
  mytab$absLLdiff[myrow]<-mytab$absLL[lastrow]-mytab$absLL[myrow]
}

if(authorversion==1){
cor(mytab$pdiff,mytab$slope,use='complete.obs')
plot(mytab$pdiff,mytab$slope,main='% correct')
abline(h=0)
abline(v=0)

plot(mytab$ediff,mytab$slope,main='Earning change vs slope for correct')
abline(h=0)
abline(v=0)

plot(mytab$adiff,mytab$slope,main='Array change vs slope for correct')
abline(h=0)
abline(v=0)
}

if(authorversion==1){
#If slopes use more data, are they more accurate reflection of learning? If so, should see more powerful effect from slope than from difference score


t.test(mytab$pdiff,alternative = 'greater') #pdiff has stronger effect
t.test(mytab$slope,alternative = 'greater') #slope based on proportion correct
t.test(mytab$ediff,alternative='greater') #strongest effect for ediff
t.test(mytab$eslope,alternative='greater')  #slope based on earnings
t.test(mytab$absLLdiff,alternative='greater')  #slope based on abs LL

#We preregistered analysis based on ediff, as this gave strongest effect in the pilot data. 

}
```



## Confirmatory analyses  
We tested our five pre-specified predictions based on the pre-registered analysis plan.  

_Prediction 1_:  Responses on quiz items will reflect overconfidence in small samples.   

 
```{r readquiz, echo=F,include=T,warning=F}
#Now analyse quiz data and create table with both task and quiz
qdata <- all.dat[!is.na(all.dat$Question.Key),] #this subset has quiz items and self-ratings etc


#Add a factor denoting pre or post training quiz
qdata$whichq <-NA
qdata$row <- 1:nrow(qdata)
 #col with randomiser.g2sn is "List A" if list A given at post test, and "List B" if list B given at posttest
w<-which(colnames(qdata)=='randomiser.g2sn')
colnames(qdata)[w]<-'PostList'

f <- filter(qdata,PostList=="List A",Task.Name=="Probability Questions 2021- List A")
qdata$whichq[f$row]<-2
f <- filter(qdata,PostList=="List A",Task.Name=="Probability Questions 2021- List B")
qdata$whichq[f$row]<-1
f <- filter(qdata,PostList=="List B",Task.Name=="Probability Questions 2021- List A")
qdata$whichq[f$row]<-1
f <- filter(qdata,PostList=="List B",Task.Name=="Probability Questions 2021- List B")
qdata$whichq[f$row]<-2


qdata$prepost <- as.factor(qdata$whichq)
levels(qdata$prepost) <- c("Pre", "Post")




```



```{r scoreQs,echo=F,warning=F}
correctA <- c(1,4,4,1,3,2,
              3,2,4,1,3,1) #list A, P then S
correctB <- c(2,1,1,2,3,1,
              1,3,3,1,3,1) #list B, P then S
biasA <- c(NA,NA,NA,NA,NA,NA,
           4,4,-4,2,4,4) #bias for law small N list A; where zero any value below correct
biasB <- c(NA,NA,NA,NA,NA,NA,
           2,2,-3,4,4,3) #bias for law small N list B; where zero any value below correct
itemsA <- paste0(c('P1A','P2A','P3A','P4A','P5A','P6A','S1A','S2A','S3A','S4A','S5A','S6A'),'-quantised')
itemsB <- paste0(c('P1B','P2B','P3B','P4B','P5B','P6B','S1B','S2B','S3B','S4B','S5B','S6B'),'-quantised')
itemtype <- c(rep('P',6),rep('S',6))

#initialise new columns
qdata$itemtype <-NA
qdata$correctresp <- NA
qdata$biasresp <-NA #responses expected if showing expected bias

for (i in 1:nrow(qdata)){
  for (l in 1:2){
  itemlist <- itemsA
  corrects <- correctA
  bias <- biasA
  if(l==2){
    itemlist <-itemsB
    corrects <- correctB
    bias <- biasB}
  for (n in 1:12){
  w<-which(qdata$Question.Key==itemlist[n])
  qdata$itemtype[w]<-itemtype[n]
  qdata$correctresp[w] <- corrects[n]
  qdata$biasresp[w] <- bias[n]
  }
  }
  
}
qdata$Response <- as.numeric(qdata$Response)
qdata$subtract <- abs(qdata$Response-qdata$correctresp)
qdata$correctp.ans <- ifelse(qdata$subtract==0,1,0)
w<-which(is.na(qdata$subtract))
qdata$correctp.ans[w]<-NA
#check all OK - can inspect temp and see if it looks OK
mycols<-c("Question.Key","Response","prepost","whichq","itemtype","correctresp","biasresp","subtract","correctp.ans")
temp<-qdata[,mycols]
```


```{r summariseq,echo=FALSE}

myagq <- aggregate(qdata$correctp.ans,by=list(qdata$subject,qdata$prepost,qdata$itemtype,qdata$PostList
                                              ),FUN=mean)
colnames(myagq)<- c('subject','prepost','itemtype','PostList','p.correct')
myagq$sample <- 1
myagq$sample[myagq$subject>nsub1]<-2
myagq2 <- aggregate(myagq$p.correct, by=list(myagq$prepost,myagq$itemtype,myagq$sample),FUN=mean)
colnames(myagq2)<-c("Quiz","Item type","Sample","Mean proportion correct")
if(authorversion==1){
myagq2
}

#Make a table showing pre- and post scores subdivided by the specific List given
myagq3 <- aggregate(qdata$correctp.ans,by=list(qdata$prepost,qdata$PostList,qdata$itemtype,qdata$sample),FUN=mean)
myagq3a <- aggregate(qdata$correctp.ans,by=list(qdata$prepost,qdata$PostList,qdata$itemtype,qdata$sample),FUN=sd)
myagq3<-cbind(myagq3,myagq3a[,5])
colnames(myagq3)<-c('PrePost','List','Item.type','Sample','Mean','SD')
myagq3[,5:6]<-round(myagq3[,5:6],3)

#Count N below 50% correct on P-items and N 
temp1S<- filter(myagq,sample==1,itemtype=='S',prepost=='Pre')
temp1P<- filter(myagq,sample==1,itemtype=='P',prepost=='Pre')
tS1 <- t.test(temp1S$p.correct*6,mu=5)
pP1 <- length(which(temp1P$p.correct<.5))/nrow(temp1P)
temp2S<- filter(myagq,sample==2,itemtype=='S',prepost=='Pre')
temp2P<- filter(myagq,sample==2,itemtype=='P',prepost=='Pre')
tS2 <- t.test(temp2S$p.correct*6,mu=5)
pP2 <- length(which(temp2P$p.correct<.5))/nrow(temp2P)

#For t-test have now multiplied proportions by 6, as reviewer preferred raw scores here
```
We first checked the pre-test quiz data against two prespecified criteria: (a) that mean number of S-items correct is less than 5/6 (83%), and (b) that fewer than 50% of participants score less than 3/6 correct on P-items (see prediction at end of Data Analysis section). Both predictions were confirmed. In Sample 1, the pre-training mean on S-items was `r round(tS1$estimate,2)`, t-test of difference of mean from 5 = `r round(tS1$statistic,2)`, `r p.format(tS1)`; In Sample 2, the pre-training mean on S-items was `r round(tS2$estimate,2)`, t-test of difference of mean from 5 = `r round(tS2$statistic,2)`, `r p.format(tS2)`.   The percentage of participants scoring less than 3/6 on the pre-training P-items was `r round(100*pP1,1)`% for Sample 1 and `r round(100*pP2,1)`% for Sample 2.  We also considered which was the most common response for each item on initial testing: for all 12 P-items, the correct response was the most common, whereas this was the case for only 5/12 S-items. We found that `r N2` participants scored 5 or more on S-items correct prior to training, and they were dropped from the learning analyses, consistent with our pre-registration. 


```{r inspectquizfactors,echo=F}
#NB we exclude fron myagq 2 subjects who perform well on S-items 

preS <- filter(myagq,prepost=='Pre',itemtype=='S')
#Identify cases with 5-6 correct on PreS
w<-which(preS$p.correct>.7)
dropsub <- myagq$subject[w] #subject numbers to drop

myagq$subjectf <- as.factor(myagq$subject)
myagq$samplef <- as.factor(myagq$sample)
levels(myagq$samplef)<-c("Sample 1","Sample 2")
myagq$itemf <- as.factor(myagq$itemtype)
myagq100<-myagq
myagq100$exclude<- 0
myagq100$exclude[myagq100$subject==dropsub]<-1
myagq<- myagq100[myagq100$exclude==0,]  #myagq is now the version with 98 subjects
bxp <- ggboxplot(
  myagq, x = "prepost", y = "p.correct",
  color = "itemf",
  facet.by =  "samplef"
  )
bxp2 <- ggboxplot(
  myagq, x = "PostList", y = "p.correct",
  color = "itemf", palette = "jco"
  )


mymod <- lmer(p.correct ~ itemf+prepost+samplef+(1|subjectf),data=myagq)
#summary(mymod)
mymodnullsample <-lmer(p.correct ~ itemf+prepost+(1|subjectf),data=myagq)
BF_BIC_sample = exp((BIC(mymodnullsample) - BIC(mymod))/2)  # From BICs to Bayes factor

s<-round(summary(mymod)$coefficients,3)
s<- as.data.frame(s)
Source<-c('(Intercept)','Item type (P/S)','Pre/Post','Sample (1/2)')
s<- cbind(Source,s)
ftquiz <- flextable(s)
ftquiz <- autofit(ftquiz)
tabnumber <- tabnumber+1
ftquiz <- set_caption(ftquiz, paste0("Table ",tabnumber,": Estimates of effects of item type( S- vs P-item), pre/post administration and sample on quiz accuracy"))
ftquiz
```


Next we checked if the two samples differed in their responses to the quiz items, by running a linear mixed model predicting proportion correct from item type, pre/post testing, and sample, with the formula:  
_lmer(p.correct ~ itemtype + prepost + sample + (1|subject)_  
where itemtype was whether P- or S-item, prepost specified whether the quiz was before or after training, sample was Sample 1 or 2, and subject was a random effect.  This model was compared with a model where the sample term was omitted, to give a Bayes Factor of `r round(BF_BIC_sample,3)`, giving strong evidence for the null hypothesis of no difference between samples.  As shown in Table `r tabnumber`, the analysis also indicated a substantial effect of item type, reflecting the superior performance on P-items, but no effect of whether the quiz was administered pre- or post-training.  We show data separately for the two samples in some subsequent analyses, but they were treated together for the main analysis.  


```{r qdatainspect, include=T,echo=F}

### Create a table showing the distribution of responses to each question. 
# Pre-training kept separate
# This is saved as quiz_items. 
qdata_prepost <- qdata[qdata$Question.Key %in% c(itemsA,itemsB),]
myq <- c('Post','Pre') #we do Post then Pre, as our main focus is on Pre for later analysis
for (q in 1:2) {
qdatax <- qdata_prepost[qdata_prepost$prepost==myq[q],]
qdatax$item <- as.factor(substr(qdatax$Question.Key,1,3))
qt <- table(qdatax$item,qdatax$Response)
qdf <- as.data.frame(unclass(qt))

qdf<-cbind(row.names(qdf),qdf)
qdf<-qdf[c(seq(1,24,2),seq(2,24,2)),]
qdf$Correct.Ans <- c(correctA,correctB)
qdf$Bias.Ans <- c(biasA,biasB)

#qdf collapses across all subjects; rows by items, with A or B suffix  to denote which parallel form. 
qdf$perc.c<-NA
for (i in 1:nrow(qdf)){
  qdf$perc.c[i] <- qdf[i,(1+qdf$Correct.Ans[i])]/(nsub/2)
}
colnames(qdf)[1:5]<-c('Item','R1','R2','R3','R4')
qdf<-qdf[order(qdf$Item),]

#addbit <- read.csv('quiz_items_extended.csv') #can use to add item wording
#qdf <- cbind(qdf[,1],addbit[,9:10],qdf[,2:8])
colnames(qdf)[1]<-'Item'
colnames(qdf)[ncol(qdf)]<-'Percent.correct'
qdf$Percent.correct <- round(100*qdf$Percent.correct,1)

w<-which(qdf$Bias.Ans=='-4')
qdf$Bias.Ans[w]<-'< 4'
w<-which(qdf$Bias.Ans=='-3')
qdf$Bias.Ans[w]<-'< 3'

myqname <- paste0('quiz_items_',myq[q],'.csv')
write.csv(qdf,myqname,row.names=F)
}

ftab <- flextable(qdf) #this is done just for the last run, i.e. Pre items. But Post is saved
ftabquizitems <- set_caption(ftab, paste0("Table ",tabnumber,": Distribution of responses to quiz items"))
ftabquizitems<-fit_to_width(ftabquizitems,max_width=15)
#This table will be printed later with exploratory analyses

#convert to wide
#make dummy combo of 2 variables - needed for later analysis
myagq$dummy <- as.factor(paste0(myagq$prepost,myagq$itemtype))
myagq2<-myagq[,-c(2,3)] #drop the cols combined into dummy
agqwide <- spread(myagq2,dummy,p.correct)

t_on_preS <- t.test(agqwide$PreS, mu = .83, alternative = "less")

```

```{r qerrors,echo=F,warning=F}
wantcols<-c('Item','R1','R2','R3','R4','Correct.Ans','Bias.Ans')
srows<- 13:24 #select rows for S items only
qerr <- qdf[srows,wantcols] 

colnames(qerr)[2:5]<-c('Correct','Bias*','Other1*','Other2*')#rename cols
colcorr<-qdf[srows,'Correct.Ans'] #column with correct ans for each S item
colbias <-qdf[srows,'Bias.Ans']#column with bias ans for each S item
colbias<-as.numeric(colbias[-c(5:6)]) #remove those with 'less than'
colcorr<-colcorr[-c(5:6)]
qerr$chisq <- NA #initialise cols to hold result of chi sq test
qerr$p <- NA
w<-which(qerr$Item %in% c('S3A','S3B'))
qerr<-qerr[-w,]
for (i in 1:nrow(qerr)){ #sort cols so that they are in order of qerr colnames
  thisrow<-qerr[i,2:5]
  myindex <-1:4
  indexlist <- c(colcorr[i],colbias[i]) #indicates which items are correct and bias
  restlist <-myindex[-c(indexlist)] #indicates which items are neither correct nor bias
  allindex <- 1+c(indexlist,restlist) #column numbers in correct order for analysis
  qerr[i,2:5]<-qerr[i,allindex]
  mychi<-chisq.test(as.numeric(qerr[i,3:5])) #omit correct col: interest is in whether errors are equally distributed across foils
  qerr$chisq[i]<-round(mychi$statistic,2)
  qerr$p[i] <- round(mychi$p.value,3)
  
}
tabnumber<-tabnumber+1
ft<-flextable(qerr[,-c(2,6,7)])
ft <- set_caption(ft, paste0("Table ",tabnumber,": Distribution of error responses on quiz pre-training S-items"))
ft
```
*Note: Columns show N selections of each foil (Samples 1 and 2 combined), where Bias indicates a foil that demonstrates sample size neglect, and Other1 and Other2 denote the other two foils. The Chi Square value tests whether, for items answered incorrectly,  the selection of foils is random.  

Table `r tabnumber` shows errors on the pre-training quiz categorised as to whether they reflected sample size neglect (bias) or not (other 1 and 2). The 'bias' foil in each case explicitly mentioned that sample size (or gathering more data) would have no effect. Items S3A and S3B are omitted from this Table, as there is no specific foil that corresponds to this bias. For each item, the distribution of error responses was subjected to chi square test, to test the hypothesis that all foils were equally likely to be selected. For most items, the chi square value was statistically significant, indicating nonrandom choice of foils, and for items 1A, 1B, 2B, 5A and 6B the most common choice was the foil designated as reflecting sample size neglect. Items 1 and 5 were based on two of Tversky and Kahneman's classic examples of 'law of small numbers' (probability 60% boys being born in a big or small hospital, and likelihood of best player winning increasing with number of games); item 6 focused on how type II error rate is influenced by sample size. On items 2A, 4A and 4B, there was strong bias to select another foil. We discuss response patterns to specific items in Exploratory analysis below. 


_Prediction 2_: Accuracy on the beeswarm task will increase with training.  

```{r Prediction2,echo=F,warnings=F,message=F,results='hide',figleg = 'Beeswarm plot of earnings by block; bars show means'}
fignumber <- fignumber+1
beedat <- filter(mytab,block %in% c(1,4))
beedat100 <- beedat
w<-which(beedat100$subject %in% dropsub)
if(length(w)>0){
beedat<-beedat100[-w,] #beedat now has the excluded subjects removed
}
beedat4 <- beedat[beedat$block==4,] #Used later on for correlation plot
figname<-paste0("Fig",fignumber,".tiff")
tiff(figname, units="in", width=5, height=4, res=300)

beeswarm(beedat$earn~beedat$block+beedat$sample,pch=16,col='red',xlab='Block',ylab='Earnings per block',xaxt="n")

beedat1 <- filter(beedat,sample==1)
beedat2 <- filter(beedat,sample==2)
y1 <- mean(beedat1$earn[beedat1$block==1])
y2 <- mean(beedat1$earn[beedat1$block==4])
y3 <- mean(beedat2$earn[beedat2$block==1])
y4 <- mean(beedat2$earn[beedat2$block==4])
segments(.8,y1,1.2,y1,lwd=3)
segments(1.8,y2,2.2,y2,lwd=3) #x defined in terms of 2 groups 1 and 2 (not 1 and 4)
segments(2.8,y3,3.2,y3,lwd=3)
segments(3.8,y4,4.2,y4,lwd=3)
axis(1, at = 1:4,
       labels = c(1,4,1,4))
text(1.5,-1.4,'Sample 1')
text(3.5,-1.4,'Sample 2')

dev.off()
```
![Distribution of earnings in first and last blocks for Samples 1 and 2 ](Fig6.tiff)
```{r matchedt,echo=F}
#One tailed matched pairs t-test compare earnings on block 4 with earning on block 1
#Do analysis for each sample and combined, and compute BF

 
beedat<-beedat[!is.na(beedat$ediff),]
#remove two cases who scored high on S-items

  for (m in 1:3){
    mybee<-beedat
  if (m<3){
    mybee<-filter(beedat,sample==m)
   }
  
tearn <- t.test(mybee$ediff,alternative='greater')

BF <- ttestBF(mybee$ediff,alternative='greater')
#Bayes factors computed by model comparison
full_lm<-lm(mybee$ediff~1)
null_lm <-lm(mybee$ediff~0)
BF_BIC=exp((BIC(null_lm)-BIC(full_lm))/2)

if(m==1){BF1 <- BF_BIC
tearn1 <- tearn}
if(m==2){BF2 <- BF_BIC
tearn2 <- tearn}

}






```
  

Figure `r fignumber` shows the distribution of earnings for the first and last blocks in the two samples. A matched pairs _t_-test was used to compare earnings on block 4 with earnings on block 1, using a one-tailed test, as the prediction that the block 4 score will be higher.  This confirmed that learning had occurred in both samples: Sample 1: t (`r tearn1$parameter`) = `r round(tearn1$statistic,2)`, `r p.format(tearn1)`;  Sample 2: t (`r tearn2$parameter`) = `r round(tearn2$statistic,2)`, `r p.format(tearn2)`.  However, for Sample 2, the Bayes Factor (`r round(BF2,3)`) provided only anecdotal evidence against the null hypothesis, whereas for Sample 1, the evidence was solidly in favour of the alternative hypothesis (that the improvement in earnings was greater than zero), Bayes Factor = `r round(BF1,3)`.  With data from both samples combined, the Bayes Factor was `r round(BF_BIC,3)`, giving overwhelming evidence of learning. 

_Prediction 3_: Increased accuracy will be associated with selection of larger array sizes as learning proceeds  
We predicted a significant association between mean array index and mean earnings, such that those who earn most will be those who select larger array sizes (tested by significance of Pearson correlation between these variables in block 4). 

```{r Prediction3,echo=F,warnings=F,message=F,results='hide',figlegend='Relationship between mean array index and earnings in block 4'}
#One tailed matched pairs t-test compare earnings on block 4 with earning on block 1
fignumber <- fignumber+1
figname<-paste0('Fig',fignumber,'.tiff')
tiff(figname,units="in", width=5, height=5, res=300)

mycor1 <- cor.test(beedat4$array[beedat4$sample==1],beedat4$earn[beedat4$sample==1])
mycor2 <- cor.test(beedat4$array[beedat4$sample==2],beedat4$earn[beedat4$sample==2])
mycorall <- cor.test(beedat4$array,beedat4$earn)
plot(beedat4$array,beedat4$earn,pch=16,xlab='Mean array index',ylab='Mean earnings',col=beedat4$sample)
text(3,4.8,paste0('r = ',round(mycor1$estimate,3)),cex=.8)
text(3,4.3,p.format(mycor1),cex=.8)
text(3,3.8,paste0('r = ',round(mycor2$estimate,3)),col='red',cex=.8)
text(3,3.3,p.format(mycor2),col='red',cex=.8)
legend("topleft", legend=c(1,2),title="Sample",
       col=c("black", "red"), pch=16, cex=0.8)



dev.off()
```
  

![Association between array index and earnings in Samples 1 and 2](Fig7.tiff)
 
As shown in Figure `r (fignumber)`, this prediction was confirmed in both samples. For the combined sample, r (`r mycorall$parameter`) = `r round(mycorall$estimate,3)`, `r p.format(mycorall)`. 


```{r Prediction4,echo=F}

wantcols <-c('subject','Response')
qualresp <- all.dat[all.dat$Question.Key=='approach.feedback',wantcols]
qualresp <- qualresp[!is.na(qualresp$Response),]
qualresp <- qualresp[order(qualresp$subject),]
write.csv(qualresp,'Qualitative responses2.csv',row.names=F) #includes all 100 subjects

#ratings for these are now saved in 'coding of free responses_N100.csv'
#DB ratings 
qualresp$wait <- c(1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
                   1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
                   1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 
                   1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
                   0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
#JT ratings 
qualresp$waitJ <- c(1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
                    1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
                    1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
                    1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
                    0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0) 

#Only disagreement on two subject coding. DB happy with coding by JT, so we will go with qualresp$waitJ 


mytab$wait <- qualresp$waitJ #the values will be recycled across all 4 blocks
mytab100<-mytab
w<-which(mytab100$subject %in% dropsub)
mytab<-mytab100[-w,]

mytab1<-filter(mytab,sample==1,block==4)
mytab2<-filter(mytab,sample==2,block==4)

waitt1<-t.test(mytab1$earn~mytab1$wait)
waitt2<-t.test(mytab2$earn~mytab2$wait)
y <- mytab$earn[mytab$block==4]
x <-mytab$wait[mytab$block==4]

waitt<-t.test(y ~ x)
```


_Prediction 4_: Self-reported strategy will be predictive of learning  
We divided the self-reported responses into those that did and did not indicate awareness that larger sample sizes (or longer waiting) gives more reliable estimates. See online Appendix 2 (https://osf.io/uej95/) for response coding by two of the authors - this agreed for all but one participant in each sample; analysis was based on coding by JT.  Consistent with Pilot 2, we confirmed the prediction that there would be higher earnings in the final block for those who showed such awareness: one-tailed t-test in sample 1: 
t (`r round(waitt1$parameter,1)`) = `r round(waitt1$statistic,2)`, `r p.format(waitt1)`, and in sample 2: t (`r round(waitt2$parameter,1)`) = `r round(waitt2$statistic,2)`, `r p.format(waitt2)`, and in the combined sample t (`r round(waitt$parameter,1)`) = `r round(waitt$statistic,2)`, `r p.format(waitt)`. 

_Prediction 5_: Generalisation of learning  

As noted above, there was no indication of improvement on the post-training estimation quiz in either sample, making it unlikely that the pre-registered analysis would find any generalisation of learning gains on the beeswarm task to the S-items on the quiz. 

The results of the linear regression confirmed this was the case: individual differences in learning on the beeswarm task did not predict improvement on the S-items of the estimation quiz. The model was specified as:  
_postS ~ preS + earn.diff_      
where postS and preS are post-training and pre-training scores on S-items, and earn.diff is the difference in earnings (a measure of success on the task) between the last and first block of training.  



```{r quizlearn,echo=F}
qdata100<-qdata
w<-which(qdata100$subject %in% dropsub)
qdata <- qdata100[-w,]
quizag <- aggregate(qdata$correctresp,by=list(qdata$itemtype,qdata$prepost,qdata$PostList),FUN='mean')

```



```{r prediction5,echo=F}



#The variables we want are now in mytab and agqwide, with subs in same order. #BF calculation based on https://rpubs.com/lindeloev/bayes_factors


nsubx <- nsub-length(dropsub)  #need to correct the sample size for the dropped subjects

moddatfull <- agqwide[!is.na(agqwide$PostS),]
moddatfull <- cbind(moddatfull,mytab$ediff[1:nsubx],mytab$wait[1:nsubx],mytab$absLLdiff[1:nsubx])
#We bolt on the self-report strategy and LLdiff code as well as ediff, as it will be used in later exploratory analysis
nc <- ncol(moddatfull) #get index for added cols
colnames(moddatfull)[(nc-2):nc]<-c('ediff','wait','absLLdiff')
for (m in 1:3){
moddat<-moddatfull

if (m<3){
  moddat<-moddatfull[moddatfull$sample==m,]
}
myn<-nsubx
if (m==1){myn <- nsub1}
if(m==2){myn <- nsubx-nsub1}
model1 <- lm(moddat$PostS~moddat$PreS+moddat$ediff)
modeln <- lm(moddat$PostS~moddat$PreS)
BF_BIC = exp((BIC(modeln) - BIC(model1))/2)  # From BICs to Bayes factor
regcor<- cor(moddat$PostS,moddat$ediff)
mysummary <- as.data.frame(summary(model1)$coefficients)
mysummary <- round(mysummary,3)
mysummary<-cbind(c('Intercept','Pre-training S-items correct','Learning index'),mysummary)
colnames(mysummary)[1] <-'Source'
tabnumber <- tabnumber+1
 ft <- flextable(mysummary)
# ft<-autofit(ft)
 ft <- set_caption(ft, paste0("Table ",tabnumber,": Linear regression results for predicting percent correct on S-items on post-training quiz, from pre-training score and learning index (earnings difference from first to last block)"))

}
#cor(agqwide[,2:5])
ft
```


The learning index from the beeswarm task did not predict the post-test score on the S-items from the quiz, and the Bayes Factor (`r round(BF_BIC,2)`), computed by comparing the Bayesian Information Criterion (BIC) of the full model, with the BIC from the model omitting the learning index, provided moderate support for the null hypothesis. 


```{r checkqitems,echo=F}


mycolsA<-c(paste0('S',1:6,'A'))
mycolsB<-c(paste0('S',1:6,'B'))
allcols<-c(mycolsA,mycolsB)

qcols <-c(paste0(allcols,'-quantised'))
qu <- all.dat[all.dat$Question.Key %in% c(allcols,qcols),c('Question.Key','Response')]

w<-which(qu$Question.Key %in% qcols)
qux <- cbind(qu[-w,],qu[w,])
colnames(qux)[3:4] <- c('Quant','ResponseNum')
qux$quizrep<-paste0(qux$Question.Key,'.',qux$ResponseNum,' ',qux$Response)
qux1<-qux[order(qux$quizrep),]
quitems <- unique(qux1$quizrep)
write.csv(quitems,'quitemcheck.csv',row.names=F)

#Confirms correct assignment of S-item responses
```

## Exploratory analyses: quiz data

Given the overall lack of improvement on quiz items, we did not conduct any further analysis to see if specific items improved, but we did look more closely at the errors made on S-items, in an attempt to understand better what was driving responses. 

Items 1A and 1B were somewhat analogous to the "proportion of male births" item from the original @tversky1971 study, although we focused on means rather than proportions, to make the question more relevant to the training. The most common response (both before and after training) was to select a foil that stated that sample size did not matter, even though in training, the participants had been able to see that the mean bars for small samples were far more variable than those of large samples. These items, then, confirmed sample size neglect is commonplace, but that the training with beeswarms showing different sample sizes did not lead to  insight on the quiz. 

Items 2A and 2B were both designed to test understanding of the fact that a result in the "wrong direction" was more common with small than large samples, but the two items proved to be of very different level of difficulty. For item 2A the majority of participants concluded that a small sample (N = 10) with mean 158 was equally likely to have come from population A (mean 160, SD 6) or B (mean 158, SD 6), rather than the correct answer, which is that B is more likely, but a further 90 samples should be collected. 
Item 2B was analogous to item 2A, but the response options were rather different: there was no foil in which the two options (been poisoned or not been poisoned) were presented as equally likely. The most common response was the correct answer, which indicated that the observed result was inconclusive and more data was needed. 

We discuss items 4A and 4B next, because they were identical to items 2A and 2B in wording, except that the specified sample size was adequate for detecting the effect of interest. Note that because of counterbalancing, each participant saw only one version of each question (kangaroos or archaeology).  Error rates on these items were higher than for any other items, with a pronounced preference for a specific foil. For item 4A, where a statistical test would indicate overwhelming evidence for one conclusion (animals poisoned), the most common response was that 100 more animals needed to be tested. This was similar to the response to the analogous item 2B, but whereas this response was correct for 2B (where the sample size was 10), it was not appropriate in item 4A (where sample size was 160). 

For item 4B, the preferred response was the same as for the analogous item 2A - that the sample was equally likely to have come from either population - even though in item 2A there were two samples each of 10, and in item 4B there was two samples each of 160. This preference was equally pronounced after training on the beeswarm task, even though that task had given repeated exposure to samples of 160 per group with effect size of .3, confirming that an effect of this size is strong evidence that the sample came from the group with the lower population mean.  

Items S3A and S3B relate to statistical power by asking directly about sample size needed to be confident in an effect. Of course, the correct answer will depend on the interpretation of 'confident': in selecting possible responses, we took this to correspond to power of 95% or higher, so that the correct responses would be to select groups of 300 for item 3A (effect size .3) and groups of 100 for item 3B (effect size .4). For both of these problems, there was a spread of responses, but the most common response was the one we had designated as correct, selected by 32-38% of respondents. Experience with the beeswarm task, however, did not affect responses: the distribution of choices was closely similar before and after training. 

Items 5A and 5B were modeled on @kahneman1982's squash example: "a game of squash can be played either to 9 or to 15 points. Holding all other rules of the game constant, if A is a better player than B, which scoring system will give A a better chance of winning?" Item 5A simply expanded the available choices to 3 points, 9 points or 21 points, as well as offering the option that the number of points would make no difference. The most common response on pre-training was "makes no difference", consistent with sample size neglect. The pattern changed after training, with the correct response (21 points) being selected most often. However, this seems likely to be a fluke rather than indicating new insight in participants, because for item 5B, an analogous item framed around chess, the opposite trend was seen: an initial tendency to select the correct response, with the "makes no difference" response being preferred post-training. The chess problem had had the logic switched so the task was to judge which scenario gave the weaker player the better chance of winning. 

Items 6A and 6B focused on risk of type II error with small samples, i.e. a scenario is envisaged where a large difference is seen even though it is known there is no true effect. In both pre-training and post-training data, there was a fairly equal split between correct responses, indicating that this kind of result is more likely in a small sample, and responses indicative of sample size neglect - with an explicit statement that there will be no effect of sample size. 

We defer further interpretation of errors on the quiz to the Discussion.  


```{r explorequizitems, echo=F}
#ftab shows distribution of scores but does not look at individual items for improvement
#tabnumber <- tabnumber+1
#ftabquizitems
```


```{r dprimes,echo=F}
#initialise cols in mytab for signal detection indices
mytab$dprime<-NA
mytab$beta <- NA
for (i in 1:nsub){
blockstats<-data.frame(matrix(nrow=4,ncol=9))
colnames(blockstats) <- c('block','hit','fa','miss','cn','dprime','beta','pcorr','meanarray')
blockstats[,1]<-1:4

  idat <- short.dat[short.dat$subject==i,]
idat$resptype<-1 #default - this is HIT
w1<-which(idat$Correct==1)
w0<-which(idat$Correct==0)
x0<-which(idat$ES==0)
x1<-which(idat$ES>0)
fps<-intersect(w0,x0)
miss <- intersect(w0,x1)
cn <- intersect(w1,x0)
if(length(fps)>0) idat$resptype[intersect(w0,x0)]<-2 #false positive
if(length(miss)>0) idat$resptype[intersect(w0,x1)]<-3 #miss
if(length(cn)>0) idat$resptype[intersect(w1,x0)]<-4 #correct negative

#we have to put in fictitious data if there are no fps or misses to ensure table dims are correct
keep1 <-0
keep2 <- 0
if(length(fps)==0){
  keep1 <- idat$resptype[1]
idat$resptype[1]<-2} #put in a code for fp even though there are none - will correct later
if(length(miss)==0)
{keep2<- idat$resptype[2]
idat$resptype[2]<-3} #put in a code for miss even though there are none - will correct later
  
myt<-table(idat$block,idat$resptype) #steps above ensure table dimensions are correct, with all 4 types of response included


blockstats[,2:5]<-myt
#now remove fictitious data
if (keep1>0){
  blockstats[1,(keep1+1)] <- blockstats[1,(keep1+1)]+1
  blockstats[1,3] <- blockstats[1,3]-1
}
if (keep2>0){
  blockstats[1,(keep2+1)] <- blockstats[1,(keep2+1)]+1
  blockstats[1,4] <- blockstats[1,4]-1
}

blockstats$pcorr<-(blockstats$hit+blockstats$cn)/(nrow(idat)/max(idat$block))
for (b in 1:4){
sdt <- psycho::dprime(blockstats$hit[b], blockstats$fa[b], blockstats$miss[b], blockstats$cn[b],adjusted=TRUE)
blockstats$dprime[b]<-sdt$dprime
blockstats$beta[b]<-sdt$beta
}

w<-which(mytab$subject==i)
mytab$dprime[w]<-blockstats$dprime
mytab$beta[w]<-blockstats$beta

}

```

```{r usequalrating,echo=F}
#Here we use the self-report strategy coding to see if that predicts S-item learning
model2 <- lm(moddatfull$PostS~moddatfull$PreS+moddatfull$wait)
mysummary <- as.data.frame(summary(model2)$coefficients)
mysummary <- round(mysummary,3)
mysummary<-cbind(c('Intercept','Pre-training S','Learning index'),mysummary)
colnames(mysummary)[1] <-'Source'

```


```{r exploreLL,echo=F}
#NOT USED IN WRITE_UP
indsubplots <- 0 #option to explore individual subjects with composite plot for each one
#This uses short.dat which includes the 2 dropped subjects 
if(indsubplots==1){
# Core wrapping function - used for putting long text strings into a figure
wrap.it <- function(x, len)
{ 
  sapply(x, function(y) paste(strwrap(y, len), 
                              collapse = "\n"), 
         USE.NAMES = FALSE)
}


#First we'll plot for each subject accuracy vs ObsE or LL, trial by trial.
sublist <- order(mytab$Correct[mytab$block==4])
resplist <- c('Excellent','Better than Average','Average','Below Average')
 
for (s in 1:nsub){
  i <- sublist[s]
  #The pdf will label according to rank order correct, rather than subject name
   pdf(file = paste0('IndPlots/subrank_',s,'.pdf'),   # The directory you want to save the file in
     width = 6, # The width of the plot in inches
     height = 8) # The height of the plot in inches

  mysub <- short.dat[short.dat$subject==i,]
  mysum <- mytab[mytab$subject==i,]
  myq<-myagq[myagq$subject==i,]
  myq$p.correct<-round(100*myq$p.correct,0)
  mysub$row<-1:nrow(mysub)
  mycolor<-c('pink','lightblue')
  mypch<-c(22,23)
  #par(mfrow=c(3,1))
  layout.matrix <- matrix(c(1,1,3,4,
                            2,5,3,4),nrow=4,ncol=2)
  layout(mat = layout.matrix,
         heights=c(1,1,1),
         widths = c(3,3))
 # layout.show(5)
  
  #First plot is just used for recording details of the subject, and their qualitative response to strategy question.
  #We start with a blank plot
  
  plot(0, type = 'n', axes=F, main=paste0("Subject ",i), xlim=c(0,20), ylim=c(0,20),xlab='',ylab='')
  quresp<-paste0('Strategy: ',qualresp$Response[i])
   text(1,18,wrap.it(quresp,40),adj=0)
  
   startdprime <- mysum$dprime[1]
   enddprime <- mysum$dprime[4]
   demobit <- demotab[demotab$ID==i,]
   text(1,11,paste0('Stats self-rating = ',demobit$Stats_self_rating),adj=0)
    text(1,9,paste0('Understanding of t-test = ',resplist[demobit$Understand_t]),adj=0)
    text(1,7,paste0('Understanding of power = ',resplist[demobit$Understand_power]),adj=0)
    text(1,5,'Quiz',adj=0)
    for (q in 1:4){
      text(1,(5-q),paste0(myq$dummy[q],': ',myq$p.correct[q],'%'),adj=0)
    }
    legend(13, 4, legend=c("blue>pink", "blue=pink","error"),
       col=c("lightblue", "pink","black"), pch=c(15,15,22))
    
    
  #2nd shows dprime change
 dprimebit <- mytab[mytab$subject==i,]   
plot(dprimebit$dprime ~ dprimebit$block,type='b',col='black',ylim=c(-1,4),ylab='mean',xlab='Block',
     xaxt = 'n')
lines(dprimebit$beta ~ dprimebit$block,type='b',col='green')
abline(h=0,lty=2)
legend(1, 4, legend=c("dprime", "beta"),
       col=c("black", "green"), lty=1,cex=0.8)
# X-axis - used xaxt =  n above to suppress axis with unwanted tick marks
axis(1, at =1:4)

  #Plots 3 and 4 show trial by trial Obs E and LL  
  for (j in 1:2){
    myy <- mysub$ObsE
    limy <- c(-.8,.8)
    placey <-.78
    meany <- .15
    mylab <- 'Observed ES'
    if(j==2){
      myy<- mysub$LL
      limy <- c(-20,20)
      placey <-18
      meany <- 0
      mylab <- 'LL'
    }

  plot(mysub$row,myy,pch=mypch[(1+mysub$Correct)],bg=mycolor[as.factor(mysub$Response)],ylim=limy,
       cex=2,col=(1+(-1*mysub$Correct)),xlab='trial',ylab=mylab)
  abline(h=meany)
  abline(v=c(20,40,60))
  maxx=placey
  text(3,maxx,paste0('pcorr=',round(mysum$Correct[1],2)),cex=.9)
  text(24,maxx,round(mysum$Correct[2],2),cex=.9)
  text(45,maxx,round(mysum$Correct[3],2),cex=.9)
  text(66,maxx,round(mysum$Correct[4],2),cex=.9)
  text(myy ~mysub$row, labels=mysub$array, cex=0.75)
  }
  


#Response colour coded: blue greater shown as blue; blue=pink shown as pink
# Black border from wrong responses; number corresponds to array index
# Figures at top are % correct per block


  #5th plot shows LL divergence over blocks
    
forbee2 <- aggregate(mysub$LL,by=list(mysub$block,mysub$Response),FUN=mean)
plot(forbee2$x[1:4],type='b',col='pink',ylim=c(-20,20),ylab='mean LL',xlab='Block',xaxt='n')
lines(forbee2$x[5:8],type='b',col='lightblue')
# X-axis - used xaxt =  n above to suppress axis with unwanted tick marks
axis(1, at =1:4)



dev.off()
}
}
```

```{r dprimebywait,echo=F,message=F}
#Here we use mytab with has excluded two subjects

dagg <- aggregate(mytab$array,by=list(mytab$wait,mytab$block),FUN='mean')
colnames(dagg)<-c('Aware','Block','array')
dagg$Aware <- as.factor(dagg$Aware)
p1 <- ggplot(data=dagg, aes(x=Block, y=array, group=Aware)) +
   labs(y="Array index")+
  geom_line(aes(color=Aware))+
  theme(legend.position = "none") +
  geom_point()

dagg <- aggregate(mytab$dprime,by=list(mytab$wait,mytab$block),FUN='mean')
colnames(dagg)<-c('Aware','Block','dprime')
dagg$Aware <- as.factor(dagg$Aware)
p2 <- ggplot(data=dagg, aes(x=Block, y=dprime, group=Aware)) +
  labs(y="d prime")+
  geom_line(aes(color=Aware))+
   theme(legend.position = "none") +
  geom_point()

dagg <- aggregate(mytab$absLL,by=list(mytab$wait,mytab$block),FUN='mean')
colnames(dagg)<-c('Aware','Block','absLL')
dagg$Aware <- as.factor(dagg$Aware)
p3 <- ggplot(data=dagg, aes(x=Block, y=absLL, group=Aware)) +
   labs(y="Absolute Log Likelihood")+
  geom_line(aes(color=Aware))+
   theme(legend.position = "none") +
  geom_point()

dagg <- aggregate(mytab$absObsE,by=list(mytab$wait,mytab$block),FUN='mean')
colnames(dagg)<-c('Aware','Block','absObsE')
dagg$Aware <- as.factor(dagg$Aware)
p4 <- ggplot(data=dagg, aes(x=Block, y=absObsE, group=Aware)) +
   labs(y="Absolute Effect Size")+
  geom_line(aes(color=Aware))+
  geom_point()


allp <- grid.arrange(p1, p2, p3, p4, nrow = 2,ncol=2)
fignumber <- fignumber+1
figname<-paste0('Fig',fignumber,'.tiff')
ggsave(figname,allp,units="in", width=6, height=4)


#We'll ignore beta: no systematic biases found, and they have been told equal N true and null trials
# dagg <- aggregate(mytab$beta,by=list(mytab$wait,mytab$block),FUN='mean')
# colnames(dagg)<-c('Aware','Block','beta')
# ggplot(data=dagg, aes(x=Block, y=beta, group=Aware)) +
#   geom_line(aes(color=as.factor(Aware)))+
#   geom_point()

```

_Figure `r fignumber`. Comparison of Aware and Unaware subgroups over 4 learning blocks, from L to R: mean array size at point of response, amount of evidence in favour of one hypothesis at the point of response (mean absolute log-likelihood), accuracy (mean d prime), and observable difference between means (mean observed effect size at the point of response)._
<!---this figure seems to print from chunk even with echo=F--->

__Beeswarm task__  
Exploratory analysis on the beeswarm task focussed on visual comparisons between those who were categorised according to self-report as being aware that sample size was important (Aware group), versus those who were not (see Figure `r fignumber`. These two groups were compared for each block in terms of mean array size at point of response, amount of evidence in favour of one hypothesis at the point of response (mean absolute log-likelihood), accuracy (mean d prime), and observable difference between means (mean observed effect size at the point of response).  The first point to note is that the data on array size validates the self-rating of strategy, with the Aware group showing an increase in array size over time that is not seen in the Unaware group. On the d-prime measure, the Unaware group does show some evidence of increasing accuracy over blocks, but they lag behind the Aware group. The Aware group show an increase in mean absolute log-likelihood over blocks, whereas the Unaware group, in contrast, appear to respond more to the observed effect size, i.e. they are influenced more by the difference between blue and pink lines depicting means, ignoring the sample size or variability around the mean. These trends are not surprising, as absolute log likelihood increases and absolute observed effect size decreases with array size.   

```{r awarenesstable,echo=F}

#See how Awareness relates to self-ratings
#First order mytab by ID
mywait<-mytab[mytab$block==1,]
mywait<-mywait[order(mywait$subject),]


mywait$Aware<-factor(mywait$wait,
                       levels=c(0,1),
                       labels=c('Unaware','Aware'))

#Add data from questionnaire
demotab2<-demotab2[order(demotab2$ID),] #subjects in numeric order
w<-which(demotab2$ID %in% dropsub)
demotab3 <- demotab2[-w,]
demotab3$Aware <- mywait$Aware
units(demotab3$Stats_self_rating) <- "self report: 0-100"
units(demotab3$Understand_t) <- "self report: 1=high, 4=low"
units(demotab3$Understand_power) <- "self report: 1=high, 4=low"

#convert proportions to total correct
demotab3$PreP <- 6*agqwide$PreP[seq(1,(2*nsubx),2)]
demotab3$PostP <- 6*agqwide$PostP[seq(1,(2*nsubx),2)]
demotab3$PreS <- 6*agqwide$PreS[seq(2,(2*nsubx),2)]
demotab3$PostS <- 6*agqwide$PostS[seq(2,(2*nsubx),2)]

awaretable <- table1(~Ed.level+Stats_self_rating+ Understand_t+Understand_power+PreP+PostP+PreS+PostS|Aware,data=demotab3)

awtab <- as.data.frame(awaretable)
awtab<-awtab[,1:3]
awtab[16,1]<- 'Quiz: Pretraining P'
awtab[19,1]<- 'Quiz: Posttraining P'
awtab[22,1]<- 'Quiz: Pretraining S'
awtab[25,1]<- 'Quiz: Posttraining S'
```

```{r cohendaware,echo=F}
awtab$CI<-""
mycols<-c("Stats_self_rating", "Understand_t", "Understand_power", "PreP", "PostP", "PreS","PostS")
myrows<-seq(8,26,3) #rows in awtab to write to
for (x in 1:length(mycols)){
  thiscol<-which(colnames(demotab3)==mycols[x])
  thisrow<-myrows[x]
  myCI <- t.test(demotab3[,thiscol]~demotab3$Aware)$conf.int
  awtab$CI[thisrow]<-paste0(round(myCI[1],2),', ',round(myCI[2],2))
  
}
w<-which(colnames(awtab)=='CI')
colnames(awtab)[w]<-'95% CI for d'

```

```{r awtabflex,echo=F}
w<-which(awtab[,1]==awtab[9,1])
awtab<-awtab[-w,]

faw<-flextable(awtab)
tabnumber<-tabnumber+1
faw <- set_caption(faw, paste0("Table ",tabnumber,":Characteristics of participants who were aware or unaware of importance of sample size"))
faw<-autofit(faw)
faw
```
 

The substantial differences in learning between participants prompts the question of whether there were any pre-existing differences that might explain why some improved in the Beeswarm game and others did not. Table `r tabnumber` shows mean scores on self-rating of statistical knowledge and on quiz items, and the 95% confidence interval for the mean difference between the Aware and Unaware subgroups. The Aware subgroup appeared to have better statistical knowledge prior to training, both in terms of self-report, and in terms of performance on the quiz,  Nevertheless, there was no indication that their quiz performance improved more with training than the Unaware group. 

# Discussion  

Results may be summed up as showing: a) Participants were poor at making probabilistic judgments where sample size was critical; b) They showed significant improvements on the beeswarm task over sessions; c) this was most pronounced in those who were aware of the need to wait for larger samples before responding; d) Improvement on the beeswarm task was not associated with performance on the quiz: scores on parallel forms were similar before and after training.

We will first discuss responses to the quiz, then performance on the beeswarm task, and then the lack of generalisation of learning from the beeswarm task. Results on both tasks should be interpreted bearing in mind that participants received monetary bonuses for correct responses on both the quiz and the beeswarm task, and this appeared to be successful in providing motivation to take the tasks seriously. Also, note that our method deliberately avoided mentioning statistical significance or p-values at any point in the study, as these can be sources of confusion. 


__Responses to the quiz__  
Results confirmed that people were considerably worse at evaluating statements where the correct answer depended on sample size than they were at judging statements that required simple interpretation or manipulation of probabilities. Furthermore, on items that were closely modeled on those of @kahneman1972, response options that stated sample size was immaterial tended to be preferred over the correct option. To that extent, the results support the original work on the "Law of Small Numbers". Nevertheless, results varied considerably across quiz items, and in some cases there was no preference for foils that were selected to detect sample size neglect. Indeed, for some items, participants preferred responses that stated more data should be gathered, even when this was not necessary, suggesting some awareness that sample size could be important. 

A particularly intriguing pattern of results was seen on items 2 and 4, which had identical wording, except that on item 2 the sample size was inadequate to demonstrate an effect, and in item 4 it was adequate. At first glance, these items seem to contradict the idea of sample size neglect, because few participants selected responses that explicitly stated that sample size didn't matter, and indeed, they tended to prefer responses indicating more data should be gathered. However, their tendency to respond this way was the same regardless of whether they were told that sample size was 10 or 160. Furthermore, for the items where they had the opportunity to say that the sample was equally likely to have come from either group, they preferred that option, regardless of sample size.  

With hindsight, it would have been helpful to ask participants to explain the reasoning behind their responses, as this could have thrown light on the unexpected patterns on these two items. Also, the results on items 2 and 4 suggest there could be interest in following the approach adopted in one study by @kahneman1972, where the sample size specified in a problem was parametrically varied. 


__Learning on the beeswarm task__  
Around one third of participants indicated that they had learned to wait for larger arrays before responding, and this tendency was associated with higher levels of accuracy on the beeswarm task. Those who learned to wait for larger arrays showed an increase in accuracy over sessions, whereas those who did not appeared to rely on observed effect sizes, which, in small samples, were an unreliable cue to whether samples came from different populations.
 
__Failure of learning to generalise__   
The failure to show any improvement on quiz items after training is not encouraging but perhaps should not surprise us. The training was relatively brief, and the link between the insight - that sample size affected the ability to detect a true difference between groups - and the content of quiz items was not made obvious. As @hodgson2000 have noted, exposure to simulated data alone is not sufficient to inculcate learning: one needs to ensure students are focused on the relevant aspect of a task, with debriefing and follow-up exercises to deepen understanding. @aberson2000 showed improved understanding of the central limit theorem after training on a web-based tutorial where students drew samples of different sizes from a population, but they provided relevant instruction and discussed the concept of the standard error of the mean in depth as part of the training.  Other studies have found mixed results from use of simulation in training [@garfield2007], [@hancock2020], and where learning is found it can remain specific to the precise context of training. In an early study, @well1990 gave people prolonged interactive training to promote understanding of the sampling distribution of the mean, but found that although this gave better understanding of the effects of sample size, it did not improve ability to answer questions about variability of the mean in different sized samples. To counteract sample size neglect, we are likely to need  more explicit instruction across a range of contexts: for instance, after asking participants to respond to a quiz item, they could be asked to simulate data based on that specific problem, to see how variation in sample size affects the parameter of interest (cf. @lane2006). Also, consistent with @well1990 and @hodgson2000, we recommend that participants be asked about the reasoning behind their responses to such questions, as this can give insights about the heuristics they adopt. 

@kahneman1972 regarded the phenomenon of sample size neglect as an instance of the representativeness heuristic, whereby judgments are based on the perceived similarity of a sample to its parent population, and the extent to which the sample reflects salient features of the process by which it is generated. The difficulty in overcoming this bias suggests that representativeness alone may be insufficient to account for it. Rather, it may be that there is conflict between knowledge about statistical parameters, such as mean and SD, which are unaffected by sample size, and the variability of means, which depend on sample size. Understanding how a mean is computed is an elementary part of statistical training; understanding the nature of variability in that mean comes later in training, if at all, and requires more computational steps. Furthermore, @well1990 found that, when given problems that required answers about the variation in a mean, many participants thought they were being asked about variability in the sample.  

Despite decades of research on sample size neglect, it remains a pervasive problem. Given that statistical methods depend on understanding sampling theory, we need to do more to optimise and evaluate methods for developing understanding of the distinction between variation in individual data-points and variation in means derived from those data-points. Using simulations to give students active experience of characteristics of different sized samples may be one component in the quest for better methods, but it clearly is not sufficient on its own. 


# Appendix 1: Questions given before and after training


<!---modified from Probqs_edit_from_feedback.rmd--->
### Initial queries (Pre-training only)

i. Compared to others at your level of education in your participant, how good do you think your understanding of basic statistics is?

```{r makestatcut,echo=F}
demotab$statrating <- cut(demotab$Stats_self_rating,breaks=c(-1,25,50,75,100))
levels(demotab$statrating) <- 1:4
```
[This item was rated on scale from 0 to 100. Divided into 4 levels at 25,50 and 75]  
`r paste0("[", length(which(demotab$statrating==4)),"]")`  a) Excellent  
`r paste0("[", length(which(demotab$statrating==3)),"]")`  b) Better than average  
`r paste0("[", length(which(demotab$statrating==2)),"]")`  c) Average  
`r paste0("[", length(which(demotab$statrating==1)),"]")`  d) Below average 

ii. How confident are you in interpreting the results of a t-test?  
`r paste0("[", length(which(demotab$Understand_t==4)),"]")` a) Very confident  
`r paste0("[", length(which(demotab$Understand_t==3)),"]")` b) Fairly confident  
`r paste0("[", length(which(demotab$Understand_t==2)),"]")` c) Not confident  
`r paste0("[", length(which(demotab$Understand_t==1)),"]")` d) Very little idea about what a t-test is  
  
iii. How familiar are you with the idea of statistical power?  
`r paste0("[", length(which(demotab$Understand_power==4)),"]")` a) Very familiar  
`r paste0("[", length(which(demotab$Understand_power==3)),"]")` b) Reasonably familiar  
`r paste0("[", length(which(demotab$Understand_power==2)),"]")` c) Somewhat familiar  
`r paste0("[", length(which(demotab$Understand_power==1)),"]")` d) Very little idea about what statistical power is  

### Qualitative report (Post-training only)
Did you feel you got better at the task over time?  
`r paste0("[", length(which(demotab$better_over_time=='Yes')),"]")` Yes
`r paste0("[", length(which(demotab$better_over_time=='Unsure')),"]")` Unsure
`r paste0("[", length(which(demotab$better_over_time=='No')),"]")` No

Did you change how you approached the task?  Please let us know if you adopted any specific strategy to guide your response?  
[See Appendix 2 for responses]  

## judgment and Reasoning Quiz  


```{r readposttraining, echo=F}
qdf2 <- read.csv('quiz_items_Post.csv')


#function to bolt together N responses of each type for pre and post trainng
maketext <- function(myitem,resp){
  respcol<- which(colnames(qdf)==paste0('R',resp))
  mytext <- paste0("[", qdf[qdf$Item==myitem,respcol],"/", qdf2[qdf2$Item==myitem,respcol],"]")
  return(mytext)
}
```


Participants received version A or B (counterbalanced) at pre-test and post-test. P-items test understanding of probability, S-items test sample size neglect. Items with the same number in version A and B are intended to be analogous but use different contexts. The brief descriptor accompanying each item below is not presented as part of the quiz. An asterisk denotes the correct response.  For S-items, x denotes the anticipated response(s) for those susceptible to sample size neglect.

P-items and S-items were inadvertently presented blocked for sample 1, but were randomly intermixed as intended for sample 2. As described in Results, this did not make any difference. This section of the Appendix has now been updated to show the frequency of endorsement of each option in square brackets, first for pre-training and then for post-training administration of the quiz.
   
### P-items
#### P1A (tests elementary knowledge of normal distribution)
You take a sample of 100 men from the general population and measure their height.  The mean is 70 inches and the standard deviation is 4 inches. What percentage of the sample will be expected to be more than 74 inches tall?  
 `r maketext('P1A',1)` a) *16%  
`r maketext('P1A',2)` b) 2%  
`r maketext('P1A',3)` c) 50%  
`r maketext('P1A',4)` d) 30%  
 
#### P1B  
A reading test is standardized on 7-year-old children in Scotland. The mean reading age is 84 months with standard deviation of 6 months.  What percentage of 7-year-olds is expected to have a reading age of 72 months or less?  
 `r maketext('P1B',1)` a) 16%  
 `r maketext('P1B',2)` b) *2%  
 `r maketext('P1B',3)` c) 50%  
 `r maketext('P1B',4)` d) 30%  
 

```{r q1a, include=F,echo=F}
#All quiz items simulated to check accuracy
#NB for accurate estimate we use N = 10000
mydata <- rnorm(10000,70,4)
w<-which(mydata>74)
ans1<-100*length(w)/10000
print(paste0("The answer is ",ans1))

mydata <- rnorm(10000,84,6)
w<-which(mydata<72)
ans2<-100*length(w)/10000
print(paste0("The answer is ",ans2))

```
#### P2A (Basic probability). 
A container is full of spare change containing 100 10p coins, 100 5p coins, 200 2p coins, and 100 1p coins. The coins are randomly mixed. You will get a prize every time you pick a 5p coin. If you make 100 selections, replacing the selected coin and shaking the jar each time, how often will you expect to pick a 5p coin?  

 `r maketext('P2A',1)` a) 1 in two occasions  
 `r maketext('P2A',2)` b) 1 in three occasions  
 `r maketext('P2A',3)` c) 1 in four occasions   
 `r maketext('P2A',4)` d) *1 in five occasions  
  
#### P2B 
You are choosing marbles from an opaque jar containing 100 red marbles, 100 blue marbles, and 200 white marbles, randomly mixed. You will get a prize every time you pick a marble that is not white. If you make 100 selections, replacing the selected marble and shaking the jar each time, how often will you expect to get a prize?  

 `r maketext('P2B',1)`  a) *1 in two occasions  
 `r maketext('P2B',2)` b) 1 in three occasions  
 `r maketext('P2B',3)` c) 1 in four occasions   
 `r maketext('P2B',4)` d) 1 in five occasions 
 

```{r P2B,include=F,echo=F}
myselect<-vector()
mymarbles<-c(rep(1,100),rep(2,100),rep(3,200))
for (i in 1:100){
  myselect<-c(myselect,sample(mymarbles,1))
}
myt<-table(myselect)
names(myt)<-c('Red','Blue','White')
myt
myt/500
 
```

#### P3A (Classic probability including averaging probabilities)
In the city of Ficticium, there is generally a 10% chance it will rain on any given day in the first half of September, a 50% chance it will rain any given day in the second half of September,  a 25% chance it will rain on any given day in November, and a 30% chance it will rain on any given day in April. (September, November and April are all 30 days long). Which month is likely to have more rainy days?  
 `r maketext('P3A',1)` a) September   
 `r maketext('P3A',2)` b)  November    
 `r maketext('P3A',3)` c)  April   
 `r maketext('P3A',4)` d) *September and April are equally likely  
   

 
 

```{r P3A, include=F, echo=F}
myrain<-data.frame(matrix(NA,nrow=1000,ncol=3))
names(myrain)<-c('Sep','Nov','Apr')
for (i in 1:10000){
sep1<-rbinom(1,15,.1)
sep2<-rbinom(1,15,.5)
nov<-rbinom(1,30,.25)
apr<-rbinom(1,30,.3)
myrain[i,1]<-sep1+sep2
myrain[i,2]<-nov
myrain[i,3]<-apr
}
print ('Mean rainy days')
print(paste('September ',mean(myrain$Sep)))
print(paste('November ',mean(myrain$Nov)))
print(paste('April ',mean(myrain$Apr)))
```

#### P3B. 
After reviewing sales of coffee, the barista noted there is generally a 50% chance of selling an espresso on any day in the first week of the month. There is a 100% chance of selling an espresso on any day in the second week of the month, and a 75% chance of selling an espresso on any day in the third and fourth week of the month. In what two-week period in the month is there most sales of espresso?  

`r maketext('P3B',1)` a) *First two weeks and second two weeks are equally likely    
`r maketext('P3B',2)` b) First two weeks      
`r maketext('P3B',3)` c) Second two weeks  
`r maketext('P3B',4)` d) First and fourth weeks  
   


#### P4A. (basic probability)
At a raffle, you can pick from a green bowl with 2 winning raffle tickets and 8 worthless tickets, a yellow bowl with 10 winning tickets and 90 worthless tickets, or a red bowl with 15 winning tickets and 85 worthless tickets.  Which bowl gives you a better chance of winning?  
 `r maketext('P4A',1)` a) *Green   
 `r maketext('P4A',2)` b) Yellow     
 `r maketext('P4A',3)` c) Red    
 `r maketext('P4A',4)` d) Green and Yellow give the same chance 

 
```{r P4A, include=F,echo=F}
greencount<-yellowcount<-redcount<-0
for (i in 1:1000){
green<-c(rep(1,2),rep(0,8))
yellow<-c(rep(1,10),rep(0,90))
red<-c(rep(1,15),rep(0,85))
greencount<-greencount+sample(green,1)
yellowcount<-yellowcount+sample(yellow,1)
redcount<-redcount+sample(red,1)
}
print(paste('Green p = ',greencount/1000))
print(paste('Yellow p = ',yellowcount/1000))
print(paste('Red p = ',redcount/1000))
```

#### P4B. 
You find three bags of jelly beans. There is a small bag with 5 red jelly beans and 20 other coloured beans. There is a  medium bag with 12 red jelly beans and 38 other coloured beans. Finally, there is a large bag with 18 red jelly beans and 82 other colour beans. As red jelly beans are your favourite flavour, which size bag gives you the highest percentage of red beans?   

 `r maketext('P4B',1)` a) Small  
`r maketext('P4B',2)` b) *Medium      
`r maketext('P4B',3)` c) Large   
`r maketext('P4B',4)` d) They are all the same  
   



#### P5A. (conjoint probability)
There are 100 girls in a class, 20% have red hair and 40% are taller than 60 inches.  How many red-headed girls would you expect who are taller than 60 inches?  
`r maketext('P5A',1)` a) 60   
`r maketext('P5A',2)` b) 16      
`r maketext('P5A',3)` c) *8   
`r maketext('P5A',4)` d) 5  

 

```{r P5A,echo=F,include=F}
# This gives a different answer on each run, but converges on 8 (.4*.2)
girls <- c(rep(1,20),rep(0,80))
tallgirls<-sample(girls,40) #random selection from girls
tallred <- sum(tallgirls)
print(tallred)
```

#### P5B
In a box of 100 CDs, 40% of the disks have been recorded by pop artists and 60% of disks feature female vocalists. Assuming that the female vocalists are equally likely to be pop artists or not, how many CDs have been recorded by female pop vocalists?   

`r maketext('P5B',1)` a) 12 CDs   
`r maketext('P5B',2)` b) 18 CDs     
`r maketext('P5B',3)`c) *24 CDs  
`r maketext('P5B',4)` d) 30 CDs  


#### P6A (classic probability - multiplication)
10 percent of the children in a school have red hair. Their names are put in a hat and you are asked to pull out two of them. What is the probability that you will select two red-headed children?  

`r maketext('P6A',1)` a)  20 percent    
`r maketext('P6A',2)` b) *1 percent      
`r maketext('P6A',3)` c)  19 percent     
`r maketext('P6A',4)` d)  close to zero  



  

```{r q12a, include=F, echo=F}
print(paste(100*.1*.1,' percent'))
```

#### P6B. 
20 percent of the 500 children in a school have a name beginning with J. Their names are put in a hat and you are asked to pull out two of them. What is the probability that you will select  two children whose name begins with J?  
`r maketext('P6B',1)` a) *4 percent       
`r maketext('P6B',2)` b)  1 percent      
`r maketext('P6B',3)` c)  40 percent  
`r maketext('P6B',4)` d)  20 percent  


 
  
   

```{r P6B,include=F,echo=F}
print(paste(100*.2*.2,' percent'))
```

### S-items 
#### S1A (frequency of extreme scores in small samples)

History courses in the UK are rated on a Student Survey, which has an overall satisfaction rating of 1 (low) to 5 (high). The mean rating overall is 3.5, with SD of 1. Those who have an average rating above 4.0 are given a gold star in league tables. There are 60 courses altogether, which can be divided according to size into small (less than 10 students), medium (between 10-50 students) and large (50+ students). If there are no real differences in student satisfaction, will the number of students affect the likelihood of getting a gold star?  

`r maketext('S1A',1)` a) Yes. Large courses have a better chance of getting a gold star     
`r maketext('S1A',2)` b) Yes. Medium-sized courses have a better chance of getting a gold star       
`r maketext('S1A',3)` c) *Yes. Small courses have a better chance of getting a gold star       
`r maketext('S1A',4)` d) x No. The number of students makes no difference.

 
  

```{r S1A, echo=F, include=F}
size <- c(5,30,60)

for (u in 1:3){
  ustring<-NA
  for (v in 1:1000){
mydat<-rnorm(size[u],3.5,1)
ustring<-c(ustring,mean(mydat))
n[u]<-length(which(ustring>4))
}
}

#Print n to see the N cases scoring above 4 for each sample size

```

#### S1B. 
A task force is looking at characteristics of schools in its area. They take a measure of mathematical ability, and identify schools as 'failing schools' where 20% or more pupils get scores more than 1 SD below the population average. The smallest schools have on average 100 pupils, middle-sized schools have 250 pupils, and the largest schools have 500 pupils. 
If there are no real differences between schools, will the size of the school affect whether it is a failing school?  
`r maketext('S1B',1)` a) *Yes. The smallest schools will be more likely to be a failing school  
`r maketext('S1B',2)` b)  xNo. All else being equal, school size should make no difference  
`r maketext('S1B',3)` c)  Yes. The largest schools will be more likely to be a failing school  

`r maketext('S1B',4)` d)  Yes. The middle-sized schools will be more likely to be a failing school  


 

```{r S1B,include=F, echo=F}
N  = 1000 #N times sampled
schoolsize<- c(100,250,500)
myschool <- data.frame(matrix(NA, nrow=N, ncol=6))
names(myschool)<-c('Small','Medium','Large','SmallF','MediumF','LargeF')
for (i in 1:N){
  myschool[i,1]<-length(which(rnorm(schoolsize[1],0,1)<(-1)))
  myschool[i,2]<-length(which(rnorm(schoolsize[2],0,1)<(-1)))
  myschool[i,3]<-length(which(rnorm(schoolsize[3],0,1)<(-1)))
  for (j in 1:3){
    myschool[i,(j+3)]<-0
    if (myschool[i,j]/schoolsize[j]>.2){
      myschool[i,(j+3)]<-1
    }
  }
}
print(paste('Out of ',N,' schools, this is number coded as failing, for Small, Medium and Large respectively'))
for (j in 1:3){
  print(sum(myschool[,(j+3)]))
}

```



#### S2A (probability of result in 'wrong' direction is higher with small sample size)
A forensic archaeologist has a set of 10 male skeletons from an ancient burial site. Experts are divided as to whether the site was colonised by Tribe A or Tribe B- the only 2 tribes in the region. Previous studies with large samples found that men from Tribe A had an average height of 160 cm, with SD of 6, and men from Tribe B had an average height of 158 cm, with SD of 6. 

The mean height of the sample is 158 cm. What can the archaeologist conclude?  
`r maketext('S2A',1)` a)    We can be very confident that the sample comes from Tribe B  
`r maketext('S2A',2)` b)  * Tribe B is more likely than Tribe A, but should collect 90 more samples to be sure  

`r maketext('S2A',3)` c)    Equally likely the sample comes from Tribe A or Tribe B  

`r maketext('S2A',4)` d)  x  Can't be sure: Tribe B is more likely than Tribe A, but collecting more samples won't help  

 

    

```{r S2A,include=F,echo=F}
#We simulate data to compute the probability that the observation comes from tribe A
 #The observed value is half a SD below average for sample A       
mA= 160  #tribe A
sA=6
mB=158 #tribe B
sB=6
obsmean <- 158
obsdiffA <- (mA-obsmean)/sA
niter<- 1000
for (n in c(10,50,100)){ #try with different sample sizes
  print(paste0('n = ',n))

counter1 <- 0 #initialise counters for various difference scores

alld<-NA
for (i in 1:niter){
xsample<-rnorm(n,mA,sA) #observed sample - actually from tribe A
d<- mean(xsample)-mA
alld<-c(alld,d) 
critd<-obsdiffA*sA
if (d>critd){counter1 <- counter1+1}	
}



print(paste0('prob that xsample is .33 SD less than xA mean: ',counter1/niter))


}


```


#### S2B (probability of result in 'wrong' direction is higher with small sample size)
There are concerns that kangaroos appear unhealthy in a particular area of the Australian bush. There is concern that this may be due to eating poisoned bait that affects the blood. A naturalist has blood samples from 10 kangaroos, and measures a distinctive blood marker that is lowered in poisoned animals. Previous studies have found that the mean blood marker in healthy kangaroos is 130 with SD of 30, whereas the mean is 120 with SD of 30 in poisoned animals. 

The mean blood marker in the sample is 120. Assuming that there is no other explanation than poisoning for an abnormal blood marker, what can the scientist conclude?  
`r maketext('S2B',1)` a)    The kangaroos have definitely been poisoned     

`r maketext('S2B',2)` b)  x  The kangaroos probably have been poisoned but can't be sure. Collecting data from more kangaroos won't help.  
`r maketext('S2B',3)` c)  * The kangaroos probably have been poisoned, but would need to collect blood from 90 more kangaroos in the affected area to be sure  
`r maketext('S2B',4)`d)    The kangaroos have not been poisoned  





    
```{r S2B,include=F,echo=F}
#We simulate data to compute the probability that the observation comes from unpoisoned animals 

mA = 120
sA = 30
mB = 130
sB = 30
alld <- vector()
obsmean <- 120

obsdiff <- (mB - obsmean) / sB
niter <- 1000
for (n in c(10, 50, 100)) {
  #try with different sample sizes
  print(paste0('n = ', n))
  
  counter1 <- 0 #initialise counter
  
  for (i in 1:niter) {
    xsample <-
      rnorm(n, mB, sB) #observed sample - actually from unpoisoned (B)
    d <- mB - mean(xsample)
    alld <- c(alld, d)  #can plot this to see distribution of differences)
    critd <- obsdiff * sB
    if (d > critd) {
      counter1 <- counter1 + 1
    }
  }
  
  print(
    paste0(
      'prob that an unpoisoned sample is .33 SD less than mean of unpoisoned population: ',
      counter1 / niter
    )
  )
}

```

#### S3A (dependence of power on sample size)
 
A fertiliser is trialled to see if it improves crop yields. Without the fertiliser the average yield is 100, with standard deviation of 10. It is expected that the fertiliser will boost yield by 3 points on average.
How many plants would be needed in the treatment and control groups to be confident of demonstrating whether or not the fertiliser was effective?  
`r maketext('S3A',1)`  a)  20 per group  
`r maketext('S3A',2)`  b) 50 per group  
`r maketext('S3A',3)`  c)  100 per group  
`r maketext('S3A',4)` d) *300 per group  

 


```{r S3a, include=F, echo=F}
require(pwr)
pwr.t.test(d = .3, sig.level = .05 , power =.95 , type = "two.sample",alternative="greater") 
```
  
#### S3B. 
You have been asked to test a treatment for obesity. People in the trial have a mean body mass index (BMI) of 35, with SD of 5. The developer argues that the treatment will reduce BMI by 2 points on average, but you are dubious as to whether it has any effect. Assuming you have a control group given a placebo and an experimental group given the treatment, what sample size should you select to give a fair test of the treatment?  
`r maketext('S3B',1)` a) x20 per group   
`r maketext('S3B',2)`  b) x50 per group   
`r maketext('S3B',3)`  c) *100 per group  
`r maketext('S3B',4)`  d) 500 per group  
 




```{r S3b, include=F, echo=F}
require(pwr)
myp<-pwr.t.test(d = .4, sig.level = .05 , power =.9 , type = "two.sample",alternative="greater") 
```


#### S4A.(likelihood of given value depends on sample size: same as S2 from parallel form, but with initial adequate N)
There are concerns that kangaroos appear unhealthy in a particular area of the Australian bush. There is concern that this may be due to eating poisoned bait that affects the blood. A naturalist has blood samples from 160 kangaroos, and measures a distinctive blood marker that is lowered in poisoned animals. Previous studies have found that the mean blood marker in healthy kangaroos is 130 with SD of 30, whereas the mean is 120 with SD of 30 in poisoned animals. 

The mean blood marker in the sample is 120. Assuming that there is no other explanation than poisoning for an abnormal blood marker, what can the scientist conclude?  
`r maketext('S4A',1)` a)  *  The kangaroos have definitely been poisoned  
`r maketext('S4A',2)`   b)  x  The kangaroos probably have been poisoned but can't be sure. Collecting data from more kangaroos won't help.  
`r maketext('S4A',3)`   c) The kangaroos probably have been poisoned, but would need to collect blood from 100 more kangaroos in the affected area of bush to be sure   
`r maketext('S4A',4)`   d) The kangaroos have not been poisoned  



 


    
```{r S4A,include=F,echo=F}
#We simulate data to compute the probability that the observation comes from unpoisoned animals 

mA = 120
sA = 30
mB = 130
sB = 30
alld <- vector()
obsmean <- 120

obsdiff <- (mB - obsmean) / sB
niter <- 1000
for (n in c(10, 50, 160)) {
  #try with different sample sizes
  print(paste0('n = ', n))
  
  counter1 <- 0 #initialise counter
  
  for (i in 1:niter) {
    xsample <-
      rnorm(n, mB, sB) #observed sample - actually from unpoisoned (B)
    d <- mB - mean(xsample)
    alld <- c(alld, d)  #can plot this to see distribution of differences)
    critd <- obsdiff * sB
    if (d > critd) {
      counter1 <- counter1 + 1
    }
  }
  
  print(
    paste0(
      'prob that an unpoisoned sample is .33 SD less than mean of unpoisoned population: ',
      counter1 / niter
    )
  )
}

```

#### S4B.(Likelihood of given value depends on sample size: same as S2 from parallel form, but with initial adequate N) 
A forensic archaeologist has a set of 160 male skeletons from an ancient burial site. Experts are divided as to whether the site was colonised by Tribe A or Tribe B, the only two tribes in the region. Previous studies with large samples found that men from Tribe A had an average height of 160 cm, with SD of 6, and men from Tribe B had an average height of 158 cm, with SD of 6. 

The mean height of the sample is 158 cm. What can the archaeologist conclude?  

`r maketext('S4B',1)`  a)   * We can be very confident that the sample comes from Tribe B  
`r maketext('S4B',2)`  b)    Tribe B is more likely than Tribe A, but should collect 100 more samples to be sure  
  
`r maketext('S4B',3)`  c)    Equally likely the sample comes from Tribe A or Tribe B 

`r maketext('S4B',4)`   d)  x Can't be sure: Tribe B is more likely than Tribe A, but collecting more samples won't help



    

```{r S4B,include=F,echo=F}
#We simulate data to compute the probability that the observation comes from tribe A
 #The observed value is half a SD below average for sample A       
mA= 160  #tribe A
sA=6
mB=158 #tribe B
sB=6
obsmean <- 158
obsdiffA <- (mA-obsmean)/sA
niter<- 1000
for (n in c(10,50,160)){ #try with different sample sizes
  print(paste0('n = ',n))

counter1 <- 0 #initialise counters for various difference scores

for (i in 1:niter){
xsample<-rnorm(n,mA,sA) #observed sample - actually from tribe A
d<- mean(xsample)-mA
alld<-c(alld,d) 
critd<-obsdiffA*sA
if (d>critd){counter1 <- counter1+1}	
}



print(paste0('prob that xsample is .33 SD less than xA mean: ',counter1/niter))


}


``` 
 
#### S5A. (classic from Tversky/Kahneman law of small numbers)
In a squash tournament, the organisers are debating whether to have games of best of 3, 9 or 21 points.  Holding all other rules of the game constant, if A is a slightly better player than B, which scoring will give A a better chance of winning?  
`r maketext('S5A',1)`   a) best of 3 points  
`r maketext('S5A',2)`   b) best of 9 points   
`r maketext('S5A',3)`   c) *best of 21 points  
`r maketext('S5A',4)`    d) x Won't make any difference 

 
 
```{r S5A, echo=F,include=F}
#NB. Simulating this makes it clear that if A is much better than B, it really doesn't make much difference with original Ns! Fine if we simulate so that prob A winning is .6 and if we make lowest the best of 3.

#Assuming we can treat p(A) win as p
#In fact, this only works if p not too high and if lowest N games is v small
p = .6
games<-c(3,9,21)


  for (j in 1:3){
    seq1 <-0
   for (i in 1:1000){

  thisg<-rbinom(games[j],games[j],p)
  thiswin <- length(which(thisg>games[j]/2))
  if(thiswin>games[j]/2) {seq1<-seq1+1}

   }
    print(paste0('N games is ',games[j]))
    print(paste0('Wins in 1000 = ',sum(seq1)))
}

```

#### S5B. 
Two chess players are having a tournament. They are considering whether to play the best of 5, 11, or 17 games.  If player A is slightly better than player B, which tournament size should player B argue for, to get the best chance of winning?  
`r maketext('S5B',1)`    a) 17 games  
`r maketext('S5B',2)`   b) 11 games    
`r maketext('S5B',3)`   c) *5 games  
`r maketext('S5B',4)`   d) x It doesn't matter: The chances are the same regardless of number of games  

 

#### S6A. (Risk of type II error with small samples)
Two scientists are both trying to test whether a certain new drug affects hunger in mice, by giving them the drug (group D) or a placebo (group P) and then measuring their food consumption. At the start of the experiment, the average mouse eats 10g of food pellets, with SD of 3g.  
Scientist A runs 10 studies with 10 mice in each group, and Scientist B runs 10 studies with 30 mice in each group. Unfortunately for them, a careless lab technician distributed a placebo in place of the new drug, so there should not be any effects except by chance. When the scientists look at the results, a large effect, a difference in food consumption of 3g, is seen between the two groups (D and P) on one run of the experiment. Which scenario is more likely:  
`r maketext('S6A',1)`   a)  *The run with a large difference is found in the smaller group  
`r maketext('S6A',2)`   b)    The run with a large difference is found in the larger group  
`r maketext('S6A',3)`   c)    The group difference shows group D eats more than group P   
`r maketext('S6A',4)`   d)   x A large difference is equally likely D>P or P>D, with no effect of sample size  
  
 
 

 


```{r S6A,include=F,echo=F}
n1<-10
n2<-30
alldiffa<-vector()
alldiffb<-vector()
for (i in 1:10){
miceDa<-rnorm(n1,10,3)
micePa<-rnorm(n1,10,3)
miceDb<-rnorm(n2,10,3)
micePb<-rnorm(n2,10,3)
diffa<-mean(miceDa)-mean(micePa)
diffb<-mean(miceDb)-mean(micePb)
#print(paste0('Mean diff n8 = ',
#             diffa))
#print(paste0('Mean diff n16 = ',
#             diffb))
alldiffa<-c(alldiffa,diffa)
alldiffb<-c(alldiffb,diffb)}
print('Differences for each run of small group: ')
      print(alldiffa)
print('Differences for each run of large group: ')
      print(alldiffb)
```

#### S6B.
Two researchers are testing the effect of oxytocin on prosocial behaviour. In their experiments, participants are given either a dose of oxytocin or a placebo. Researcher A runs 5 studies with 10 participants in each condition and Researcher B runs 5 studies with 20 participants in each condition. Unknown to the researchers, the oxytocin had been replaced by a placebo, yet a large difference (1 SD) is observed between two groups on one run of the experiment. Which is most likely:  
`r maketext('S6B',1)`    a) *The large difference is seen in a study by the researcher using groups of 10 
`r maketext('S6B',2)`   b)  The large difference is seen in a study by the researcher using groups of 20  
`r maketext('S6B',3)`   c) x The difference is a type II error, and could equally likely be seen with groups of 10 or 20  
`r maketext('S6B',4)`  d)  The difference is real - there was confusion between oxytocin and placebo  

```{r saveworkspace, echo=F}
#Just saving those variables that are needed in Abstract before they are computed

N1 <- round(100*length(which(agqwide$PreS==0))/(nrow(agqwide)/2),0)
N2 <- length(dropsub)
N3 <- round(100*length(which(qualresp$waitJ==1))/nrow(qualresp),0)

save(N1,N2,N3,file='smallnums.RData')
#to reload, load("smallnums.RData") 
```
 



## References 


