---
title: "Simulating data for Law of Small Numbers study"
output: html_notebook
---
```{r loadpackages}
require(tidyverse)
require(here)
require(flextable) #for nice table output
require(psycho) #for dprime
require(lme4)
```
<!-- for stats nb http://journal.sjdm.org/stat.htm-->
See simulate model based data script for working up to the logic of the simulation



# Simulating data for analysis

For simulated data, let us supposed we have N subjects, who initially start with strategy B, with distribution 25% array 1, 50% array 2 and 15% array 3 and 10% array 4. In the second two blocks, a proportion of them (p) move to increase the mean array size by A (1 or 2). 

We will then simulate their questionnaire data as well and check power to detect main effects from various analyses. 

```{r loadspreadsheet, echo=F}
loadspreadsheet<-function(myfile){
  
  mysheet <- read.csv(here('Gorilla_spreadsheets',myfile))
  w<- which(is.na(mysheet$randomise_trials))
  mysheet<-mysheet[-w,] #remove rows that occur between blocks
  return(mysheet)
}
```

```{r simulated_data_A_with_learning}
#Make a generic spreadsheet for each strategy. Index will be the value of array, ESboundary or L, depending on strategy. NB this simulation assumes S is same for blocks 1-2 and then 3-4. But later on we just focus on comparing 1 and 4 - this allows for possibility that some learning might occur in blocks 1-2 and/or that best performance may only be seen in block 4. Since we will ignore blocks 2-3 in analysis, this method of simulation will be OK.

makesim<-function(mysheet,nsub,p.learn,errrate,A){ #array is increase in array size
  nperhalf<-nrow(mysheet)/2
  
  hvals<-matrix(c(1,nperhalf,(nperhalf+1),(2*nperhalf)),byrow=T,nrow=2)#range of rows for each half
  simdataA1<-data.frame(matrix(NA,nrow=nsub,ncol=10))
  colnames(simdataA1)<-c('ID','strategy','index1','mean.array1','dprime1','p.corr1','index2','mean.array2','dprime2','p.corr2')
  #simulate each subject one at a time by going through blocks 1-2 then 3-4
  
  o1 <- which(colnames(mysheet)=='ObsE1') # column with observed effect size (array 1)
  simdataA1$ID<-1:nsub
  simdataA1$strategy <- 'A'
  allarrayvals <- c( 1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,6,6) #
  learnvals <- c(0,1)
  
  for (i in 1:nsub){
    
    simdataA1$index1[i]<-0 #this indicates whether stick with low array or learn to go higher
    simdataA1$index2[i]<-0 #this indicates whether stick with low array or learn to go higher
    learnp <- runif(1)
    if(learnp<p.learn){
      simdataA1$index2[i] <- sample(learnvals,1)
    }
    
    for (h in 1:2){ #do for each half
      arrays <- vector()
      thistrial<-0
      for (r in hvals[h,1]:hvals[h,2]){
        thistrial<-thistrial+1
        thisarray<-sample(allarrayvals,1)
        if(h==2){
          if(simdataA1$index2[i]==1){
            thisarray<-thisarray+A
            if(thisarray>6){thisarray<-6}
          }
        }
        
        myans <- o1+thisarray-1 #column with array size that will be used
        ESvals <-mysheet[hvals[h,1]:hvals[h,2],myans]
        myresp <- rep(1,nperhalf) #response is YES if obsES greater than 1.5
        w <- which(ESvals<.15) #rows where obsES less than .15
        myresp[w] <- 0 #response set to zero
        #Need to add error responses : occasional wrong button press, so response flipped
        err.p <-runif(nperhalf)
        we<-which(err.p<errrate)
        myresp[we]<-abs(myresp[we]-1) #flips from 0 to 1 or 1 to 0
        
        arrays[thistrial]<-thisarray
        resptable <- table(myresp,mysheet$ES[hvals[h,1]:hvals[h,2]])
        p.corr<-(resptable[1,1]+resptable[2,2])/nperhalf
        dprime <- psycho::dprime(
          n_hit = resptable[1,1],
          n_fa = resptable[2,1],
          n_miss= resptable[1,2],
          n_cr=  resptable[2,2],
          adjusted = TRUE)$dprime
        if(h==1){
          simdataA1$p.corr1[i] <- p.corr
          simdataA1$dprime1[i] <- dprime
          simdataA1$mean.array1[i]<-mean(arrays)
        }
        if(h==2){
          simdataA1$p.corr2[i] <- p.corr
          simdataA1$dprime2[i] <- dprime
          simdataA1$mean.array2[i]<-mean(arrays)
        }
        
      }
    }
  }
  
  simdataA1$arraydiff<-simdataA1$mean.array2-simdataA1$mean.array1
  simdataA1$pcorrdiff<-simdataA1$p.corr2-simdataA1$p.corr1
  simdataA1$dprimediff<-simdataA1$dprime2-simdataA1$dprime1
  #Now check if differences between 1st and 2nd half are evident on t-test
  t.array<- t.test(simdataA1$mean.array1,simdataA1$mean.array2)
  t.p.corr<-  t.test(simdataA1$p.corr1,simdataA1$p.corr2)
  t.dprime <-t.test(simdataA1$dprime1,simdataA1$dprime2)
  
  return(simdataA1)
}
```




We would then aim to use difference score as predictor of gains in quiz.
Question is which difference score is best. The array measure is most sensitive in these simulated data.

```{r quizsim}
#Assume pcorr on quiz initially 3/12
#Those in learning=1 group increase by 2 pts.
# Again, need to have binomial distribution
quizsim <- function(simdata,pbad,pgood){
  binomq1 <- pbinom(0:6,6,pbad) #cumulative probabilities for scores of 1-6 out of 6, prob pass =.3
  binomq2 <- pbinom(0:6,6,pgood) #cumulative probabilities for scores of 1-6 out of 6, prob pass =.3
  
  simdata$quizSpre <- 6 #default N correct
  simdata$quizSpost <- 6#default N correct
  
  for (i in 1:nsub){
    tempqp <- runif(1)
    w<- which(binomq1>tempqp)[1]
    if(length(w)>0){
      simdata$quizSpre[i]<-w-1
    }
    tempqp <- runif(1)
    w<- which(binomq1>tempqp)[1]
    if(simdata$index2[i]==1){
      w<- which(binomq2>tempqp)[1]
    }
    if(length(w)>0){
      simdata$quizSpost[i]<-w-1
    }
    
  }
  return(simdata)
}








```


```{r tryMLL}
binsim<-function(simdata){
  #needs data in long format
  #stack time1 and time2 for questionnaire
  mc <- which(colnames(simdata)=='quizSpost')
  ma <- which(colnames(simdata)=='arraydiff')
  longsim <- rbind(simdata[,c(1,ma,mc)],simdata[,c(1,ma,mc)]) #start by just duplicating postquiz
  longsim$post <- 1
  longsim$post[1:nrow(simdata)]<-0
  colnames(longsim)[3]<-'quizscore'
  longsim$quizscore[1:nrow(simdata)]<-simdata[,(mc-1)] #now substitute prequiz
  longsim$quizscore<-longsim$quizscore/6 #proportions
  
  binsim <- glmer(quizscore  ~ 1 + post * arraydiff+ (1|ID),
                  data = longsim, family = "binomial")
  return(summary(binsim))
}



```


```{r makebigsummary,warnings=F}
mysheet<-loadspreadsheet('spreadsheet1.csv')

firstlast<-1
if(firstlast==1){ #Rather than compariong two halves (each of 2 blocks) we can just take 1st and last block
  #In this case  we just compare 2 blocks.
  #this is simulated by first shuffling blocks, then just taking 2 of them.
  #In effect, subject would do all blocks, but we just compare first and last, so only need two
  #Need to check power for this scenario, as small N items will make less reliable}
  blockbits<-matrix(c(1, 20,
                      21, 40,
                      41, 60,
                      61, 80),byrow=1,nrow=4)
  theseblocks<-sample(1:4,2,replace=FALSE) #select 2 blocks at random
  myrows<-(1+(theseblocks[1]-1)*20):(theseblocks[1]*20)
  myrows2<-(1+(theseblocks[2]-1)*20):(theseblocks[2]*20)
  
  mysheet<-mysheet[c(myrows,myrows2),]
}

nperhalf<-nrow(mysheet)/2
nsubvals <- c(60,90,120)
subbits<-paste0(nsubvals,collapse="_")
plearnvals <- c(0,.25,.33,.5)
pbits<-paste0(plearnvals,collapse="_")
quizgoodvals<-c(.5,.66) #posttest proportion correct on s items for those who learned
qbits<-paste0(quizgoodvals,collapse='_')
errrate<-0.5/nperhalf #random errs per 40 items
pbad <- .33 #probability of answering q correctly prior to learning (avg 2/6 correct)
pgood <- .66 #probability of answering q for those who learned - now treated as variable, see below


niter<-500
thisrow<-0
quizgoodvals<-c(.5,.66) #posttest proportion correct on s items for those who learned
quizbits<-paste0(quizgoodvals,collapse='_')
bigsummary<-data.frame(matrix(NA,nrow=niter*length(nsubvals)*length(plearnvals)*length(quizgoodvals)*2,ncol=14))
colnames(bigsummary)<-c('run','nsub','plearn','A','p.t.below5','p.t.array','quiz.pbad','quiz.pgood','ptquiz','lmpre.p','lmarraydiff.p',
                        'binpost.p','binarraydiff.p','bin.interact.p')
for (i in 1:niter){
  print(i)

    for (p.learn in plearnvals){
      for (A in 1:2){ #this is amount of gain in array index for those who learn
        for (pgood in quizgoodvals){
    
      simdata<-makesim(mysheet, nsub,p.learn,errrate,A)
       for (thissub in nsubvals){
       thisrow<-thisrow+1
      bigsummary$run[thisrow]<-i
     
      bigsummary$nsub[thisrow]<-thissub
      bigsummary$plearn[thisrow]<-p.learn
      bigsummary$A[thisrow]<-A
      bigsummary$quiz.pbad[thisrow]<-pbad #percent correct for nonlearners
      bigsummary$quiz.pgood[thisrow]<-pgood#percent correct for learners
      bigsummary$p.t.below5[thisrow]<-t.test(simdata$mean.array1[1:thissub],mu=5)$p.value
      bigsummary$p.t.array[thisrow]<-t.test(simdata$mean.array1[1:thissub],simdata$mean.array2)$p.value #pvalue for t-test comparing arrays for 1st and last block
      simdata<-quizsim(simdata,pbad,pgood) #adds questionnaire scores
      bigsummary$ptquiz[thisrow]<-t.test(simdata$quizSpre[1:thissub],simdata$quizSpost[1:thissub])$p.value
      regsim <- lm(quizSpost ~ quizSpre + arraydiff, data=simdata[1:thissub,])
      s<-summary(regsim)
      bigsummary$lmpre.p[thisrow]<-s$coefficients[2,4]
      bigsummary$lmarraydiff.p[thisrow]<-s$coefficients[3,4]
      dobin<-0 #set to 1 to include binomial MLL indices
      if(dobin==1){
      b<-binsim(simdata[1:thissub,])
      bigsummary$binpost.p[thisrow]<-b$coefficients[2,4]
      bigsummary$binarraydiff.p[thisrow]<-b$coefficients[3,4]
      bigsummary$bin.interact.p[thisrow]<-b$coefficients[4,4]
      }
      }
    }
  }
  }
}

bigname<-paste0('bigsummary_A12flat_pgood',qbits,'_props',pbits,'_N',subbits,'.csv')
write.csv(bigsummary,bigname,row.names=F)

```

Compute percentage < alpha

```{r powerbit}
thisrow<-0
alpha <- .01
powersummary<-data.frame(matrix(NA,ncol=7,nrow=length(nsubvals)*length(plearnvals)*length(quizgoodvals)*2))

wc<-which(colnames(bigsummary)=='lmpre.p')
colnames(powersummary)[6:7]<-colnames(bigsummary)[wc:(wc+1)]
colnames(powersummary)[1:5]<-c('nsub','plearn','A','quizgood','tlearn')
row<-0
for (Ad in 1:2){
    for (p in plearnvals){
      for (q in quizgoodvals){
      for (n in nsubvals){

      row<-row+1
      powersummary$nsub[row]<-n
      powersummary$plearn[row]<-p
      powersummary$A[row]<-Ad
      powersummary$quizgood[row]<-q
      temp<-filter(bigsummary,nsub==n,plearn==p,A==Ad,quiz.pgood==q)
      for(mycol in wc:(wc+4)){
        w<-length(which(temp[,mycol]<alpha))
        powersummary[row,(mycol-4)]<-w/nrow(temp)
      }
      powersummary$tlearn[row]<-length(which(temp$p.t.array<alpha))/nrow(temp)
    }
  }
}
}

doplot <- 0
if(doplot==1){
ggplot(data=powersummary, aes(x=plearn, y=tlearn, group=interaction(nsub,A),col=interaction(nsub,A))) +
  geom_line()+
  geom_point()+
  ggtitle("T-test for array index, block 1 vs block 4")+
  ylab("Power")+
  xlab("Proportion of subjects who learn")

ggplot(data=powersummary, aes(x=plearn, y=lmarraydiff.p, group=interaction(nsub,A),col=interaction(nsub,A))) +
  geom_line()+
  geom_point()+
    ggtitle("Prediction of Quiz S-item correct from array index increase")+
  ylab("Power")+
  xlab("Proportion of subjects who learn")
}
#Make one table for learning effect and another for impact on quiz


flextable(powersummary[,1:7])


```
